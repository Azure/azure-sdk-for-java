// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.analytics.synapse.artifacts.models;

import com.azure.core.annotation.Fluent;
import com.azure.json.JsonReader;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;

/**
 * DatabricksSparkPython activity.
 */
@Fluent
public class DatabricksSparkPythonActivity extends ExecutionActivity {
    /*
     * Type of activity.
     */
    private String type = "DatabricksSparkPython";

    /*
     * The URI of the Python file to be executed. DBFS paths are supported. Type: string (or Expression with resultType string).
     */
    private Object pythonFile;

    /*
     * Command line parameters that will be passed to the Python file.
     */
    private List<Object> parameters;

    /*
     * A list of libraries to be installed on the cluster that will execute the job.
     */
    private List<Map<String, Object>> libraries;

    /**
     * Creates an instance of DatabricksSparkPythonActivity class.
     */
    public DatabricksSparkPythonActivity() {
    }

    /**
     * Get the type property: Type of activity.
     * 
     * @return the type value.
     */
    @Override
    public String getType() {
        return this.type;
    }

    /**
     * Get the pythonFile property: The URI of the Python file to be executed. DBFS paths are supported. Type: string
     * (or Expression with resultType string).
     * 
     * @return the pythonFile value.
     */
    public Object getPythonFile() {
        return this.pythonFile;
    }

    /**
     * Set the pythonFile property: The URI of the Python file to be executed. DBFS paths are supported. Type: string
     * (or Expression with resultType string).
     * 
     * @param pythonFile the pythonFile value to set.
     * @return the DatabricksSparkPythonActivity object itself.
     */
    public DatabricksSparkPythonActivity setPythonFile(Object pythonFile) {
        this.pythonFile = pythonFile;
        return this;
    }

    /**
     * Get the parameters property: Command line parameters that will be passed to the Python file.
     * 
     * @return the parameters value.
     */
    public List<Object> getParameters() {
        return this.parameters;
    }

    /**
     * Set the parameters property: Command line parameters that will be passed to the Python file.
     * 
     * @param parameters the parameters value to set.
     * @return the DatabricksSparkPythonActivity object itself.
     */
    public DatabricksSparkPythonActivity setParameters(List<Object> parameters) {
        this.parameters = parameters;
        return this;
    }

    /**
     * Get the libraries property: A list of libraries to be installed on the cluster that will execute the job.
     * 
     * @return the libraries value.
     */
    public List<Map<String, Object>> getLibraries() {
        return this.libraries;
    }

    /**
     * Set the libraries property: A list of libraries to be installed on the cluster that will execute the job.
     * 
     * @param libraries the libraries value to set.
     * @return the DatabricksSparkPythonActivity object itself.
     */
    public DatabricksSparkPythonActivity setLibraries(List<Map<String, Object>> libraries) {
        this.libraries = libraries;
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setLinkedServiceName(LinkedServiceReference linkedServiceName) {
        super.setLinkedServiceName(linkedServiceName);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setPolicy(ActivityPolicy policy) {
        super.setPolicy(policy);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setName(String name) {
        super.setName(name);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setDescription(String description) {
        super.setDescription(description);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setState(ActivityState state) {
        super.setState(state);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setOnInactiveMarkAs(ActivityOnInactiveMarkAs onInactiveMarkAs) {
        super.setOnInactiveMarkAs(onInactiveMarkAs);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setDependsOn(List<ActivityDependency> dependsOn) {
        super.setDependsOn(dependsOn);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public DatabricksSparkPythonActivity setUserProperties(List<UserProperty> userProperties) {
        super.setUserProperties(userProperties);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("name", getName());
        jsonWriter.writeStringField("description", getDescription());
        jsonWriter.writeStringField("state", getState() == null ? null : getState().toString());
        jsonWriter.writeStringField("onInactiveMarkAs",
            getOnInactiveMarkAs() == null ? null : getOnInactiveMarkAs().toString());
        jsonWriter.writeArrayField("dependsOn", getDependsOn(), (writer, element) -> writer.writeJson(element));
        jsonWriter.writeArrayField("userProperties", getUserProperties(),
            (writer, element) -> writer.writeJson(element));
        jsonWriter.writeJsonField("linkedServiceName", getLinkedServiceName());
        jsonWriter.writeJsonField("policy", getPolicy());
        jsonWriter.writeStringField("type", this.type);
        if (pythonFile != null || parameters != null || libraries != null) {
            jsonWriter.writeStartObject("typeProperties");
            jsonWriter.writeUntypedField("pythonFile", this.pythonFile);
            jsonWriter.writeArrayField("parameters", this.parameters,
                (writer, element) -> writer.writeUntyped(element));
            jsonWriter.writeArrayField("libraries", this.libraries,
                (writer, element) -> writer.writeMap(element, (writer1, element1) -> writer1.writeUntyped(element1)));
            jsonWriter.writeEndObject();
        }
        if (getAdditionalProperties() != null) {
            for (Map.Entry<String, Object> additionalProperty : getAdditionalProperties().entrySet()) {
                jsonWriter.writeUntypedField(additionalProperty.getKey(), additionalProperty.getValue());
            }
        }
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of DatabricksSparkPythonActivity from the JsonReader.
     * 
     * @param jsonReader The JsonReader being read.
     * @return An instance of DatabricksSparkPythonActivity if the JsonReader was pointing to an instance of it, or null
     * if it was pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the DatabricksSparkPythonActivity.
     */
    public static DatabricksSparkPythonActivity fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            DatabricksSparkPythonActivity deserializedDatabricksSparkPythonActivity
                = new DatabricksSparkPythonActivity();
            Map<String, Object> additionalProperties = null;
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();

                if ("name".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity.setName(reader.getString());
                } else if ("description".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity.setDescription(reader.getString());
                } else if ("state".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity.setState(ActivityState.fromString(reader.getString()));
                } else if ("onInactiveMarkAs".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity
                        .setOnInactiveMarkAs(ActivityOnInactiveMarkAs.fromString(reader.getString()));
                } else if ("dependsOn".equals(fieldName)) {
                    List<ActivityDependency> dependsOn
                        = reader.readArray(reader1 -> ActivityDependency.fromJson(reader1));
                    deserializedDatabricksSparkPythonActivity.setDependsOn(dependsOn);
                } else if ("userProperties".equals(fieldName)) {
                    List<UserProperty> userProperties = reader.readArray(reader1 -> UserProperty.fromJson(reader1));
                    deserializedDatabricksSparkPythonActivity.setUserProperties(userProperties);
                } else if ("linkedServiceName".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity
                        .setLinkedServiceName(LinkedServiceReference.fromJson(reader));
                } else if ("policy".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity.setPolicy(ActivityPolicy.fromJson(reader));
                } else if ("type".equals(fieldName)) {
                    deserializedDatabricksSparkPythonActivity.type = reader.getString();
                } else if ("typeProperties".equals(fieldName) && reader.currentToken() == JsonToken.START_OBJECT) {
                    while (reader.nextToken() != JsonToken.END_OBJECT) {
                        fieldName = reader.getFieldName();
                        reader.nextToken();

                        if ("pythonFile".equals(fieldName)) {
                            deserializedDatabricksSparkPythonActivity.pythonFile = reader.readUntyped();
                        } else if ("parameters".equals(fieldName)) {
                            List<Object> parameters = reader.readArray(reader1 -> reader1.readUntyped());
                            deserializedDatabricksSparkPythonActivity.parameters = parameters;
                        } else if ("libraries".equals(fieldName)) {
                            List<Map<String, Object>> libraries
                                = reader.readArray(reader1 -> reader1.readMap(reader2 -> reader2.readUntyped()));
                            deserializedDatabricksSparkPythonActivity.libraries = libraries;
                        } else {
                            reader.skipChildren();
                        }
                    }
                } else {
                    if (additionalProperties == null) {
                        additionalProperties = new LinkedHashMap<>();
                    }

                    additionalProperties.put(fieldName, reader.readUntyped());
                }
            }
            deserializedDatabricksSparkPythonActivity.setAdditionalProperties(additionalProperties);

            return deserializedDatabricksSparkPythonActivity;
        });
    }
}
