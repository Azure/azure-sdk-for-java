// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.analytics.synapse.artifacts.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.annotation.JsonFlatten;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.annotation.JsonTypeName;
import java.util.List;
import java.util.Map;

/** Execute spark job activity. */
@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = "type")
@JsonTypeName("SparkJob")
@JsonFlatten
@Fluent
public class SynapseSparkJobDefinitionActivity extends ExecutionActivity {
    /*
     * Synapse spark job reference.
     */
    @JsonProperty(value = "typeProperties.sparkJob", required = true)
    private SynapseSparkJobReference sparkJob;

    /*
     * User specified arguments to SynapseSparkJobDefinitionActivity.
     */
    @JsonProperty(value = "typeProperties.args")
    private List<Object> arguments;

    /*
     * The main file used for the job, which will override the 'file' of the spark job definition you provide. Type:
     * string (or Expression with resultType string).
     */
    @JsonProperty(value = "typeProperties.file")
    private Object file;

    /*
     * Scanning subfolders from the root folder of the main definition file, these files will be added as reference
     * files. The folders named 'jars', 'pyFiles', 'files' or 'archives' will be scanned, and the folders name are case
     * sensitive. Type: boolean (or Expression with resultType boolean).
     */
    @JsonProperty(value = "typeProperties.scanFolder")
    private Object scanFolder;

    /*
     * The fully-qualified identifier or the main class that is in the main definition file, which will override the
     * 'className' of the spark job definition you provide. Type: string (or Expression with resultType string).
     */
    @JsonProperty(value = "typeProperties.className")
    private Object className;

    /*
     * (Deprecated. Please use pythonCodeReference and filesV2) Additional files used for reference in the main
     * definition file, which will override the 'files' of the spark job definition you provide.
     */
    @JsonProperty(value = "typeProperties.files")
    private List<Object> files;

    /*
     * Additional python code files used for reference in the main definition file, which will override the 'pyFiles'
     * of the spark job definition you provide.
     */
    @JsonProperty(value = "typeProperties.pythonCodeReference")
    private List<Object> pythonCodeReference;

    /*
     * Additional files used for reference in the main definition file, which will override the 'jars' and 'files' of
     * the spark job definition you provide.
     */
    @JsonProperty(value = "typeProperties.filesV2")
    private List<Object> filesV2;

    /*
     * The name of the big data pool which will be used to execute the spark batch job, which will override the
     * 'targetBigDataPool' of the spark job definition you provide.
     */
    @JsonProperty(value = "typeProperties.targetBigDataPool")
    private BigDataPoolParametrizationReference targetBigDataPool;

    /*
     * Number of core and memory to be used for executors allocated in the specified Spark pool for the job, which will
     * be used for overriding 'executorCores' and 'executorMemory' of the spark job definition you provide. Type:
     * string (or Expression with resultType string).
     */
    @JsonProperty(value = "typeProperties.executorSize")
    private Object executorSize;

    /*
     * Spark configuration properties, which will override the 'conf' of the spark job definition you provide.
     */
    @JsonProperty(value = "typeProperties.conf")
    private Object conf;

    /*
     * Number of core and memory to be used for driver allocated in the specified Spark pool for the job, which will be
     * used for overriding 'driverCores' and 'driverMemory' of the spark job definition you provide. Type: string (or
     * Expression with resultType string).
     */
    @JsonProperty(value = "typeProperties.driverSize")
    private Object driverSize;

    /*
     * Number of executors to launch for this job, which will override the 'numExecutors' of the spark job definition
     * you provide. Type: integer (or Expression with resultType integer).
     */
    @JsonProperty(value = "typeProperties.numExecutors")
    private Object numExecutors;

    /*
     * The type of the spark config.
     */
    @JsonProperty(value = "typeProperties.configurationType")
    private ConfigurationType configurationType;

    /*
     * The spark configuration of the spark job.
     */
    @JsonProperty(value = "typeProperties.targetSparkConfiguration")
    private SparkConfigurationParametrizationReference targetSparkConfiguration;

    /*
     * Spark configuration property.
     */
    @JsonProperty(value = "typeProperties.sparkConfig")
    private Map<String, Object> sparkConfig;

    /** Creates an instance of SynapseSparkJobDefinitionActivity class. */
    public SynapseSparkJobDefinitionActivity() {}

    /**
     * Get the sparkJob property: Synapse spark job reference.
     *
     * @return the sparkJob value.
     */
    public SynapseSparkJobReference getSparkJob() {
        return this.sparkJob;
    }

    /**
     * Set the sparkJob property: Synapse spark job reference.
     *
     * @param sparkJob the sparkJob value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setSparkJob(SynapseSparkJobReference sparkJob) {
        this.sparkJob = sparkJob;
        return this;
    }

    /**
     * Get the arguments property: User specified arguments to SynapseSparkJobDefinitionActivity.
     *
     * @return the arguments value.
     */
    public List<Object> getArguments() {
        return this.arguments;
    }

    /**
     * Set the arguments property: User specified arguments to SynapseSparkJobDefinitionActivity.
     *
     * @param arguments the arguments value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setArguments(List<Object> arguments) {
        this.arguments = arguments;
        return this;
    }

    /**
     * Get the file property: The main file used for the job, which will override the 'file' of the spark job definition
     * you provide. Type: string (or Expression with resultType string).
     *
     * @return the file value.
     */
    public Object getFile() {
        return this.file;
    }

    /**
     * Set the file property: The main file used for the job, which will override the 'file' of the spark job definition
     * you provide. Type: string (or Expression with resultType string).
     *
     * @param file the file value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setFile(Object file) {
        this.file = file;
        return this;
    }

    /**
     * Get the scanFolder property: Scanning subfolders from the root folder of the main definition file, these files
     * will be added as reference files. The folders named 'jars', 'pyFiles', 'files' or 'archives' will be scanned, and
     * the folders name are case sensitive. Type: boolean (or Expression with resultType boolean).
     *
     * @return the scanFolder value.
     */
    public Object getScanFolder() {
        return this.scanFolder;
    }

    /**
     * Set the scanFolder property: Scanning subfolders from the root folder of the main definition file, these files
     * will be added as reference files. The folders named 'jars', 'pyFiles', 'files' or 'archives' will be scanned, and
     * the folders name are case sensitive. Type: boolean (or Expression with resultType boolean).
     *
     * @param scanFolder the scanFolder value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setScanFolder(Object scanFolder) {
        this.scanFolder = scanFolder;
        return this;
    }

    /**
     * Get the className property: The fully-qualified identifier or the main class that is in the main definition file,
     * which will override the 'className' of the spark job definition you provide. Type: string (or Expression with
     * resultType string).
     *
     * @return the className value.
     */
    public Object getClassName() {
        return this.className;
    }

    /**
     * Set the className property: The fully-qualified identifier or the main class that is in the main definition file,
     * which will override the 'className' of the spark job definition you provide. Type: string (or Expression with
     * resultType string).
     *
     * @param className the className value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setClassName(Object className) {
        this.className = className;
        return this;
    }

    /**
     * Get the files property: (Deprecated. Please use pythonCodeReference and filesV2) Additional files used for
     * reference in the main definition file, which will override the 'files' of the spark job definition you provide.
     *
     * @return the files value.
     */
    public List<Object> getFiles() {
        return this.files;
    }

    /**
     * Set the files property: (Deprecated. Please use pythonCodeReference and filesV2) Additional files used for
     * reference in the main definition file, which will override the 'files' of the spark job definition you provide.
     *
     * @param files the files value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setFiles(List<Object> files) {
        this.files = files;
        return this;
    }

    /**
     * Get the pythonCodeReference property: Additional python code files used for reference in the main definition
     * file, which will override the 'pyFiles' of the spark job definition you provide.
     *
     * @return the pythonCodeReference value.
     */
    public List<Object> getPythonCodeReference() {
        return this.pythonCodeReference;
    }

    /**
     * Set the pythonCodeReference property: Additional python code files used for reference in the main definition
     * file, which will override the 'pyFiles' of the spark job definition you provide.
     *
     * @param pythonCodeReference the pythonCodeReference value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setPythonCodeReference(List<Object> pythonCodeReference) {
        this.pythonCodeReference = pythonCodeReference;
        return this;
    }

    /**
     * Get the filesV2 property: Additional files used for reference in the main definition file, which will override
     * the 'jars' and 'files' of the spark job definition you provide.
     *
     * @return the filesV2 value.
     */
    public List<Object> getFilesV2() {
        return this.filesV2;
    }

    /**
     * Set the filesV2 property: Additional files used for reference in the main definition file, which will override
     * the 'jars' and 'files' of the spark job definition you provide.
     *
     * @param filesV2 the filesV2 value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setFilesV2(List<Object> filesV2) {
        this.filesV2 = filesV2;
        return this;
    }

    /**
     * Get the targetBigDataPool property: The name of the big data pool which will be used to execute the spark batch
     * job, which will override the 'targetBigDataPool' of the spark job definition you provide.
     *
     * @return the targetBigDataPool value.
     */
    public BigDataPoolParametrizationReference getTargetBigDataPool() {
        return this.targetBigDataPool;
    }

    /**
     * Set the targetBigDataPool property: The name of the big data pool which will be used to execute the spark batch
     * job, which will override the 'targetBigDataPool' of the spark job definition you provide.
     *
     * @param targetBigDataPool the targetBigDataPool value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setTargetBigDataPool(
            BigDataPoolParametrizationReference targetBigDataPool) {
        this.targetBigDataPool = targetBigDataPool;
        return this;
    }

    /**
     * Get the executorSize property: Number of core and memory to be used for executors allocated in the specified
     * Spark pool for the job, which will be used for overriding 'executorCores' and 'executorMemory' of the spark job
     * definition you provide. Type: string (or Expression with resultType string).
     *
     * @return the executorSize value.
     */
    public Object getExecutorSize() {
        return this.executorSize;
    }

    /**
     * Set the executorSize property: Number of core and memory to be used for executors allocated in the specified
     * Spark pool for the job, which will be used for overriding 'executorCores' and 'executorMemory' of the spark job
     * definition you provide. Type: string (or Expression with resultType string).
     *
     * @param executorSize the executorSize value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setExecutorSize(Object executorSize) {
        this.executorSize = executorSize;
        return this;
    }

    /**
     * Get the conf property: Spark configuration properties, which will override the 'conf' of the spark job definition
     * you provide.
     *
     * @return the conf value.
     */
    public Object getConf() {
        return this.conf;
    }

    /**
     * Set the conf property: Spark configuration properties, which will override the 'conf' of the spark job definition
     * you provide.
     *
     * @param conf the conf value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setConf(Object conf) {
        this.conf = conf;
        return this;
    }

    /**
     * Get the driverSize property: Number of core and memory to be used for driver allocated in the specified Spark
     * pool for the job, which will be used for overriding 'driverCores' and 'driverMemory' of the spark job definition
     * you provide. Type: string (or Expression with resultType string).
     *
     * @return the driverSize value.
     */
    public Object getDriverSize() {
        return this.driverSize;
    }

    /**
     * Set the driverSize property: Number of core and memory to be used for driver allocated in the specified Spark
     * pool for the job, which will be used for overriding 'driverCores' and 'driverMemory' of the spark job definition
     * you provide. Type: string (or Expression with resultType string).
     *
     * @param driverSize the driverSize value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setDriverSize(Object driverSize) {
        this.driverSize = driverSize;
        return this;
    }

    /**
     * Get the numExecutors property: Number of executors to launch for this job, which will override the 'numExecutors'
     * of the spark job definition you provide. Type: integer (or Expression with resultType integer).
     *
     * @return the numExecutors value.
     */
    public Object getNumExecutors() {
        return this.numExecutors;
    }

    /**
     * Set the numExecutors property: Number of executors to launch for this job, which will override the 'numExecutors'
     * of the spark job definition you provide. Type: integer (or Expression with resultType integer).
     *
     * @param numExecutors the numExecutors value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setNumExecutors(Object numExecutors) {
        this.numExecutors = numExecutors;
        return this;
    }

    /**
     * Get the configurationType property: The type of the spark config.
     *
     * @return the configurationType value.
     */
    public ConfigurationType getConfigurationType() {
        return this.configurationType;
    }

    /**
     * Set the configurationType property: The type of the spark config.
     *
     * @param configurationType the configurationType value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setConfigurationType(ConfigurationType configurationType) {
        this.configurationType = configurationType;
        return this;
    }

    /**
     * Get the targetSparkConfiguration property: The spark configuration of the spark job.
     *
     * @return the targetSparkConfiguration value.
     */
    public SparkConfigurationParametrizationReference getTargetSparkConfiguration() {
        return this.targetSparkConfiguration;
    }

    /**
     * Set the targetSparkConfiguration property: The spark configuration of the spark job.
     *
     * @param targetSparkConfiguration the targetSparkConfiguration value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setTargetSparkConfiguration(
            SparkConfigurationParametrizationReference targetSparkConfiguration) {
        this.targetSparkConfiguration = targetSparkConfiguration;
        return this;
    }

    /**
     * Get the sparkConfig property: Spark configuration property.
     *
     * @return the sparkConfig value.
     */
    public Map<String, Object> getSparkConfig() {
        return this.sparkConfig;
    }

    /**
     * Set the sparkConfig property: Spark configuration property.
     *
     * @param sparkConfig the sparkConfig value to set.
     * @return the SynapseSparkJobDefinitionActivity object itself.
     */
    public SynapseSparkJobDefinitionActivity setSparkConfig(Map<String, Object> sparkConfig) {
        this.sparkConfig = sparkConfig;
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setLinkedServiceName(LinkedServiceReference linkedServiceName) {
        super.setLinkedServiceName(linkedServiceName);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setPolicy(ActivityPolicy policy) {
        super.setPolicy(policy);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setName(String name) {
        super.setName(name);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setDescription(String description) {
        super.setDescription(description);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setState(ActivityState state) {
        super.setState(state);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setOnInactiveMarkAs(ActivityOnInactiveMarkAs onInactiveMarkAs) {
        super.setOnInactiveMarkAs(onInactiveMarkAs);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setDependsOn(List<ActivityDependency> dependsOn) {
        super.setDependsOn(dependsOn);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SynapseSparkJobDefinitionActivity setUserProperties(List<UserProperty> userProperties) {
        super.setUserProperties(userProperties);
        return this;
    }
}
