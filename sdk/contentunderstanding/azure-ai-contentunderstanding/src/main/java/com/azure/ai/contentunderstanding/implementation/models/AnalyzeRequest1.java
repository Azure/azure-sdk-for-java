// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.ai.contentunderstanding.implementation.models;

import com.azure.ai.contentunderstanding.models.AnalyzeInput;
import com.azure.core.annotation.Fluent;
import com.azure.core.annotation.Generated;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.List;
import java.util.Map;

/**
 * The AnalyzeRequest1 model.
 */
@Fluent
public final class AnalyzeRequest1 implements JsonSerializable<AnalyzeRequest1> {
    /*
     * Inputs to analyze. Currently, only pro mode supports multiple inputs.
     */
    @Generated
    private final List<AnalyzeInput> inputs;

    /*
     * Override the resource-level default mapping of supported large language model (LLM) names to model deployment
     * names in Microsoft Foundry. Dictionary of string to string
     * (LLM model name -> model deployment name in Microsoft Foundry). Keys must be supported model names for the
     * analyzer you are calling (get them via Get Analyzer, GET /analyzers/{analyzerId}, response.supportedModels).
     * Values are model deployment names in your Microsoft Foundry resource.
     * To get more information for a quickstart for REST API, see https://aka.ms/cudoc-quickstart-rest.
     * Example: { "gpt-4.1": "myGpt41Deployment", "text-embedding-3-large": "myTextEmbedding3LargeDeployment" }.
     */
    @Generated
    private Map<String, String> modelDeployments;

    /**
     * Creates an instance of AnalyzeRequest1 class.
     * 
     * @param inputs the inputs value to set.
     */
    @Generated
    public AnalyzeRequest1(List<AnalyzeInput> inputs) {
        this.inputs = inputs;
    }

    /**
     * Get the inputs property: Inputs to analyze. Currently, only pro mode supports multiple inputs.
     * 
     * @return the inputs value.
     */
    @Generated
    public List<AnalyzeInput> getInputs() {
        return this.inputs;
    }

    /**
     * Get the modelDeployments property: Override the resource-level default mapping of supported large language model
     * (LLM) names to model deployment names in Microsoft Foundry. Dictionary of string to string
     * (LLM model name -&gt; model deployment name in Microsoft Foundry). Keys must be supported model names for the
     * analyzer you are calling (get them via Get Analyzer, GET /analyzers/{analyzerId}, response.supportedModels).
     * Values are model deployment names in your Microsoft Foundry resource.
     * To get more information for a quickstart for REST API, see https://aka.ms/cudoc-quickstart-rest.
     * Example: { "gpt-4.1": "myGpt41Deployment", "text-embedding-3-large": "myTextEmbedding3LargeDeployment" }.
     * 
     * @return the modelDeployments value.
     */
    @Generated
    public Map<String, String> getModelDeployments() {
        return this.modelDeployments;
    }

    /**
     * Set the modelDeployments property: Override the resource-level default mapping of supported large language model
     * (LLM) names to model deployment names in Microsoft Foundry. Dictionary of string to string
     * (LLM model name -&gt; model deployment name in Microsoft Foundry). Keys must be supported model names for the
     * analyzer you are calling (get them via Get Analyzer, GET /analyzers/{analyzerId}, response.supportedModels).
     * Values are model deployment names in your Microsoft Foundry resource.
     * To get more information for a quickstart for REST API, see https://aka.ms/cudoc-quickstart-rest.
     * Example: { "gpt-4.1": "myGpt41Deployment", "text-embedding-3-large": "myTextEmbedding3LargeDeployment" }.
     * 
     * @param modelDeployments the modelDeployments value to set.
     * @return the AnalyzeRequest1 object itself.
     */
    @Generated
    public AnalyzeRequest1 setModelDeployments(Map<String, String> modelDeployments) {
        this.modelDeployments = modelDeployments;
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeArrayField("inputs", this.inputs, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeMapField("modelDeployments", this.modelDeployments,
            (writer, element) -> writer.writeString(element));
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of AnalyzeRequest1 from the JsonReader.
     * 
     * @param jsonReader The JsonReader being read.
     * @return An instance of AnalyzeRequest1 if the JsonReader was pointing to an instance of it, or null if it was
     * pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the AnalyzeRequest1.
     */
    @Generated
    public static AnalyzeRequest1 fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            List<AnalyzeInput> inputs = null;
            Map<String, String> modelDeployments = null;
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();

                if ("inputs".equals(fieldName)) {
                    inputs = reader.readArray(reader1 -> AnalyzeInput.fromJson(reader1));
                } else if ("modelDeployments".equals(fieldName)) {
                    modelDeployments = reader.readMap(reader1 -> reader1.getString());
                } else {
                    reader.skipChildren();
                }
            }
            AnalyzeRequest1 deserializedAnalyzeRequest1 = new AnalyzeRequest1(inputs);
            deserializedAnalyzeRequest1.modelDeployments = modelDeployments;

            return deserializedAnalyzeRequest1;
        });
    }
}
