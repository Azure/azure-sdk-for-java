// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.ai.contentunderstanding.generated;

import com.azure.ai.contentunderstanding.models.AnalyzeInput;
import com.azure.ai.contentunderstanding.models.AnalyzeResult;
import com.azure.ai.contentunderstanding.models.ArrayField;
import com.azure.ai.contentunderstanding.models.ContentAnalyzer;
import com.azure.ai.contentunderstanding.models.ContentAnalyzerConfig;
import com.azure.ai.contentunderstanding.models.ContentFieldDefinition;
import com.azure.ai.contentunderstanding.models.ContentFieldSchema;
import com.azure.ai.contentunderstanding.models.ContentFieldType;
import com.azure.ai.contentunderstanding.models.DocumentContent;
import com.azure.ai.contentunderstanding.models.GenerationMethod;
import com.azure.ai.contentunderstanding.models.LabeledDataKnowledgeSource;
import com.azure.ai.contentunderstanding.models.ObjectField;
import com.azure.ai.contentunderstanding.models.StringField;
import com.azure.core.util.polling.SyncPoller;
import org.junit.jupiter.api.Test;

import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Sample demonstrates how to create an analyzer with labeled training data from Azure Blob Storage.
 *
 * Required environment variables:
 * - TRAINING_DATA_STORAGE_ACCOUNT: Azure Storage account name
 * - TRAINING_DATA_CONTAINER_NAME: Container name with training data
 * - TRAINING_DATA_SAS_URL: SAS URL for the container (optional, if not using managed identity)
 *
 * Training data structure:
 * - Container should have labeled documents with .labels.json and .result.json files
 * - Example: receipt.pdf, receipt.pdf.labels.json, receipt.pdf.result.json
 */
public class Sample16_CreateAnalyzerWithLabels extends ContentUnderstandingClientTestBase {

    /**
     * Demonstrates creating an analyzer with labeled training data.
     *
     * This test demonstrates the API pattern without requiring actual training data setup.
     */
    @Test
    public void testCreateAnalyzerWithLabelsAsync() {

        String analyzerId = testResourceNamer.randomName("test_receipt_analyzer_", 50);

        try {
            // BEGIN: com.azure.ai.contentunderstanding.createAnalyzerWithLabels
            // Step 1: Define field schema for receipt extraction
            Map<String, ContentFieldDefinition> fields = new HashMap<>();

            // MerchantName field
            ContentFieldDefinition merchantNameField = new ContentFieldDefinition();
            merchantNameField.setType(ContentFieldType.STRING);
            merchantNameField.setMethod(GenerationMethod.EXTRACT);
            merchantNameField.setDescription("Name of the merchant");
            fields.put("MerchantName", merchantNameField);

            // Items array field - define item structure
            ContentFieldDefinition itemDefinition = new ContentFieldDefinition();
            itemDefinition.setType(ContentFieldType.OBJECT);
            itemDefinition.setMethod(GenerationMethod.EXTRACT);
            itemDefinition.setDescription("Individual item details");

            Map<String, ContentFieldDefinition> itemProperties = new HashMap<>();

            ContentFieldDefinition quantityField = new ContentFieldDefinition();
            quantityField.setType(ContentFieldType.STRING);
            quantityField.setMethod(GenerationMethod.EXTRACT);
            quantityField.setDescription("Quantity of the item");
            itemProperties.put("Quantity", quantityField);

            ContentFieldDefinition nameField = new ContentFieldDefinition();
            nameField.setType(ContentFieldType.STRING);
            nameField.setMethod(GenerationMethod.EXTRACT);
            nameField.setDescription("Name of the item");
            itemProperties.put("Name", nameField);

            ContentFieldDefinition priceField = new ContentFieldDefinition();
            priceField.setType(ContentFieldType.STRING);
            priceField.setMethod(GenerationMethod.EXTRACT);
            priceField.setDescription("Price of the item");
            itemProperties.put("Price", priceField);

            itemDefinition.setProperties(itemProperties);

            // Items array field
            ContentFieldDefinition itemsField = new ContentFieldDefinition();
            itemsField.setType(ContentFieldType.ARRAY);
            itemsField.setMethod(GenerationMethod.GENERATE);
            itemsField.setDescription("List of items purchased");
            itemsField.setItemDefinition(itemDefinition);
            fields.put("Items", itemsField);

            // Total field
            ContentFieldDefinition totalField = new ContentFieldDefinition();
            totalField.setType(ContentFieldType.STRING);
            totalField.setMethod(GenerationMethod.EXTRACT);
            totalField.setDescription("Total amount");
            fields.put("Total", totalField);

            ContentFieldSchema fieldSchema = new ContentFieldSchema();
            fieldSchema.setName("receipt_schema");
            fieldSchema.setDescription("Schema for receipt extraction with items");
            fieldSchema.setFields(fields);

            // Step 2: Create labeled data knowledge source
            // For actual use, provide Azure Blob Storage SAS URL with training data:
            //
            // String storageAccount = System.getenv("TRAINING_DATA_STORAGE_ACCOUNT");
            // String containerName = System.getenv("TRAINING_DATA_CONTAINER_NAME");
            // String sasUrl = System.getenv("TRAINING_DATA_SAS_URL");
            // String trainingDataPath = "training_data/"; // Path prefix in container
            //
            // LabeledDataKnowledgeSource knowledgeSource = new LabeledDataKnowledgeSource();
            // knowledgeSource.setUrl(sasUrl);
            // knowledgeSource.setPrefix(trainingDataPath);
            //
            // List<LabeledDataKnowledgeSource> knowledgeSources = Collections.singletonList(knowledgeSource);

            // Step 3: Create analyzer with labeled data
            ContentAnalyzerConfig config = new ContentAnalyzerConfig();
            config.setEnableLayout(true);
            config.setEnableOcr(true);

            ContentAnalyzer analyzer = new ContentAnalyzer();
            analyzer.setBaseAnalyzerId("prebuilt-document");
            analyzer.setDescription("Receipt analyzer with labeled training data");
            analyzer.setConfig(config);
            analyzer.setFieldSchema(fieldSchema);
            // analyzer.setKnowledgeSources(knowledgeSources); // Add when using actual training data

            // Add model mappings
            Map<String, String> models = new HashMap<>();
            models.put("completion", "gpt-4.1");
            models.put("embedding", "text-embedding-3-large");
            analyzer.setModels(models);

            // For demonstration without actual training data, create analyzer without knowledge sources
            SyncPoller<com.azure.ai.contentunderstanding.models.ContentAnalyzerOperationStatus, ContentAnalyzer> createPoller
                = contentUnderstandingClient.beginCreateAnalyzer(analyzerId, analyzer, true);
            ContentAnalyzer result = createPoller.getFinalResult();

            System.out.println("Analyzer created: " + analyzerId);
            System.out.println("  Description: " + result.getDescription());
            System.out.println("  Base analyzer: " + result.getBaseAnalyzerId());
            System.out.println("  Fields: " + result.getFieldSchema().getFields().size());
            // END: com.azure.ai.contentunderstanding.createAnalyzerWithLabels

            // Verify analyzer creation
            System.out.println("\nüìã Analyzer Creation Verification:");
            assertNotNull(result, "Analyzer should not be null");
            assertEquals("prebuilt-document", result.getBaseAnalyzerId());
            assertEquals("Receipt analyzer with labeled training data", result.getDescription());
            assertNotNull(result.getFieldSchema());
            assertEquals("receipt_schema", result.getFieldSchema().getName());
            assertEquals(3, result.getFieldSchema().getFields().size());
            System.out.println("Analyzer created successfully");

            // Verify field schema
            Map<String, ContentFieldDefinition> resultFields = result.getFieldSchema().getFields();
            assertTrue(resultFields.containsKey("MerchantName"), "Should have MerchantName field");
            assertTrue(resultFields.containsKey("Items"), "Should have Items field");
            assertTrue(resultFields.containsKey("Total"), "Should have Total field");

            ContentFieldDefinition itemsFieldResult = resultFields.get("Items");
            assertEquals(ContentFieldType.ARRAY, itemsFieldResult.getType());
            assertNotNull(itemsFieldResult.getItemDefinition());
            assertEquals(ContentFieldType.OBJECT, itemsFieldResult.getItemDefinition().getType());
            assertEquals(3, itemsFieldResult.getItemDefinition().getProperties().size());
            System.out.println("Field schema verified:");
            System.out.println("  MerchantName: String (Extract)");
            System.out.println("  Items: Array of Objects (Generate)");
            System.out.println("    - Quantity, Name, Price");
            System.out.println("  Total: String (Extract)");

            // Display API pattern information
            System.out.println("\nüìö CreateAnalyzerWithLabels API Pattern:");
            System.out.println("   1. Define field schema with nested structures (arrays, objects)");
            System.out.println("   2. Upload training data to Azure Blob Storage:");
            System.out.println("      - Documents: receipt1.pdf, receipt2.pdf, ...");
            System.out.println("      - Labels: receipt1.pdf.labels.json, receipt2.pdf.labels.json, ...");
            System.out.println("      - OCR: receipt1.pdf.result.json, receipt2.pdf.result.json, ...");
            System.out.println("   3. Create LabeledDataKnowledgeSource with storage SAS URL");
            System.out.println("   4. Create analyzer with field schema and knowledge sources");
            System.out.println("   5. Use analyzer for document analysis");

            System.out.println("\n‚úÖ CreateAnalyzerWithLabels pattern demonstration completed");
            System.out.println("   Note: This sample demonstrates the API pattern.");
            System.out.println("   For actual training, provide TRAINING_DATA_SAS_URL with labeled data.");

        } finally {
            // Cleanup
            try {
                contentUnderstandingClient.deleteAnalyzer(analyzerId);
                System.out.println("\nAnalyzer deleted: " + analyzerId);
            } catch (Exception e) {
                System.out.println("Note: Failed to delete analyzer: " + e.getMessage());
            }
        }
    }

    /**
     * Demonstrates creating and using an analyzer with actual labeled training data.
     *
     * Requires environment variables:
     * - TRAINING_DATA_SAS_URL: SAS URL for Azure Blob Storage container with training data
     */
    @Test
    public void testCreateAnalyzerWithActualLabels() {
        String trainingDataSasUrl = System.getenv("TRAINING_DATA_SAS_URL");

        if (trainingDataSasUrl == null || trainingDataSasUrl.trim().isEmpty()) {
            System.out.println("‚ö†Ô∏è TRAINING_DATA_SAS_URL not provided. Skipping test with actual training data.");
            System.out.println("   To run this test, set TRAINING_DATA_SAS_URL to your Azure Blob Storage SAS URL.");
            System.out.println("   Training data should include:");
            System.out.println("   - Documents (e.g., receipt1.pdf)");
            System.out.println("   - Labels (e.g., receipt1.pdf.labels.json)");
            System.out.println("   - OCR results (e.g., receipt1.pdf.result.json)");
            return;
        }

        String analyzerId = testResourceNamer.randomName("receipt_analyzer_with_training_", 50);
        String trainingDataPath = System.getenv("TRAINING_DATA_PATH");
        if (trainingDataPath == null) {
            trainingDataPath = "training_data/";
        }
        if (!trainingDataPath.endsWith("/")) {
            trainingDataPath += "/";
        }

        try {
            // Define field schema
            Map<String, ContentFieldDefinition> fields = new HashMap<>();

            ContentFieldDefinition merchantNameField = new ContentFieldDefinition();
            merchantNameField.setType(ContentFieldType.STRING);
            merchantNameField.setMethod(GenerationMethod.EXTRACT);
            fields.put("MerchantName", merchantNameField);

            ContentFieldDefinition totalField = new ContentFieldDefinition();
            totalField.setType(ContentFieldType.STRING);
            totalField.setMethod(GenerationMethod.EXTRACT);
            fields.put("Total", totalField);

            ContentFieldSchema fieldSchema = new ContentFieldSchema();
            fieldSchema.setName("receipt_schema_trained");
            fieldSchema.setFields(fields);

            // Create knowledge source with training data
            LabeledDataKnowledgeSource knowledgeSource = new LabeledDataKnowledgeSource();
            knowledgeSource.setContainerUrl(trainingDataSasUrl);
            knowledgeSource.setPrefix(trainingDataPath);

            // Create analyzer
            ContentAnalyzer analyzer = new ContentAnalyzer();
            analyzer.setBaseAnalyzerId("prebuilt-document");
            analyzer.setDescription("Receipt analyzer trained with labeled data");
            analyzer.setFieldSchema(fieldSchema);
            analyzer.setKnowledgeSources(Collections.singletonList(knowledgeSource));

            ContentAnalyzer result
                = contentUnderstandingClient.beginCreateAnalyzer(analyzerId, analyzer).getFinalResult();
            System.out.println("Analyzer with training data created: " + analyzerId);

            // Test the analyzer with a sample document
            String testDocUrl
                = "https://github.com/Azure-Samples/cognitive-services-REST-api-samples/raw/master/curl/form-recognizer/sample-invoice.pdf";

            AnalyzeInput input = new AnalyzeInput();
            input.setUrl(testDocUrl);

            AnalyzeResult analyzeResult = contentUnderstandingClient
                .beginAnalyze(analyzerId, null, null, Collections.singletonList(input), null)
                .getFinalResult();

            System.out.println("Analysis completed!");
            assertNotNull(analyzeResult);
            assertNotNull(analyzeResult.getContents());
            assertTrue(analyzeResult.getContents().size() > 0);

            if (analyzeResult.getContents().get(0) instanceof DocumentContent) {
                DocumentContent docContent = (DocumentContent) analyzeResult.getContents().get(0);
                System.out.println("Extracted fields: " + docContent.getFields().size());

                // Display extracted values
                if (docContent.getFields().containsKey("MerchantName")) {
                    Object merchantField = docContent.getFields().get("MerchantName");
                    if (merchantField instanceof StringField) {
                        System.out.println("  MerchantName: " + ((StringField) merchantField).getValueString());
                    }
                }
                if (docContent.getFields().containsKey("Total")) {
                    Object totalFieldValue = docContent.getFields().get("Total");
                    if (totalFieldValue instanceof StringField) {
                        System.out.println("  Total: " + ((StringField) totalFieldValue).getValueString());
                    }
                }
            }

            System.out.println("‚úÖ Analyzer with training data test completed successfully");

        } finally {
            // Cleanup
            try {
                contentUnderstandingClient.deleteAnalyzer(analyzerId);
                System.out.println("Analyzer deleted: " + analyzerId);
            } catch (Exception e) {
                // Ignore cleanup errors
            }
        }
    }
}
