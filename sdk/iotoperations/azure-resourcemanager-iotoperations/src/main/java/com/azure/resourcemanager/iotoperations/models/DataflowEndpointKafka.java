// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.resourcemanager.iotoperations.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.util.logging.ClientLogger;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;

/**
 * Kafka endpoint properties.
 */
@Fluent
public final class DataflowEndpointKafka implements JsonSerializable<DataflowEndpointKafka> {
    /*
     * Authentication configuration. NOTE - only authentication property is allowed per entry.
     */
    private DataflowEndpointKafkaAuthentication authentication;

    /*
     * Consumer group ID.
     */
    private String consumerGroupId;

    /*
     * Kafka endpoint host.
     */
    private String host;

    /*
     * Batching configuration.
     */
    private DataflowEndpointKafkaBatching batching;

    /*
     * Copy Broker properties. No effect if the endpoint is used as a source or if the dataflow doesn't have an Broker
     * source.
     */
    private OperationalMode copyMqttProperties;

    /*
     * Compression. Can be none, gzip, lz4, or snappy. No effect if the endpoint is used as a source.
     */
    private DataflowEndpointKafkaCompression compression;

    /*
     * Kafka acks. Can be all, one, or zero. No effect if the endpoint is used as a source.
     */
    private DataflowEndpointKafkaAcks kafkaAcks;

    /*
     * Partition handling strategy. Can be default or static. No effect if the endpoint is used as a source.
     */
    private DataflowEndpointKafkaPartitionStrategy partitionStrategy;

    /*
     * TLS configuration.
     */
    private TlsProperties tls;

    /*
     * Cloud event mapping config.
     */
    private CloudEventAttributeType cloudEventAttributes;

    /**
     * Creates an instance of DataflowEndpointKafka class.
     */
    public DataflowEndpointKafka() {
    }

    /**
     * Get the authentication property: Authentication configuration. NOTE - only authentication property is allowed per
     * entry.
     * 
     * @return the authentication value.
     */
    public DataflowEndpointKafkaAuthentication authentication() {
        return this.authentication;
    }

    /**
     * Set the authentication property: Authentication configuration. NOTE - only authentication property is allowed per
     * entry.
     * 
     * @param authentication the authentication value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withAuthentication(DataflowEndpointKafkaAuthentication authentication) {
        this.authentication = authentication;
        return this;
    }

    /**
     * Get the consumerGroupId property: Consumer group ID.
     * 
     * @return the consumerGroupId value.
     */
    public String consumerGroupId() {
        return this.consumerGroupId;
    }

    /**
     * Set the consumerGroupId property: Consumer group ID.
     * 
     * @param consumerGroupId the consumerGroupId value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withConsumerGroupId(String consumerGroupId) {
        this.consumerGroupId = consumerGroupId;
        return this;
    }

    /**
     * Get the host property: Kafka endpoint host.
     * 
     * @return the host value.
     */
    public String host() {
        return this.host;
    }

    /**
     * Set the host property: Kafka endpoint host.
     * 
     * @param host the host value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withHost(String host) {
        this.host = host;
        return this;
    }

    /**
     * Get the batching property: Batching configuration.
     * 
     * @return the batching value.
     */
    public DataflowEndpointKafkaBatching batching() {
        return this.batching;
    }

    /**
     * Set the batching property: Batching configuration.
     * 
     * @param batching the batching value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withBatching(DataflowEndpointKafkaBatching batching) {
        this.batching = batching;
        return this;
    }

    /**
     * Get the copyMqttProperties property: Copy Broker properties. No effect if the endpoint is used as a source or if
     * the dataflow doesn't have an Broker source.
     * 
     * @return the copyMqttProperties value.
     */
    public OperationalMode copyMqttProperties() {
        return this.copyMqttProperties;
    }

    /**
     * Set the copyMqttProperties property: Copy Broker properties. No effect if the endpoint is used as a source or if
     * the dataflow doesn't have an Broker source.
     * 
     * @param copyMqttProperties the copyMqttProperties value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withCopyMqttProperties(OperationalMode copyMqttProperties) {
        this.copyMqttProperties = copyMqttProperties;
        return this;
    }

    /**
     * Get the compression property: Compression. Can be none, gzip, lz4, or snappy. No effect if the endpoint is used
     * as a source.
     * 
     * @return the compression value.
     */
    public DataflowEndpointKafkaCompression compression() {
        return this.compression;
    }

    /**
     * Set the compression property: Compression. Can be none, gzip, lz4, or snappy. No effect if the endpoint is used
     * as a source.
     * 
     * @param compression the compression value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withCompression(DataflowEndpointKafkaCompression compression) {
        this.compression = compression;
        return this;
    }

    /**
     * Get the kafkaAcks property: Kafka acks. Can be all, one, or zero. No effect if the endpoint is used as a source.
     * 
     * @return the kafkaAcks value.
     */
    public DataflowEndpointKafkaAcks kafkaAcks() {
        return this.kafkaAcks;
    }

    /**
     * Set the kafkaAcks property: Kafka acks. Can be all, one, or zero. No effect if the endpoint is used as a source.
     * 
     * @param kafkaAcks the kafkaAcks value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withKafkaAcks(DataflowEndpointKafkaAcks kafkaAcks) {
        this.kafkaAcks = kafkaAcks;
        return this;
    }

    /**
     * Get the partitionStrategy property: Partition handling strategy. Can be default or static. No effect if the
     * endpoint is used as a source.
     * 
     * @return the partitionStrategy value.
     */
    public DataflowEndpointKafkaPartitionStrategy partitionStrategy() {
        return this.partitionStrategy;
    }

    /**
     * Set the partitionStrategy property: Partition handling strategy. Can be default or static. No effect if the
     * endpoint is used as a source.
     * 
     * @param partitionStrategy the partitionStrategy value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withPartitionStrategy(DataflowEndpointKafkaPartitionStrategy partitionStrategy) {
        this.partitionStrategy = partitionStrategy;
        return this;
    }

    /**
     * Get the tls property: TLS configuration.
     * 
     * @return the tls value.
     */
    public TlsProperties tls() {
        return this.tls;
    }

    /**
     * Set the tls property: TLS configuration.
     * 
     * @param tls the tls value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withTls(TlsProperties tls) {
        this.tls = tls;
        return this;
    }

    /**
     * Get the cloudEventAttributes property: Cloud event mapping config.
     * 
     * @return the cloudEventAttributes value.
     */
    public CloudEventAttributeType cloudEventAttributes() {
        return this.cloudEventAttributes;
    }

    /**
     * Set the cloudEventAttributes property: Cloud event mapping config.
     * 
     * @param cloudEventAttributes the cloudEventAttributes value to set.
     * @return the DataflowEndpointKafka object itself.
     */
    public DataflowEndpointKafka withCloudEventAttributes(CloudEventAttributeType cloudEventAttributes) {
        this.cloudEventAttributes = cloudEventAttributes;
        return this;
    }

    /**
     * Validates the instance.
     * 
     * @throws IllegalArgumentException thrown if the instance is not valid.
     */
    public void validate() {
        if (authentication() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Missing required property authentication in model DataflowEndpointKafka"));
        } else {
            authentication().validate();
        }
        if (host() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Missing required property host in model DataflowEndpointKafka"));
        }
        if (batching() != null) {
            batching().validate();
        }
        if (tls() != null) {
            tls().validate();
        }
    }

    private static final ClientLogger LOGGER = new ClientLogger(DataflowEndpointKafka.class);

    /**
     * {@inheritDoc}
     */
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeJsonField("authentication", this.authentication);
        jsonWriter.writeStringField("host", this.host);
        jsonWriter.writeStringField("consumerGroupId", this.consumerGroupId);
        jsonWriter.writeJsonField("batching", this.batching);
        jsonWriter.writeStringField("copyMqttProperties",
            this.copyMqttProperties == null ? null : this.copyMqttProperties.toString());
        jsonWriter.writeStringField("compression", this.compression == null ? null : this.compression.toString());
        jsonWriter.writeStringField("kafkaAcks", this.kafkaAcks == null ? null : this.kafkaAcks.toString());
        jsonWriter.writeStringField("partitionStrategy",
            this.partitionStrategy == null ? null : this.partitionStrategy.toString());
        jsonWriter.writeJsonField("tls", this.tls);
        jsonWriter.writeStringField("cloudEventAttributes",
            this.cloudEventAttributes == null ? null : this.cloudEventAttributes.toString());
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of DataflowEndpointKafka from the JsonReader.
     * 
     * @param jsonReader The JsonReader being read.
     * @return An instance of DataflowEndpointKafka if the JsonReader was pointing to an instance of it, or null if it
     * was pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the DataflowEndpointKafka.
     */
    public static DataflowEndpointKafka fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            DataflowEndpointKafka deserializedDataflowEndpointKafka = new DataflowEndpointKafka();
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();

                if ("authentication".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.authentication
                        = DataflowEndpointKafkaAuthentication.fromJson(reader);
                } else if ("host".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.host = reader.getString();
                } else if ("consumerGroupId".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.consumerGroupId = reader.getString();
                } else if ("batching".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.batching = DataflowEndpointKafkaBatching.fromJson(reader);
                } else if ("copyMqttProperties".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.copyMqttProperties
                        = OperationalMode.fromString(reader.getString());
                } else if ("compression".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.compression
                        = DataflowEndpointKafkaCompression.fromString(reader.getString());
                } else if ("kafkaAcks".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.kafkaAcks
                        = DataflowEndpointKafkaAcks.fromString(reader.getString());
                } else if ("partitionStrategy".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.partitionStrategy
                        = DataflowEndpointKafkaPartitionStrategy.fromString(reader.getString());
                } else if ("tls".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.tls = TlsProperties.fromJson(reader);
                } else if ("cloudEventAttributes".equals(fieldName)) {
                    deserializedDataflowEndpointKafka.cloudEventAttributes
                        = CloudEventAttributeType.fromString(reader.getString());
                } else {
                    reader.skipChildren();
                }
            }

            return deserializedDataflowEndpointKafka;
        });
    }
}
