// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.datafactory.generated;

import com.azure.core.util.BinaryData;
import com.azure.resourcemanager.datafactory.fluent.models.SynapseNotebookActivityTypeProperties;
import com.azure.resourcemanager.datafactory.models.BigDataPoolParametrizationReference;
import com.azure.resourcemanager.datafactory.models.BigDataPoolReferenceType;
import com.azure.resourcemanager.datafactory.models.ConfigurationType;
import com.azure.resourcemanager.datafactory.models.NotebookParameter;
import com.azure.resourcemanager.datafactory.models.NotebookParameterType;
import com.azure.resourcemanager.datafactory.models.NotebookReferenceType;
import com.azure.resourcemanager.datafactory.models.SparkConfigurationParametrizationReference;
import com.azure.resourcemanager.datafactory.models.SparkConfigurationReferenceType;
import com.azure.resourcemanager.datafactory.models.SynapseNotebookReference;
import java.util.HashMap;
import java.util.Map;
import org.junit.jupiter.api.Assertions;

public final class SynapseNotebookActivityTypePropertiesTests {
    @org.junit.jupiter.api.Test
    public void testDeserialize() throws Exception {
        SynapseNotebookActivityTypeProperties model = BinaryData.fromString(
            "{\"notebook\":{\"type\":\"NotebookReference\",\"referenceName\":\"datazdpgtbytibpg\"},\"sparkPool\":{\"type\":\"BigDataPoolReference\",\"referenceName\":\"dataiujfputci\"},\"parameters\":{\"ou\":{\"value\":\"datapksjwaglhwnnfgy\",\"type\":\"float\"},\"mfqozvfeljytshj\":{\"value\":\"datamw\",\"type\":\"bool\"},\"goujsvhezhezy\":{\"value\":\"datao\",\"type\":\"int\"},\"yzjzeylthdr\":{\"value\":\"dataofayyshfv\",\"type\":\"float\"}},\"executorSize\":\"dataeidblred\",\"conf\":\"datacckticwg\",\"driverSize\":\"datavqybvgceb\",\"numExecutors\":\"datask\",\"configurationType\":\"Customized\",\"targetSparkConfiguration\":{\"type\":\"SparkConfigurationReference\",\"referenceName\":\"datatlzomsqebmfo\"},\"sparkConfig\":{\"eozgjtuhdgmshuyq\":\"datayfuliatbosnla\",\"ptoentuve\":\"datahbpr\",\"xwrets\":\"datamtlfbzlziduq\"}}")
            .toObject(SynapseNotebookActivityTypeProperties.class);
        Assertions.assertEquals(NotebookReferenceType.NOTEBOOK_REFERENCE, model.notebook().type());
        Assertions.assertEquals(BigDataPoolReferenceType.BIG_DATA_POOL_REFERENCE, model.sparkPool().type());
        Assertions.assertEquals(NotebookParameterType.FLOAT, model.parameters().get("ou").type());
        Assertions.assertEquals(ConfigurationType.CUSTOMIZED, model.configurationType());
        Assertions.assertEquals(SparkConfigurationReferenceType.SPARK_CONFIGURATION_REFERENCE,
            model.targetSparkConfiguration().type());
    }

    @org.junit.jupiter.api.Test
    public void testSerialize() throws Exception {
        SynapseNotebookActivityTypeProperties model = new SynapseNotebookActivityTypeProperties()
            .withNotebook(new SynapseNotebookReference().withType(NotebookReferenceType.NOTEBOOK_REFERENCE)
                .withReferenceName("datazdpgtbytibpg"))
            .withSparkPool(
                new BigDataPoolParametrizationReference().withType(BigDataPoolReferenceType.BIG_DATA_POOL_REFERENCE)
                    .withReferenceName("dataiujfputci"))
            .withParameters(mapOf("ou",
                new NotebookParameter().withValue("datapksjwaglhwnnfgy").withType(NotebookParameterType.FLOAT),
                "mfqozvfeljytshj", new NotebookParameter().withValue("datamw").withType(NotebookParameterType.BOOL),
                "goujsvhezhezy", new NotebookParameter().withValue("datao").withType(NotebookParameterType.INT),
                "yzjzeylthdr",
                new NotebookParameter().withValue("dataofayyshfv").withType(NotebookParameterType.FLOAT)))
            .withExecutorSize("dataeidblred")
            .withConf("datacckticwg")
            .withDriverSize("datavqybvgceb")
            .withNumExecutors("datask")
            .withConfigurationType(ConfigurationType.CUSTOMIZED)
            .withTargetSparkConfiguration(new SparkConfigurationParametrizationReference()
                .withType(SparkConfigurationReferenceType.SPARK_CONFIGURATION_REFERENCE)
                .withReferenceName("datatlzomsqebmfo"))
            .withSparkConfig(
                mapOf("eozgjtuhdgmshuyq", "datayfuliatbosnla", "ptoentuve", "datahbpr", "xwrets", "datamtlfbzlziduq"));
        model = BinaryData.fromObject(model).toObject(SynapseNotebookActivityTypeProperties.class);
        Assertions.assertEquals(NotebookReferenceType.NOTEBOOK_REFERENCE, model.notebook().type());
        Assertions.assertEquals(BigDataPoolReferenceType.BIG_DATA_POOL_REFERENCE, model.sparkPool().type());
        Assertions.assertEquals(NotebookParameterType.FLOAT, model.parameters().get("ou").type());
        Assertions.assertEquals(ConfigurationType.CUSTOMIZED, model.configurationType());
        Assertions.assertEquals(SparkConfigurationReferenceType.SPARK_CONFIGURATION_REFERENCE,
            model.targetSparkConfiguration().type());
    }

    // Use "Map.of" if available
    @SuppressWarnings("unchecked")
    private static <T> Map<String, T> mapOf(Object... inputs) {
        Map<String, T> map = new HashMap<>();
        for (int i = 0; i < inputs.length; i += 2) {
            String key = (String) inputs[i];
            T value = (T) inputs[i + 1];
            map.put(key, value);
        }
        return map;
    }
}
