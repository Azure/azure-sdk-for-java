// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.datafactory.generated;

import com.azure.core.util.BinaryData;
import com.azure.resourcemanager.datafactory.models.ActivityDependency;
import com.azure.resourcemanager.datafactory.models.ActivityOnInactiveMarkAs;
import com.azure.resourcemanager.datafactory.models.ActivityPolicy;
import com.azure.resourcemanager.datafactory.models.ActivityState;
import com.azure.resourcemanager.datafactory.models.DatabricksSparkPythonActivity;
import com.azure.resourcemanager.datafactory.models.DependencyCondition;
import com.azure.resourcemanager.datafactory.models.LinkedServiceReference;
import com.azure.resourcemanager.datafactory.models.UserProperty;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import org.junit.jupiter.api.Assertions;

public final class DatabricksSparkPythonActivityTests {
    @org.junit.jupiter.api.Test
    public void testDeserialize() throws Exception {
        DatabricksSparkPythonActivity model = BinaryData.fromString(
            "{\"type\":\"DatabricksSparkPython\",\"typeProperties\":{\"pythonFile\":\"dataktg\",\"parameters\":[\"datazzsohcaet\",\"dataepmhohq\",\"datalk\",\"dataatliwoodndub\"],\"libraries\":[{\"zdttgbsdaruwvrvx\":\"datazirgiyqzuhnb\",\"adeqslhz\":\"dataozyhu\",\"mqazolroqusrlkp\":\"datay\"},{\"ae\":\"datayqydrnws\"},{\"gzjmjdoqitfd\":\"databpdpkdlhuslqiko\"}]},\"linkedServiceName\":{\"referenceName\":\"ekbb\",\"parameters\":{\"zhrwpjtj\":\"dataoxddgjdpyhem\",\"gaiusglg\":\"datapoynbsttureqvxzl\",\"swmkxbbziffpvvg\":\"dataecsreo\"}},\"policy\":{\"timeout\":\"datamghe\",\"retry\":\"datamoetygevy\",\"retryIntervalInSeconds\":1499181439,\"secureInput\":false,\"secureOutput\":true,\"\":{\"mr\":\"datayjqklaihqrb\",\"tpydjsubtifb\":\"dataljqqbu\",\"jguwdfn\":\"datacveomdlr\"}},\"name\":\"qvuq\",\"description\":\"aowuib\",\"state\":\"Active\",\"onInactiveMarkAs\":\"Skipped\",\"dependsOn\":[{\"activity\":\"ynlvovjslxe\",\"dependencyConditions\":[\"Failed\"],\"\":{\"gzlrnfmme\":\"datahjawmrhp\",\"d\":\"datappjxtgffwq\",\"t\":\"datagfgirrzyngdvdr\",\"kqaqfbimfpnpmkdg\":\"dataqfrxggvstyxv\"}},{\"activity\":\"ndwtdorvxdwgpu\",\"dependencyConditions\":[\"Completed\",\"Completed\",\"Completed\"],\"\":{\"dzjmjkg\":\"datadwqr\",\"as\":\"dataupplcoqbouetfxza\"}},{\"activity\":\"dlokhimzfltxqpoz\",\"dependencyConditions\":[\"Skipped\"],\"\":{\"jwjnvhu\":\"datakfevhgjk\"}},{\"activity\":\"wmwvqbpazjmfqu\",\"dependencyConditions\":[\"Failed\",\"Completed\",\"Skipped\",\"Failed\"],\"\":{\"hdyifjvfrg\":\"datagjaa\",\"cwpjpkaf\":\"datanquj\",\"vuyc\":\"datakaf\",\"qsmk\":\"datatlmnrdkiqsqbdvk\"}}],\"userProperties\":[{\"name\":\"ljxnkpd\",\"value\":\"datamexrofqh\"},{\"name\":\"ptsdlcsrhttmh\",\"value\":\"datagwov\"},{\"name\":\"duzqu\",\"value\":\"datakrcwnlyqq\"},{\"name\":\"knul\",\"value\":\"dataq\"}],\"\":{\"euifndgrjnzjyghq\":\"datausmosjawbnxci\",\"ln\":\"datafs\",\"ems\":\"datavgec\"}}")
            .toObject(DatabricksSparkPythonActivity.class);
        Assertions.assertEquals("qvuq", model.name());
        Assertions.assertEquals("aowuib", model.description());
        Assertions.assertEquals(ActivityState.ACTIVE, model.state());
        Assertions.assertEquals(ActivityOnInactiveMarkAs.SKIPPED, model.onInactiveMarkAs());
        Assertions.assertEquals("ynlvovjslxe", model.dependsOn().get(0).activity());
        Assertions.assertEquals(DependencyCondition.FAILED, model.dependsOn().get(0).dependencyConditions().get(0));
        Assertions.assertEquals("ljxnkpd", model.userProperties().get(0).name());
        Assertions.assertEquals("ekbb", model.linkedServiceName().referenceName());
        Assertions.assertEquals(1499181439, model.policy().retryIntervalInSeconds());
        Assertions.assertEquals(false, model.policy().secureInput());
        Assertions.assertEquals(true, model.policy().secureOutput());
    }

    @org.junit.jupiter.api.Test
    public void testSerialize() throws Exception {
        DatabricksSparkPythonActivity model = new DatabricksSparkPythonActivity().withName("qvuq")
            .withDescription("aowuib")
            .withState(ActivityState.ACTIVE)
            .withOnInactiveMarkAs(ActivityOnInactiveMarkAs.SKIPPED)
            .withDependsOn(Arrays.asList(
                new ActivityDependency().withActivity("ynlvovjslxe")
                    .withDependencyConditions(Arrays.asList(DependencyCondition.FAILED))
                    .withAdditionalProperties(mapOf()),
                new ActivityDependency().withActivity("ndwtdorvxdwgpu")
                    .withDependencyConditions(Arrays.asList(DependencyCondition.COMPLETED,
                        DependencyCondition.COMPLETED, DependencyCondition.COMPLETED))
                    .withAdditionalProperties(mapOf()),
                new ActivityDependency().withActivity("dlokhimzfltxqpoz")
                    .withDependencyConditions(Arrays.asList(DependencyCondition.SKIPPED))
                    .withAdditionalProperties(mapOf()),
                new ActivityDependency().withActivity("wmwvqbpazjmfqu")
                    .withDependencyConditions(Arrays.asList(DependencyCondition.FAILED, DependencyCondition.COMPLETED,
                        DependencyCondition.SKIPPED, DependencyCondition.FAILED))
                    .withAdditionalProperties(mapOf())))
            .withUserProperties(Arrays.asList(new UserProperty().withName("ljxnkpd").withValue("datamexrofqh"),
                new UserProperty().withName("ptsdlcsrhttmh").withValue("datagwov"),
                new UserProperty().withName("duzqu").withValue("datakrcwnlyqq"),
                new UserProperty().withName("knul").withValue("dataq")))
            .withLinkedServiceName(new LinkedServiceReference().withReferenceName("ekbb")
                .withParameters(mapOf("zhrwpjtj", "dataoxddgjdpyhem", "gaiusglg", "datapoynbsttureqvxzl",
                    "swmkxbbziffpvvg", "dataecsreo")))
            .withPolicy(new ActivityPolicy().withTimeout("datamghe")
                .withRetry("datamoetygevy")
                .withRetryIntervalInSeconds(1499181439)
                .withSecureInput(false)
                .withSecureOutput(true)
                .withAdditionalProperties(mapOf()))
            .withPythonFile("dataktg")
            .withParameters(Arrays.asList("datazzsohcaet", "dataepmhohq", "datalk", "dataatliwoodndub"))
            .withLibraries(Arrays.asList(
                mapOf("zdttgbsdaruwvrvx", "datazirgiyqzuhnb", "adeqslhz", "dataozyhu", "mqazolroqusrlkp", "datay"),
                mapOf("ae", "datayqydrnws"), mapOf("gzjmjdoqitfd", "databpdpkdlhuslqiko")));
        model = BinaryData.fromObject(model).toObject(DatabricksSparkPythonActivity.class);
        Assertions.assertEquals("qvuq", model.name());
        Assertions.assertEquals("aowuib", model.description());
        Assertions.assertEquals(ActivityState.ACTIVE, model.state());
        Assertions.assertEquals(ActivityOnInactiveMarkAs.SKIPPED, model.onInactiveMarkAs());
        Assertions.assertEquals("ynlvovjslxe", model.dependsOn().get(0).activity());
        Assertions.assertEquals(DependencyCondition.FAILED, model.dependsOn().get(0).dependencyConditions().get(0));
        Assertions.assertEquals("ljxnkpd", model.userProperties().get(0).name());
        Assertions.assertEquals("ekbb", model.linkedServiceName().referenceName());
        Assertions.assertEquals(1499181439, model.policy().retryIntervalInSeconds());
        Assertions.assertEquals(false, model.policy().secureInput());
        Assertions.assertEquals(true, model.policy().secureOutput());
    }

    // Use "Map.of" if available
    @SuppressWarnings("unchecked")
    private static <T> Map<String, T> mapOf(Object... inputs) {
        Map<String, T> map = new HashMap<>();
        for (int i = 0; i < inputs.length; i += 2) {
            String key = (String) inputs[i];
            T value = (T) inputs[i + 1];
            map.put(key, value);
        }
        return map;
    }
}
