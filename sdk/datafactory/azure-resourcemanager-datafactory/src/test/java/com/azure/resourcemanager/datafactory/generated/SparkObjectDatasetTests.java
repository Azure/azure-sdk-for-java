// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.datafactory.generated;

import com.azure.core.util.BinaryData;
import com.azure.resourcemanager.datafactory.models.DatasetFolder;
import com.azure.resourcemanager.datafactory.models.LinkedServiceReference;
import com.azure.resourcemanager.datafactory.models.ParameterSpecification;
import com.azure.resourcemanager.datafactory.models.ParameterType;
import com.azure.resourcemanager.datafactory.models.SparkObjectDataset;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import org.junit.jupiter.api.Assertions;

public final class SparkObjectDatasetTests {
    @org.junit.jupiter.api.Test
    public void testDeserialize() throws Exception {
        SparkObjectDataset model = BinaryData.fromString(
            "{\"type\":\"brhhvipgt\",\"typeProperties\":{\"tableName\":\"datarchmyuc\",\"table\":\"datamwyvhdiyp\",\"schema\":\"dataqqq\"},\"description\":\"aaoylwhfmkbwe\",\"structure\":\"datagypjixdmobadydw\",\"schema\":\"datauwdvclsxdqdc\",\"linkedServiceName\":{\"referenceName\":\"nzi\",\"parameters\":{\"gd\":\"datargsrwxxqkwargc\"}},\"parameters\":{\"gflhdhoxur\":{\"type\":\"Int\",\"defaultValue\":\"dataiqexqwqykmv\"},\"vmbgyvxhfmu\":{\"type\":\"Float\",\"defaultValue\":\"datankvthwta\"},\"ucqico\":{\"type\":\"Object\",\"defaultValue\":\"dataz\"},\"utpdwneec\":{\"type\":\"Int\",\"defaultValue\":\"datavbeqzjdwx\"}},\"annotations\":[\"datalxug\",\"datarwvn\",\"datafaofkvfruxz\"],\"folder\":{\"name\":\"vhgykz\"},\"\":{\"gubqkfnoxhvoyj\":\"datatvymdqaymqmyrn\",\"xwejuguvnx\":\"datagfkrqsjrvpakxrde\",\"yfenrozoijoxcbpk\":\"dataohpzurnzoytkbea\",\"pan\":\"datawseacbtaxd\"}}")
            .toObject(SparkObjectDataset.class);
        Assertions.assertEquals("aaoylwhfmkbwe", model.description());
        Assertions.assertEquals("nzi", model.linkedServiceName().referenceName());
        Assertions.assertEquals(ParameterType.INT, model.parameters().get("gflhdhoxur").type());
        Assertions.assertEquals("vhgykz", model.folder().name());
    }

    @org.junit.jupiter.api.Test
    public void testSerialize() throws Exception {
        SparkObjectDataset model = new SparkObjectDataset().withDescription("aaoylwhfmkbwe")
            .withStructure("datagypjixdmobadydw")
            .withSchema("datauwdvclsxdqdc")
            .withLinkedServiceName(
                new LinkedServiceReference().withReferenceName("nzi").withParameters(mapOf("gd", "datargsrwxxqkwargc")))
            .withParameters(mapOf("gflhdhoxur",
                new ParameterSpecification().withType(ParameterType.INT).withDefaultValue("dataiqexqwqykmv"),
                "vmbgyvxhfmu",
                new ParameterSpecification().withType(ParameterType.FLOAT).withDefaultValue("datankvthwta"), "ucqico",
                new ParameterSpecification().withType(ParameterType.OBJECT).withDefaultValue("dataz"), "utpdwneec",
                new ParameterSpecification().withType(ParameterType.INT).withDefaultValue("datavbeqzjdwx")))
            .withAnnotations(Arrays.asList("datalxug", "datarwvn", "datafaofkvfruxz"))
            .withFolder(new DatasetFolder().withName("vhgykz"))
            .withTableName("datarchmyuc")
            .withTable("datamwyvhdiyp")
            .withSchemaTypePropertiesSchema("dataqqq");
        model = BinaryData.fromObject(model).toObject(SparkObjectDataset.class);
        Assertions.assertEquals("aaoylwhfmkbwe", model.description());
        Assertions.assertEquals("nzi", model.linkedServiceName().referenceName());
        Assertions.assertEquals(ParameterType.INT, model.parameters().get("gflhdhoxur").type());
        Assertions.assertEquals("vhgykz", model.folder().name());
    }

    // Use "Map.of" if available
    @SuppressWarnings("unchecked")
    private static <T> Map<String, T> mapOf(Object... inputs) {
        Map<String, T> map = new HashMap<>();
        for (int i = 0; i < inputs.length; i += 2) {
            String key = (String) inputs[i];
            T value = (T) inputs[i + 1];
            map.put(key, value);
        }
        return map;
    }
}
