// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.voicelive.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.annotation.Generated;
import com.azure.json.JsonReader;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.List;

/**
 * Configuration for LLM-based filler response generation.
 * Uses LLM to generate context-aware filler responses when any trigger condition is met.
 */
@Fluent
public final class LlmFillerResponseConfig extends FillerResponseConfigBase {

    /*
     * The type of filler response configuration.
     */
    @Generated
    private FillerResponseConfigType type = FillerResponseConfigType.LLM_FILLER;

    /*
     * The model to use for LLM-based filler generation. Default is gpt-4.1-mini.
     */
    @Generated
    private String model;

    /*
     * Custom instructions for generating filler responses. If not provided, a default prompt is used.
     */
    @Generated
    private String instructions;

    /*
     * Maximum number of tokens to generate for the filler response.
     */
    @Generated
    private Integer maxCompletionTokens;

    /**
     * Creates an instance of LlmFillerResponseConfig class.
     */
    @Generated
    public LlmFillerResponseConfig() {
    }

    /**
     * Get the type property: The type of filler response configuration.
     *
     * @return the type value.
     */
    @Generated
    @Override
    public FillerResponseConfigType getType() {
        return this.type;
    }

    /**
     * Get the model property: The model to use for LLM-based filler generation. Default is gpt-4.1-mini.
     *
     * @return the model value.
     */
    @Generated
    public String getModel() {
        return this.model;
    }

    /**
     * Set the model property: The model to use for LLM-based filler generation. Default is gpt-4.1-mini.
     *
     * @param model the model value to set.
     * @return the LlmFillerResponseConfig object itself.
     */
    @Generated
    public LlmFillerResponseConfig setModel(String model) {
        this.model = model;
        return this;
    }

    /**
     * Get the instructions property: Custom instructions for generating filler responses. If not provided, a default
     * prompt is used.
     *
     * @return the instructions value.
     */
    @Generated
    public String getInstructions() {
        return this.instructions;
    }

    /**
     * Set the instructions property: Custom instructions for generating filler responses. If not provided, a default
     * prompt is used.
     *
     * @param instructions the instructions value to set.
     * @return the LlmFillerResponseConfig object itself.
     */
    @Generated
    public LlmFillerResponseConfig setInstructions(String instructions) {
        this.instructions = instructions;
        return this;
    }

    /**
     * Get the maxCompletionTokens property: Maximum number of tokens to generate for the filler response.
     *
     * @return the maxCompletionTokens value.
     */
    @Generated
    public Integer getMaxCompletionTokens() {
        return this.maxCompletionTokens;
    }

    /**
     * Set the maxCompletionTokens property: Maximum number of tokens to generate for the filler response.
     *
     * @param maxCompletionTokens the maxCompletionTokens value to set.
     * @return the LlmFillerResponseConfig object itself.
     */
    @Generated
    public LlmFillerResponseConfig setMaxCompletionTokens(Integer maxCompletionTokens) {
        this.maxCompletionTokens = maxCompletionTokens;
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public LlmFillerResponseConfig setTriggers(List<FillerTrigger> triggers) {
        super.setTriggers(triggers);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public LlmFillerResponseConfig setLatencyThresholdMs(Integer latencyThresholdMs) {
        super.setLatencyThresholdMs(latencyThresholdMs);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeArrayField("triggers", getTriggers(),
            (writer, element) -> writer.writeString(element == null ? null : element.toString()));
        jsonWriter.writeNumberField("latency_threshold_ms", getLatencyThresholdMs());
        jsonWriter.writeStringField("type", this.type == null ? null : this.type.toString());
        jsonWriter.writeStringField("model", this.model);
        jsonWriter.writeStringField("instructions", this.instructions);
        jsonWriter.writeNumberField("max_completion_tokens", this.maxCompletionTokens);
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of LlmFillerResponseConfig from the JsonReader.
     *
     * @param jsonReader The JsonReader being read.
     * @return An instance of LlmFillerResponseConfig if the JsonReader was pointing to an instance of it, or null if it
     * was pointing to JSON null.
     * @throws IOException If an error occurs while reading the LlmFillerResponseConfig.
     */
    @Generated
    public static LlmFillerResponseConfig fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            LlmFillerResponseConfig deserializedLlmFillerResponseConfig = new LlmFillerResponseConfig();
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();
                if ("triggers".equals(fieldName)) {
                    List<FillerTrigger> triggers
                        = reader.readArray(reader1 -> FillerTrigger.fromString(reader1.getString()));
                    deserializedLlmFillerResponseConfig.setTriggers(triggers);
                } else if ("latency_threshold_ms".equals(fieldName)) {
                    deserializedLlmFillerResponseConfig.setLatencyThresholdMs(reader.getNullable(JsonReader::getInt));
                } else if ("type".equals(fieldName)) {
                    deserializedLlmFillerResponseConfig.type = FillerResponseConfigType.fromString(reader.getString());
                } else if ("model".equals(fieldName)) {
                    deserializedLlmFillerResponseConfig.model = reader.getString();
                } else if ("instructions".equals(fieldName)) {
                    deserializedLlmFillerResponseConfig.instructions = reader.getString();
                } else if ("max_completion_tokens".equals(fieldName)) {
                    deserializedLlmFillerResponseConfig.maxCompletionTokens = reader.getNullable(JsonReader::getInt);
                } else {
                    reader.skipChildren();
                }
            }
            return deserializedLlmFillerResponseConfig;
        });
    }
}
