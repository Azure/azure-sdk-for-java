// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.vision.face;

import com.azure.ai.vision.face.implementation.FaceClientImpl;
import com.azure.ai.vision.face.implementation.models.DetectFromUrlImplOptions;
import com.azure.ai.vision.face.implementation.models.DetectFromUrlRequest;
import com.azure.ai.vision.face.implementation.models.FindSimilarFromLargeFaceListRequest;
import com.azure.ai.vision.face.implementation.models.FindSimilarRequest;
import com.azure.ai.vision.face.implementation.models.GroupRequest;
import com.azure.ai.vision.face.implementation.models.IdentifyFromLargePersonGroupRequest;
import com.azure.ai.vision.face.implementation.models.VerifyFaceToFaceRequest;
import com.azure.ai.vision.face.implementation.models.VerifyFromLargePersonGroupRequest;
import com.azure.ai.vision.face.models.FaceAttributeType;
import com.azure.ai.vision.face.models.FaceDetectionModel;
import com.azure.ai.vision.face.models.FaceDetectionResult;
import com.azure.ai.vision.face.models.FaceFindSimilarResult;
import com.azure.ai.vision.face.models.FaceGroupingResult;
import com.azure.ai.vision.face.models.FaceIdentificationResult;
import com.azure.ai.vision.face.models.FaceRecognitionModel;
import com.azure.ai.vision.face.models.FaceVerificationResult;
import com.azure.ai.vision.face.models.FindSimilarMatchMode;
import com.azure.core.annotation.Generated;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceClient;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.exception.ClientAuthenticationException;
import com.azure.core.exception.HttpResponseException;
import com.azure.core.exception.ResourceModifiedException;
import com.azure.core.exception.ResourceNotFoundException;
import com.azure.core.http.rest.RequestOptions;
import com.azure.core.http.rest.Response;
import com.azure.core.util.BinaryData;
import com.azure.core.util.FluxUtil;
import com.azure.core.util.serializer.TypeReference;
import java.util.List;
import java.util.Objects;
import java.util.stream.Collectors;
import reactor.core.publisher.Mono;
import com.azure.ai.vision.face.models.DetectOptions;
import static com.azure.ai.vision.face.implementation.ClientUtils.addOptionalQueryParameterForDetection;
import static com.azure.ai.vision.face.implementation.ClientUtils.addRequiredQueryParameterForDetection;

/**
 * Initializes a new instance of the asynchronous FaceClient type.
 */
@ServiceClient(builder = FaceClientBuilder.class, isAsync = true)
public final class FaceAsyncClient {

    @Generated
    private final FaceClientImpl serviceClient;

    /**
     * Initializes an instance of FaceAsyncClient class.
     *
     * @param serviceClient the service client implementation.
     */
    @Generated
    FaceAsyncClient(FaceClientImpl serviceClient) {
        this.serviceClient = serviceClient;
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId array contains the
     * faces created by Detect.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar for more
     * details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId: String (Required)
     *     maxNumOfCandidatesReturned: Integer (Optional)
     *     mode: String(matchPerson/matchFace) (Optional)
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         confidence: double (Required)
     *         faceId: String (Optional)
     *         persistedFaceId: String (Optional)
     *     }
     * ]
     * }</pre>
     *
     * @param findSimilarRequest The findSimilarRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> findSimilarWithResponse(BinaryData findSimilarRequest,
        RequestOptions requestOptions) {
        return this.serviceClient.findSimilarWithResponseAsync(findSimilarRequest, requestOptions);
    }

    /**
     * Verify whether two faces belong to a same person.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/verify-face-to-face for
     * more details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId1: String (Required)
     *     faceId2: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     isIdentical: boolean (Required)
     *     confidence: double (Required)
     * }
     * }</pre>
     *
     * @param verifyFaceToFaceRequest The verifyFaceToFaceRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return verify result along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> verifyFaceToFaceWithResponse(BinaryData verifyFaceToFaceRequest,
        RequestOptions requestOptions) {
        return this.serviceClient.verifyFaceToFaceWithResponseAsync(verifyFaceToFaceRequest, requestOptions);
    }

    /**
     * Divide candidate faces into groups based on face similarity.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/group for more details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     groups (Required): [
     *          (Required)[
     *             String (Required)
     *         ]
     *     ]
     *     messyGroup (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     *
     * @param groupRequest The groupRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return response body for group face operation along with {@link Response} on successful completion of
     * {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> groupWithResponse(BinaryData groupRequest, RequestOptions requestOptions) {
        return this.serviceClient.groupWithResponseAsync(groupRequest, requestOptions);
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId array contains the
     * faces created by Detect.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar for more
     * details.
     *
     * @param faceId faceId of the query face. User needs to call "Detect" first to get a valid faceId. Note that this
     * faceId is not persisted and will expire 24 hours after the detection call.
     * @param faceIds An array of candidate faceIds. All of them are created by "Detect" and the faceIds will expire 24
     * hours after the detection call. The number of faceIds is limited to 1000.
     * @param maxNumOfCandidatesReturned The number of top similar faces returned. The valid range is [1, 1000]. Default
     * value is 20.
     * @param mode Similar face searching mode. It can be 'matchPerson' or 'matchFace'. Default value is 'matchPerson'.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceFindSimilarResult>> findSimilar(String faceId, List<String> faceIds,
        Integer maxNumOfCandidatesReturned, FindSimilarMatchMode mode) {
        // Generated convenience method for findSimilarWithResponse
        RequestOptions requestOptions = new RequestOptions();
        FindSimilarRequest findSimilarRequestObj
            = new FindSimilarRequest(faceId, faceIds).setMaxNumOfCandidatesReturned(maxNumOfCandidatesReturned)
                .setMode(mode);
        BinaryData findSimilarRequest = BinaryData.fromObject(findSimilarRequestObj);
        return findSimilarWithResponse(findSimilarRequest, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_FIND_SIMILAR_RESULT));
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId array contains the
     * faces created by Detect.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar for more
     * details.
     *
     * @param faceId faceId of the query face. User needs to call "Detect" first to get a valid faceId. Note that this
     * faceId is not persisted and will expire 24 hours after the detection call.
     * @param faceIds An array of candidate faceIds. All of them are created by "Detect" and the faceIds will expire 24
     * hours after the detection call. The number of faceIds is limited to 1000.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceFindSimilarResult>> findSimilar(String faceId, List<String> faceIds) {
        // Generated convenience method for findSimilarWithResponse
        RequestOptions requestOptions = new RequestOptions();
        FindSimilarRequest findSimilarRequestObj = new FindSimilarRequest(faceId, faceIds);
        BinaryData findSimilarRequest = BinaryData.fromObject(findSimilarRequestObj);
        return findSimilarWithResponse(findSimilarRequest, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_FIND_SIMILAR_RESULT));
    }

    /**
     * Verify whether two faces belong to a same person.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/verify-face-to-face for
     * more details.
     *
     * @param faceId1 The faceId of one face, come from "Detect".
     * @param faceId2 The faceId of another face, come from "Detect".
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return verify result on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<FaceVerificationResult> verifyFaceToFace(String faceId1, String faceId2) {
        // Generated convenience method for verifyFaceToFaceWithResponse
        RequestOptions requestOptions = new RequestOptions();
        VerifyFaceToFaceRequest verifyFaceToFaceRequestObj = new VerifyFaceToFaceRequest(faceId1, faceId2);
        BinaryData verifyFaceToFaceRequest = BinaryData.fromObject(verifyFaceToFaceRequestObj);
        return verifyFaceToFaceWithResponse(verifyFaceToFaceRequest, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(FaceVerificationResult.class));
    }

    /**
     * Divide candidate faces into groups based on face similarity.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/group for more details.
     *
     * @param faceIds Array of candidate faceIds created by "Detect". The maximum is 1000 faces.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return response body for group face operation on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<FaceGroupingResult> group(List<String> faceIds) {
        // Generated convenience method for groupWithResponse
        RequestOptions requestOptions = new RequestOptions();
        GroupRequest groupRequestObj = new GroupRequest(faceIds);
        BinaryData groupRequest = BinaryData.fromObject(groupRequestObj);
        return groupWithResponse(groupRequest, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(FaceGroupingResult.class));
    }

    @Generated
    private static final TypeReference<List<FaceFindSimilarResult>> TYPE_REFERENCE_LIST_FACE_FIND_SIMILAR_RESULT
        = new TypeReference<List<FaceFindSimilarResult>>() {
        };

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-detection-operations/detect-from-url for more
     * details.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. 'detection_03' is recommended since its accuracy is improved on smaller faces (64x64
     * pixels) and rotated face orientations. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     url: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     *
     * @param detectFromUrlRequest The detectFromUrlRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    Mono<Response<BinaryData>> detectFromUrlImplWithResponse(BinaryData detectFromUrlRequest,
        RequestOptions requestOptions) {
        return this.serviceClient.detectFromUrlImplWithResponseAsync(detectFromUrlRequest, requestOptions);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-detection-operations/detect for more details.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. 'detection_03' is recommended since its accuracy is improved on smaller faces (64x64
     * pixels) and rotated face orientations. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * BinaryData
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     *
     * @param imageContent The input image binary.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    Mono<Response<BinaryData>> detectImplWithResponse(BinaryData imageContent, RequestOptions requestOptions) {
        return this.serviceClient.detectImplWithResponseAsync(imageContent, requestOptions);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-detection-operations/detect-from-url for more
     * details.
     *
     * @param options Options for detectFromUrlImpl API.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    Mono<List<FaceDetectionResult>> detectFromUrlImpl(DetectFromUrlImplOptions options) {
        // Generated convenience method for detectFromUrlImplWithResponse
        RequestOptions requestOptions = new RequestOptions();
        FaceDetectionModel detectionModel = options.getDetectionModel();
        FaceRecognitionModel recognitionModel = options.getRecognitionModel();
        Boolean returnFaceId = options.isReturnFaceId();
        List<FaceAttributeType> returnFaceAttributes = options.getReturnFaceAttributes();
        Boolean returnFaceLandmarks = options.isReturnFaceLandmarks();
        Boolean returnRecognitionModel = options.isReturnRecognitionModel();
        Integer faceIdTimeToLive = options.getFaceIdTimeToLive();
        DetectFromUrlRequest detectFromUrlRequestObj = new DetectFromUrlRequest(options.getUrl());
        BinaryData detectFromUrlRequest = BinaryData.fromObject(detectFromUrlRequestObj);
        if (detectionModel != null) {
            requestOptions.addQueryParam("detectionModel", detectionModel.toString(), false);
        }
        if (recognitionModel != null) {
            requestOptions.addQueryParam("recognitionModel", recognitionModel.toString(), false);
        }
        if (returnFaceId != null) {
            requestOptions.addQueryParam("returnFaceId", String.valueOf(returnFaceId), false);
        }
        if (returnFaceAttributes != null) {
            requestOptions.addQueryParam("returnFaceAttributes",
                returnFaceAttributes.stream()
                    .map(paramItemValue -> Objects.toString(paramItemValue, ""))
                    .collect(Collectors.joining(",")),
                false);
        }
        if (returnFaceLandmarks != null) {
            requestOptions.addQueryParam("returnFaceLandmarks", String.valueOf(returnFaceLandmarks), false);
        }
        if (returnRecognitionModel != null) {
            requestOptions.addQueryParam("returnRecognitionModel", String.valueOf(returnRecognitionModel), false);
        }
        if (faceIdTimeToLive != null) {
            requestOptions.addQueryParam("faceIdTimeToLive", String.valueOf(faceIdTimeToLive), false);
        }
        return detectFromUrlImplWithResponse(detectFromUrlRequest, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_DETECTION_RESULT));
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-detection-operations/detect for more details.
     *
     * @param imageContent The input image binary.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'. The default value is 'detection_01'.
     * 'detection_03' is recommended since its accuracy is improved on smaller faces (64x64 pixels) and rotated face
     * orientations.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'. The default value is
     * 'recognition_01'. 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared
     * with 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not. The default value is true.
     * @param returnFaceAttributes Analyze and return the one or more specified face attributes in the comma-separated
     * string like 'returnFaceAttributes=headPose,glasses'. Face attribute analysis has additional computational and
     * time cost.
     * @param returnFaceLandmarks Return face landmarks of the detected faces or not. The default value is false.
     * @param returnRecognitionModel Return 'recognitionModel' or not. The default value is false. This is only
     * applicable when returnFaceId = true.
     * @param faceIdTimeToLive The number of seconds for the face ID being cached. Supported range from 60 seconds up to
     * 86400 seconds. The default value is 86400 (24 hours).
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    Mono<List<FaceDetectionResult>> detectImpl(BinaryData imageContent, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, Boolean returnFaceId, List<FaceAttributeType> returnFaceAttributes,
        Boolean returnFaceLandmarks, Boolean returnRecognitionModel, Integer faceIdTimeToLive) {
        // Generated convenience method for detectImplWithResponse
        RequestOptions requestOptions = new RequestOptions();
        if (detectionModel != null) {
            requestOptions.addQueryParam("detectionModel", detectionModel.toString(), false);
        }
        if (recognitionModel != null) {
            requestOptions.addQueryParam("recognitionModel", recognitionModel.toString(), false);
        }
        if (returnFaceId != null) {
            requestOptions.addQueryParam("returnFaceId", String.valueOf(returnFaceId), false);
        }
        if (returnFaceAttributes != null) {
            requestOptions.addQueryParam("returnFaceAttributes",
                returnFaceAttributes.stream()
                    .map(paramItemValue -> Objects.toString(paramItemValue, ""))
                    .collect(Collectors.joining(",")),
                false);
        }
        if (returnFaceLandmarks != null) {
            requestOptions.addQueryParam("returnFaceLandmarks", String.valueOf(returnFaceLandmarks), false);
        }
        if (returnRecognitionModel != null) {
            requestOptions.addQueryParam("returnRecognitionModel", String.valueOf(returnRecognitionModel), false);
        }
        if (faceIdTimeToLive != null) {
            requestOptions.addQueryParam("faceIdTimeToLive", String.valueOf(faceIdTimeToLive), false);
        }
        return detectImplWithResponse(imageContent, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_DETECTION_RESULT));
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-detection-operations/detect for more details.
     *
     * @param imageContent The input image binary.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    Mono<List<FaceDetectionResult>> detectImpl(BinaryData imageContent) {
        // Generated convenience method for detectImplWithResponse
        RequestOptions requestOptions = new RequestOptions();
        return detectImplWithResponse(imageContent, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_DETECTION_RESULT));
    }

    @Generated
    private static final TypeReference<List<FaceDetectionResult>> TYPE_REFERENCE_LIST_FACE_DETECTION_RESULT
        = new TypeReference<List<FaceDetectionResult>>() {
        };

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param imageContent The input image binary.
     * @param options Options for detect API.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(BinaryData imageContent, DetectOptions options) {
        return this.detectImpl(imageContent, options.getDetectionModel(), options.getRecognitionModel(),
            options.isReturnFaceId(), options.getReturnFaceAttributes(), options.isReturnFaceLandmarks(),
            options.isReturnRecognitionModel(), options.getFaceIdTimeToLive());
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param imageContent The input image binary.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * @param returnFaceAttributes Analyze and return the one or more specified face attributes in the comma-separated
     * string like 'returnFaceAttributes=headPose,glasses'. Face attribute analysis has additional computational and
     * time cost.
     * @param returnFaceLandmarks Return face landmarks of the detected faces or not. The default value is false.
     * @param returnRecognitionModel Return 'recognitionModel' or not. The default value is false.
     * @param faceIdTimeToLive The number of seconds for the face ID being cached. Supported range from 60 seconds up to
     * 86400 seconds. The default value is 86400 (24 hours).
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(BinaryData imageContent, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId, List<FaceAttributeType> returnFaceAttributes,
        Boolean returnFaceLandmarks, Boolean returnRecognitionModel, Integer faceIdTimeToLive) {
        return this.detectImpl(imageContent, detectionModel, recognitionModel, returnFaceId, returnFaceAttributes,
            returnFaceLandmarks, returnRecognitionModel, faceIdTimeToLive);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param imageContent The input image binary.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * 86400 seconds. The default value is 86400 (24 hours).
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(BinaryData imageContent, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId) {
        return this.detectImpl(imageContent, detectionModel, recognitionModel, returnFaceId, null, null, null, null);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param imageContent The input image binary.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * @param returnFaceAttributes Analyze and return the one or more specified face attributes in the comma-separated
     * string like 'returnFaceAttributes=headPose,glasses'. Face attribute analysis has additional computational and
     * time cost.
     * 86400 seconds. The default value is 86400 (24 hours).
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(BinaryData imageContent, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId, List<FaceAttributeType> returnFaceAttributes) {
        return this.detectImpl(imageContent, detectionModel, recognitionModel, returnFaceId, returnFaceAttributes, null,
            null, null);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param url the URL of input image.
     * @param options Options for detect API.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(String url, DetectOptions options) {
        return this.detect(url, options.getDetectionModel(), options.getRecognitionModel(), options.isReturnFaceId(),
            options.getReturnFaceAttributes(), options.isReturnFaceLandmarks(), options.isReturnRecognitionModel(),
            options.getFaceIdTimeToLive());
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param url the URL of input image.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * @param returnFaceAttributes Analyze and return the one or more specified face attributes in the comma-separated
     * string like 'returnFaceAttributes=headPose,glasses'. Face attribute analysis has additional computational and
     * time cost.
     * @param returnFaceLandmarks Return face landmarks of the detected faces or not. The default value is false.
     * @param returnRecognitionModel Return 'recognitionModel' or not. The default value is false.
     * @param faceIdTimeToLive The number of seconds for the face ID being cached. Supported range from 60 seconds up to
     * 86400 seconds. The default value is 86400 (24 hours).
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(String url, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId, List<FaceAttributeType> returnFaceAttributes,
        Boolean returnFaceLandmarks, Boolean returnRecognitionModel, Integer faceIdTimeToLive) {
        RequestOptions requestOptions = new RequestOptions();
        addRequiredQueryParameterForDetection(requestOptions, detectionModel, recognitionModel, returnFaceId);
        addOptionalQueryParameterForDetection(requestOptions, returnFaceAttributes, returnFaceLandmarks,
            returnRecognitionModel, faceIdTimeToLive);
        DetectFromUrlRequest requestObj = new DetectFromUrlRequest(url);
        BinaryData request = BinaryData.fromObject(requestObj);
        return detectFromUrlImplWithResponse(request, requestOptions).flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_DETECTION_RESULT));
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param url the URL of input image.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(String url, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId) {
        return this.detect(url, detectionModel, recognitionModel, returnFaceId, null, null, null, null);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     *
     * &gt; [!IMPORTANT]
     * &gt; To mitigate potential misuse that can subject people to stereotyping, discrimination, or unfair denial of
     * services, we are retiring Face API attributes that predict emotion, gender, age, smile, facial hair, hair, and
     * makeup. Read more about this decision
     * https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     *
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask and headPose only) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     *
     * @param url the URL of input image.
     * @param detectionModel The 'detectionModel' associated with the detected faceIds. Supported 'detectionModel'
     * values include 'detection_01', 'detection_02' and 'detection_03'.
     * @param recognitionModel The 'recognitionModel' associated with the detected faceIds. Supported 'recognitionModel'
     * values include 'recognition_01', 'recognition_02', 'recognition_03' or 'recognition_04'.
     * 'recognition_04' is recommended since its accuracy is improved on faces wearing masks compared with
     * 'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'.
     * @param returnFaceId Return faceIds of the detected faces or not.
     * @param returnFaceAttributes Analyze and return the one or more specified face attributes in the comma-separated
     * string like 'returnFaceAttributes=headPose,glasses'. Face attribute analysis has additional computational and
     * time cost.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceDetectionResult>> detect(String url, FaceDetectionModel detectionModel,
        FaceRecognitionModel recognitionModel, boolean returnFaceId, List<FaceAttributeType> returnFaceAttributes) {
        return this.detect(url, detectionModel, recognitionModel, returnFaceId, returnFaceAttributes, null, null, null);
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a Large Face List. A 'largeFaceListId' is
     * created by Create Large Face List.
     *
     * Please refer to
     * https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar-from-large-face-list for more
     * details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId: String (Required)
     *     maxNumOfCandidatesReturned: Integer (Optional)
     *     mode: String(matchPerson/matchFace) (Optional)
     *     largeFaceListId: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         confidence: double (Required)
     *         faceId: String (Optional)
     *         persistedFaceId: String (Optional)
     *     }
     * ]
     * }</pre>
     *
     * @param findSimilarFromLargeFaceListRequest The findSimilarFromLargeFaceListRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> findSimilarFromLargeFaceListWithResponse(
        BinaryData findSimilarFromLargeFaceListRequest, RequestOptions requestOptions) {
        return this.serviceClient.findSimilarFromLargeFaceListWithResponseAsync(findSimilarFromLargeFaceListRequest,
            requestOptions);
    }

    /**
     * 1-to-many identification to find the closest matches of the specific query person face from a Large Person Group.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/identify-from-person-group
     * for more details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     *     largePersonGroupId: String (Required)
     *     maxNumOfCandidatesReturned: Integer (Optional)
     *     confidenceThreshold: Double (Optional)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Required)
     *         candidates (Required): [
     *              (Required){
     *                 personId: String (Required)
     *                 confidence: double (Required)
     *             }
     *         ]
     *     }
     * ]
     * }</pre>
     *
     * @param identifyFromLargePersonGroupRequest The identifyFromLargePersonGroupRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> identifyFromLargePersonGroupWithResponse(
        BinaryData identifyFromLargePersonGroupRequest, RequestOptions requestOptions) {
        return this.serviceClient.identifyFromLargePersonGroupWithResponseAsync(identifyFromLargePersonGroupRequest,
            requestOptions);
    }

    /**
     * Verify whether a face belongs to a person in a Large Person Group.
     *
     * Please refer to
     * https://learn.microsoft.com/rest/api/face/face-recognition-operations/verify-from-large-person-group for more
     * details.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId: String (Required)
     *     largePersonGroupId: String (Required)
     *     personId: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     isIdentical: boolean (Required)
     *     confidence: double (Required)
     * }
     * }</pre>
     *
     * @param verifyFromLargePersonGroupRequest The verifyFromLargePersonGroupRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return verify result along with {@link Response} on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> verifyFromLargePersonGroupWithResponse(
        BinaryData verifyFromLargePersonGroupRequest, RequestOptions requestOptions) {
        return this.serviceClient.verifyFromLargePersonGroupWithResponseAsync(verifyFromLargePersonGroupRequest,
            requestOptions);
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a Large Face List. A 'largeFaceListId' is
     * created by Create Large Face List.
     *
     * Please refer to
     * https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar-from-large-face-list for more
     * details.
     *
     * @param faceId faceId of the query face. User needs to call "Detect" first to get a valid faceId. Note that this
     * faceId is not persisted and will expire 24 hours after the detection call.
     * @param largeFaceListId An existing user-specified unique candidate Large Face List, created in "Create Large Face
     * List". Large Face List contains a set of persistedFaceIds which are persisted and will never expire.
     * @param maxNumOfCandidatesReturned The number of top similar faces returned. The valid range is [1, 1000]. Default
     * value is 20.
     * @param mode Similar face searching mode. It can be 'matchPerson' or 'matchFace'. Default value is 'matchPerson'.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceFindSimilarResult>> findSimilarFromLargeFaceList(String faceId, String largeFaceListId,
        Integer maxNumOfCandidatesReturned, FindSimilarMatchMode mode) {
        // Generated convenience method for findSimilarFromLargeFaceListWithResponse
        RequestOptions requestOptions = new RequestOptions();
        FindSimilarFromLargeFaceListRequest findSimilarFromLargeFaceListRequestObj
            = new FindSimilarFromLargeFaceListRequest(faceId, largeFaceListId)
                .setMaxNumOfCandidatesReturned(maxNumOfCandidatesReturned)
                .setMode(mode);
        BinaryData findSimilarFromLargeFaceListRequest = BinaryData.fromObject(findSimilarFromLargeFaceListRequestObj);
        return findSimilarFromLargeFaceListWithResponse(findSimilarFromLargeFaceListRequest, requestOptions)
            .flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_FIND_SIMILAR_RESULT));
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a Large Face List. A 'largeFaceListId' is
     * created by Create Large Face List.
     *
     * Please refer to
     * https://learn.microsoft.com/rest/api/face/face-recognition-operations/find-similar-from-large-face-list for more
     * details.
     *
     * @param faceId faceId of the query face. User needs to call "Detect" first to get a valid faceId. Note that this
     * faceId is not persisted and will expire 24 hours after the detection call.
     * @param largeFaceListId An existing user-specified unique candidate Large Face List, created in "Create Large Face
     * List". Large Face List contains a set of persistedFaceIds which are persisted and will never expire.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceFindSimilarResult>> findSimilarFromLargeFaceList(String faceId, String largeFaceListId) {
        // Generated convenience method for findSimilarFromLargeFaceListWithResponse
        RequestOptions requestOptions = new RequestOptions();
        FindSimilarFromLargeFaceListRequest findSimilarFromLargeFaceListRequestObj
            = new FindSimilarFromLargeFaceListRequest(faceId, largeFaceListId);
        BinaryData findSimilarFromLargeFaceListRequest = BinaryData.fromObject(findSimilarFromLargeFaceListRequestObj);
        return findSimilarFromLargeFaceListWithResponse(findSimilarFromLargeFaceListRequest, requestOptions)
            .flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_FIND_SIMILAR_RESULT));
    }

    /**
     * 1-to-many identification to find the closest matches of the specific query person face from a Large Person Group.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/identify-from-person-group
     * for more details.
     *
     * @param faceIds Array of query faces faceIds, created by the "Detect". Each of the faces are identified
     * independently. The valid number of faceIds is between [1, 10].
     * @param largePersonGroupId largePersonGroupId of the target Large Person Group, created by "Create Large Person
     * Group". Parameter personGroupId and largePersonGroupId should not be provided at the same time.
     * @param maxNumOfCandidatesReturned The range of maxNumOfCandidatesReturned is between 1 and 100. Default value is
     * 10.
     * @param confidenceThreshold Customized identification confidence threshold, in the range of [0, 1]. Advanced user
     * can tweak this value to override default internal threshold for better precision on their scenario data. Note
     * there is no guarantee of this threshold value working on other data and after algorithm updates.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceIdentificationResult>> identifyFromLargePersonGroup(List<String> faceIds,
        String largePersonGroupId, Integer maxNumOfCandidatesReturned, Double confidenceThreshold) {
        // Generated convenience method for identifyFromLargePersonGroupWithResponse
        RequestOptions requestOptions = new RequestOptions();
        IdentifyFromLargePersonGroupRequest identifyFromLargePersonGroupRequestObj
            = new IdentifyFromLargePersonGroupRequest(faceIds, largePersonGroupId)
                .setMaxNumOfCandidatesReturned(maxNumOfCandidatesReturned)
                .setConfidenceThreshold(confidenceThreshold);
        BinaryData identifyFromLargePersonGroupRequest = BinaryData.fromObject(identifyFromLargePersonGroupRequestObj);
        return identifyFromLargePersonGroupWithResponse(identifyFromLargePersonGroupRequest, requestOptions)
            .flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_IDENTIFICATION_RESULT));
    }

    /**
     * 1-to-many identification to find the closest matches of the specific query person face from a Large Person Group.
     *
     * Please refer to https://learn.microsoft.com/rest/api/face/face-recognition-operations/identify-from-person-group
     * for more details.
     *
     * @param faceIds Array of query faces faceIds, created by the "Detect". Each of the faces are identified
     * independently. The valid number of faceIds is between [1, 10].
     * @param largePersonGroupId largePersonGroupId of the target Large Person Group, created by "Create Large Person
     * Group". Parameter personGroupId and largePersonGroupId should not be provided at the same time.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<List<FaceIdentificationResult>> identifyFromLargePersonGroup(List<String> faceIds,
        String largePersonGroupId) {
        // Generated convenience method for identifyFromLargePersonGroupWithResponse
        RequestOptions requestOptions = new RequestOptions();
        IdentifyFromLargePersonGroupRequest identifyFromLargePersonGroupRequestObj
            = new IdentifyFromLargePersonGroupRequest(faceIds, largePersonGroupId);
        BinaryData identifyFromLargePersonGroupRequest = BinaryData.fromObject(identifyFromLargePersonGroupRequestObj);
        return identifyFromLargePersonGroupWithResponse(identifyFromLargePersonGroupRequest, requestOptions)
            .flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(TYPE_REFERENCE_LIST_FACE_IDENTIFICATION_RESULT));
    }

    /**
     * Verify whether a face belongs to a person in a Large Person Group.
     *
     * Please refer to
     * https://learn.microsoft.com/rest/api/face/face-recognition-operations/verify-from-large-person-group for more
     * details.
     *
     * @param faceId The faceId of the face, come from "Detect".
     * @param largePersonGroupId Using existing largePersonGroupId and personId for fast loading a specified person.
     * largePersonGroupId is created in "Create Large Person Group".
     * @param personId Specify a certain person in Large Person Group.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return verify result on successful completion of {@link Mono}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<FaceVerificationResult> verifyFromLargePersonGroup(String faceId, String largePersonGroupId,
        String personId) {
        // Generated convenience method for verifyFromLargePersonGroupWithResponse
        RequestOptions requestOptions = new RequestOptions();
        VerifyFromLargePersonGroupRequest verifyFromLargePersonGroupRequestObj
            = new VerifyFromLargePersonGroupRequest(faceId, largePersonGroupId, personId);
        BinaryData verifyFromLargePersonGroupRequest = BinaryData.fromObject(verifyFromLargePersonGroupRequestObj);
        return verifyFromLargePersonGroupWithResponse(verifyFromLargePersonGroupRequest, requestOptions)
            .flatMap(FluxUtil::toMono)
            .map(protocolMethodData -> protocolMethodData.toObject(FaceVerificationResult.class));
    }

    @Generated
    private static final TypeReference<List<FaceIdentificationResult>> TYPE_REFERENCE_LIST_FACE_IDENTIFICATION_RESULT
        = new TypeReference<List<FaceIdentificationResult>>() {
        };
}
