// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.ai.vision.face.implementation;

import com.azure.ai.vision.face.FaceServiceVersion;
import com.azure.core.annotation.BodyParam;
import com.azure.core.annotation.ExpectedResponses;
import com.azure.core.annotation.HeaderParam;
import com.azure.core.annotation.Host;
import com.azure.core.annotation.HostParam;
import com.azure.core.annotation.Post;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceInterface;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.annotation.UnexpectedResponseExceptionType;
import com.azure.core.exception.ClientAuthenticationException;
import com.azure.core.exception.HttpResponseException;
import com.azure.core.exception.ResourceModifiedException;
import com.azure.core.exception.ResourceNotFoundException;
import com.azure.core.http.HttpPipeline;
import com.azure.core.http.HttpPipelineBuilder;
import com.azure.core.http.policy.RetryPolicy;
import com.azure.core.http.policy.UserAgentPolicy;
import com.azure.core.http.rest.RequestOptions;
import com.azure.core.http.rest.Response;
import com.azure.core.http.rest.RestProxy;
import com.azure.core.util.BinaryData;
import com.azure.core.util.Context;
import com.azure.core.util.FluxUtil;
import com.azure.core.util.serializer.JacksonAdapter;
import com.azure.core.util.serializer.SerializerAdapter;
import reactor.core.publisher.Mono;

/**
 * Initializes a new instance of the FaceClient type.
 */
public final class FaceClientImpl {
    /**
     * The proxy service used to perform REST calls.
     */
    private final FaceClientService service;

    /**
     * Supported Cognitive Services endpoints (protocol and hostname, for example:
     * https://{resource-name}.cognitiveservices.azure.com).
     */
    private final String endpoint;

    /**
     * Gets Supported Cognitive Services endpoints (protocol and hostname, for example:
     * https://{resource-name}.cognitiveservices.azure.com).
     * 
     * @return the endpoint value.
     */
    public String getEndpoint() {
        return this.endpoint;
    }

    /**
     * Service version.
     */
    private final FaceServiceVersion serviceVersion;

    /**
     * Gets Service version.
     * 
     * @return the serviceVersion value.
     */
    public FaceServiceVersion getServiceVersion() {
        return this.serviceVersion;
    }

    /**
     * The HTTP pipeline to send requests through.
     */
    private final HttpPipeline httpPipeline;

    /**
     * Gets The HTTP pipeline to send requests through.
     * 
     * @return the httpPipeline value.
     */
    public HttpPipeline getHttpPipeline() {
        return this.httpPipeline;
    }

    /**
     * The serializer to serialize an object into a string.
     */
    private final SerializerAdapter serializerAdapter;

    /**
     * Gets The serializer to serialize an object into a string.
     * 
     * @return the serializerAdapter value.
     */
    public SerializerAdapter getSerializerAdapter() {
        return this.serializerAdapter;
    }

    /**
     * Initializes an instance of FaceClient client.
     * 
     * @param endpoint Supported Cognitive Services endpoints (protocol and hostname, for example:
     * https://{resource-name}.cognitiveservices.azure.com).
     * @param serviceVersion Service version.
     */
    public FaceClientImpl(String endpoint, FaceServiceVersion serviceVersion) {
        this(new HttpPipelineBuilder().policies(new UserAgentPolicy(), new RetryPolicy()).build(),
            JacksonAdapter.createDefaultSerializerAdapter(), endpoint, serviceVersion);
    }

    /**
     * Initializes an instance of FaceClient client.
     * 
     * @param httpPipeline The HTTP pipeline to send requests through.
     * @param endpoint Supported Cognitive Services endpoints (protocol and hostname, for example:
     * https://{resource-name}.cognitiveservices.azure.com).
     * @param serviceVersion Service version.
     */
    public FaceClientImpl(HttpPipeline httpPipeline, String endpoint, FaceServiceVersion serviceVersion) {
        this(httpPipeline, JacksonAdapter.createDefaultSerializerAdapter(), endpoint, serviceVersion);
    }

    /**
     * Initializes an instance of FaceClient client.
     * 
     * @param httpPipeline The HTTP pipeline to send requests through.
     * @param serializerAdapter The serializer to serialize an object into a string.
     * @param endpoint Supported Cognitive Services endpoints (protocol and hostname, for example:
     * https://{resource-name}.cognitiveservices.azure.com).
     * @param serviceVersion Service version.
     */
    public FaceClientImpl(HttpPipeline httpPipeline, SerializerAdapter serializerAdapter, String endpoint,
        FaceServiceVersion serviceVersion) {
        this.httpPipeline = httpPipeline;
        this.serializerAdapter = serializerAdapter;
        this.endpoint = endpoint;
        this.serviceVersion = serviceVersion;
        this.service = RestProxy.create(FaceClientService.class, this.httpPipeline, this.getSerializerAdapter());
    }

    /**
     * The interface defining all the services for FaceClient to be used by the proxy service to perform REST calls.
     */
    @Host("{endpoint}/face/{apiVersion}")
    @ServiceInterface(name = "FaceClient")
    public interface FaceClientService {
        @Post("/detect")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Mono<Response<BinaryData>> detectFromUrlImpl(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("content-type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData detectFromUrlRequest,
            RequestOptions requestOptions, Context context);

        @Post("/detect")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Response<BinaryData> detectFromUrlImplSync(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("content-type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData detectFromUrlRequest,
            RequestOptions requestOptions, Context context);

        @Post("/detect")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Mono<Response<BinaryData>> detectImpl(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("content-type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/octet-stream") BinaryData imageContent,
            RequestOptions requestOptions, Context context);

        @Post("/detect")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Response<BinaryData> detectImplSync(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("content-type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/octet-stream") BinaryData imageContent,
            RequestOptions requestOptions, Context context);

        @Post("/findsimilars")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Mono<Response<BinaryData>> findSimilar(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData findSimilarRequest,
            RequestOptions requestOptions, Context context);

        @Post("/findsimilars")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Response<BinaryData> findSimilarSync(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData findSimilarRequest,
            RequestOptions requestOptions, Context context);

        @Post("/verify")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Mono<Response<BinaryData>> verifyFaceToFace(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData verifyFaceToFaceRequest,
            RequestOptions requestOptions, Context context);

        @Post("/verify")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Response<BinaryData> verifyFaceToFaceSync(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData verifyFaceToFaceRequest,
            RequestOptions requestOptions, Context context);

        @Post("/group")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Mono<Response<BinaryData>> group(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData groupRequest,
            RequestOptions requestOptions, Context context);

        @Post("/group")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(value = ClientAuthenticationException.class, code = { 401 })
        @UnexpectedResponseExceptionType(value = ResourceNotFoundException.class, code = { 404 })
        @UnexpectedResponseExceptionType(value = ResourceModifiedException.class, code = { 409 })
        @UnexpectedResponseExceptionType(HttpResponseException.class)
        Response<BinaryData> groupSync(@HostParam("endpoint") String endpoint,
            @HostParam("apiVersion") String apiVersion, @HeaderParam("Content-Type") String contentType,
            @HeaderParam("Accept") String accept, @BodyParam("application/json") BinaryData groupRequest,
            RequestOptions requestOptions, Context context);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     * 
     * &gt; [!IMPORTANT]
     * &gt; Microsoft has retired or limited facial recognition capabilities that can be used to try to infer emotional
     * states and identity attributes which, if misused, can subject people to stereotyping, discrimination or unfair
     * denial of services. The retired capabilities are emotion and gender. The limited capabilities are age, smile,
     * facial hair, hair and makeup. Email Azure Face API &lt;azureface&#064;microsoft.com&gt; if you have a responsible
     * use case that would benefit from the use of any of the limited capabilities. Read more about this decision
     * https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     * 
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     url: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     * 
     * @param detectFromUrlRequest The detectFromUrlRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> detectFromUrlImplWithResponseAsync(BinaryData detectFromUrlRequest,
        RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return FluxUtil.withContext(context -> service.detectFromUrlImpl(this.getEndpoint(),
            this.getServiceVersion().getVersion(), contentType, accept, detectFromUrlRequest, requestOptions, context));
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     * 
     * &gt; [!IMPORTANT]
     * &gt; Microsoft has retired or limited facial recognition capabilities that can be used to try to infer emotional
     * states and identity attributes which, if misused, can subject people to stereotyping, discrimination or unfair
     * denial of services. The retired capabilities are emotion and gender. The limited capabilities are age, smile,
     * facial hair, hair and makeup. Email Azure Face API &lt;azureface&#064;microsoft.com&gt; if you have a responsible
     * use case that would benefit from the use of any of the limited capabilities. Read more about this decision
     * https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     * 
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     url: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     * 
     * @param detectFromUrlRequest The detectFromUrlRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> detectFromUrlImplWithResponse(BinaryData detectFromUrlRequest,
        RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return service.detectFromUrlImplSync(this.getEndpoint(), this.getServiceVersion().getVersion(), contentType,
            accept, detectFromUrlRequest, requestOptions, Context.NONE);
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     * 
     * &gt; [!IMPORTANT]
     * &gt; Microsoft has retired or limited facial recognition capabilities that can be used to try to infer emotional
     * states and identity attributes which, if misused, can subject people to stereotyping, discrimination or unfair
     * denial of services. The retired capabilities are emotion and gender. The limited capabilities are age, smile,
     * facial hair, hair and makeup. Email Azure Face API &lt;azureface&#064;microsoft.com&gt; if you have a responsible
     * use case that would benefit from the use of any of the limited capabilities. Read more about this decision
     * https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     * 
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * BinaryData
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     * 
     * @param imageContent The input image binary.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> detectImplWithResponseAsync(BinaryData imageContent,
        RequestOptions requestOptions) {
        final String contentType = "application/octet-stream";
        final String accept = "application/json";
        return FluxUtil.withContext(context -> service.detectImpl(this.getEndpoint(),
            this.getServiceVersion().getVersion(), contentType, accept, imageContent, requestOptions, context));
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.
     * 
     * &gt; [!IMPORTANT]
     * &gt; Microsoft has retired or limited facial recognition capabilities that can be used to try to infer emotional
     * states and identity attributes which, if misused, can subject people to stereotyping, discrimination or unfair
     * denial of services. The retired capabilities are emotion and gender. The limited capabilities are age, smile,
     * facial hair, hair and makeup. Email Azure Face API &lt;azureface&#064;microsoft.com&gt; if you have a responsible
     * use case that would benefit from the use of any of the limited capabilities. Read more about this decision
     * https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.
     * 
     * *
     * * No image will be stored. Only the extracted face feature(s) will be stored on server. The faceId is an
     * identifier of the face feature and will be used in "Identify", "Verify", and "Find Similar". The stored face
     * features will expire and be deleted at the time specified by faceIdTimeToLive after the original detection call.
     * * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose, glasses, occlusion,
     * accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of the results returned for specific
     * attributes may not be highly accurate.
     * * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.
     * * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with
     * dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.
     * * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.
     * * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is true), please use
     * faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).
     * * Different 'detectionModel' values can be provided. To use and compare different detection models, please refer
     * to https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model
     * * 'detection_02': Face attributes and landmarks are disabled if you choose this detection model.
     * * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if you choose this
     * detection model.
     * * Different 'recognitionModel' values are provided. If follow-up operations like "Verify", "Identify", "Find
     * Similar" are needed, please specify the recognition model with 'recognitionModel' parameter. The default value
     * for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need
     * in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to
     * https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.
     * <p><strong>Query Parameters</strong></p>
     * <table border="1">
     * <caption>Query Parameters</caption>
     * <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     * <tr><td>detectionModel</td><td>String</td><td>No</td><td>The 'detectionModel' associated with the detected
     * faceIds. Supported 'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
     * value is 'detection_01'. Allowed values: "detection_01", "detection_02", "detection_03".</td></tr>
     * <tr><td>recognitionModel</td><td>String</td><td>No</td><td>The 'recognitionModel' associated with the detected
     * faceIds. Supported 'recognitionModel' values include 'recognition_01', 'recognition_02', 'recognition_03' or
     * 'recognition_04'. The default value is 'recognition_01'. 'recognition_04' is recommended since its accuracy is
     * improved on faces wearing masks compared with 'recognition_03', and its overall accuracy is improved compared
     * with 'recognition_01' and 'recognition_02'. Allowed values: "recognition_01", "recognition_02", "recognition_03",
     * "recognition_04".</td></tr>
     * <tr><td>returnFaceId</td><td>Boolean</td><td>No</td><td>Return faceIds of the detected faces or not. The default
     * value is true.</td></tr>
     * <tr><td>returnFaceAttributes</td><td>List&lt;String&gt;</td><td>No</td><td>Analyze and return the one or more
     * specified face attributes in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face
     * attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     * <tr><td>returnFaceLandmarks</td><td>Boolean</td><td>No</td><td>Return face landmarks of the detected faces or
     * not. The default value is false.</td></tr>
     * <tr><td>returnRecognitionModel</td><td>Boolean</td><td>No</td><td>Return 'recognitionModel' or not. The default
     * value is false. This is only applicable when returnFaceId = true.</td></tr>
     * <tr><td>faceIdTimeToLive</td><td>Integer</td><td>No</td><td>The number of seconds for the face ID being cached.
     * Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     * You can add these to a request with {@link RequestOptions#addQueryParam}
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * BinaryData
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         faceId: String (Optional)
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04) (Optional)
     *         faceRectangle (Required): {
     *             top: int (Required)
     *             left: int (Required)
     *             width: int (Required)
     *             height: int (Required)
     *         }
     *         faceLandmarks (Optional): {
     *             pupilLeft (Required): {
     *                 x: double (Required)
     *                 y: double (Required)
     *             }
     *             pupilRight (Required): (recursive schema, see pupilRight above)
     *             noseTip (Required): (recursive schema, see noseTip above)
     *             mouthLeft (Required): (recursive schema, see mouthLeft above)
     *             mouthRight (Required): (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter (Required): (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner (Required): (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter (Required): (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop (Required): (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom (Required): (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner (Required): (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner (Required): (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter (Required): (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner (Required): (recursive schema, see eyeRightInner above)
     *             eyeRightTop (Required): (recursive schema, see eyeRightTop above)
     *             eyeRightBottom (Required): (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter (Required): (recursive schema, see eyeRightOuter above)
     *             noseRootLeft (Required): (recursive schema, see noseRootLeft above)
     *             noseRootRight (Required): (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop (Required): (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop (Required): (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip (Required): (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip (Required): (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop (Required): (recursive schema, see upperLipTop above)
     *             upperLipBottom (Required): (recursive schema, see upperLipBottom above)
     *             underLipTop (Required): (recursive schema, see underLipTop above)
     *             underLipBottom (Required): (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes (Optional): {
     *             age: Double (Optional)
     *             smile: Double (Optional)
     *             facialHair (Optional): {
     *                 moustache: double (Required)
     *                 beard: double (Required)
     *                 sideburns: double (Required)
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles) (Optional)
     *             headPose (Optional): {
     *                 pitch: double (Required)
     *                 roll: double (Required)
     *                 yaw: double (Required)
     *             }
     *             hair (Optional): {
     *                 bald: double (Required)
     *                 invisible: boolean (Required)
     *                 hairColor (Required): [
     *                      (Required){
     *                         color: String(unknown/white/gray/blond/brown/red/black/other) (Required)
     *                         confidence: double (Required)
     *                     }
     *                 ]
     *             }
     *             occlusion (Optional): {
     *                 foreheadOccluded: boolean (Required)
     *                 eyeOccluded: boolean (Required)
     *                 mouthOccluded: boolean (Required)
     *             }
     *             accessories (Optional): [
     *                  (Optional){
     *                     type: String(headwear/glasses/mask) (Required)
     *                     confidence: double (Required)
     *                 }
     *             ]
     *             blur (Optional): {
     *                 blurLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             exposure (Optional): {
     *                 exposureLevel: String(underExposure/goodExposure/overExposure) (Required)
     *                 value: double (Required)
     *             }
     *             noise (Optional): {
     *                 noiseLevel: String(low/medium/high) (Required)
     *                 value: double (Required)
     *             }
     *             mask (Optional): {
     *                 noseAndMouthCovered: boolean (Required)
     *                 type: String(faceMask/noMask/otherMaskOrOcclusion/uncertain) (Required)
     *             }
     *             qualityForRecognition: String(low/medium/high) (Optional)
     *         }
     *     }
     * ]
     * }</pre>
     * 
     * @param imageContent The input image binary.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> detectImplWithResponse(BinaryData imageContent, RequestOptions requestOptions) {
        final String contentType = "application/octet-stream";
        final String accept = "application/json";
        return service.detectImplSync(this.getEndpoint(), this.getServiceVersion().getVersion(), contentType, accept,
            imageContent, requestOptions, Context.NONE);
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId array contains the
     * faces created by Detect.
     * 
     * Depending on the input the returned similar faces list contains faceIds or persistedFaceIds ranked by similarity.
     * 
     * Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default mode that it
     * tries to find faces of the same person as possible by using internal same-person thresholds. It is useful to find
     * a known person's other photos. Note that an empty list will be returned if no faces pass the internal thresholds.
     * "matchFace" mode ignores same-person thresholds and returns ranked similar faces anyway, even the similarity is
     * low. It can be used in the cases like searching celebrity-looking faces.
     * 
     * The 'recognitionModel' associated with the query faceId should be the same as the 'recognitionModel' used by the
     * target faceId array.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId: String (Required)
     *     maxNumOfCandidatesReturned: Integer (Optional)
     *     mode: String(matchPerson/matchFace) (Optional)
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         confidence: double (Required)
     *         faceId: String (Optional)
     *         persistedFaceId: String (Optional)
     *     }
     * ]
     * }</pre>
     * 
     * @param findSimilarRequest The findSimilarRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> findSimilarWithResponseAsync(BinaryData findSimilarRequest,
        RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return FluxUtil.withContext(context -> service.findSimilar(this.getEndpoint(),
            this.getServiceVersion().getVersion(), contentType, accept, findSimilarRequest, requestOptions, context));
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId array contains the
     * faces created by Detect.
     * 
     * Depending on the input the returned similar faces list contains faceIds or persistedFaceIds ranked by similarity.
     * 
     * Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default mode that it
     * tries to find faces of the same person as possible by using internal same-person thresholds. It is useful to find
     * a known person's other photos. Note that an empty list will be returned if no faces pass the internal thresholds.
     * "matchFace" mode ignores same-person thresholds and returns ranked similar faces anyway, even the similarity is
     * low. It can be used in the cases like searching celebrity-looking faces.
     * 
     * The 'recognitionModel' associated with the query faceId should be the same as the 'recognitionModel' used by the
     * target faceId array.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId: String (Required)
     *     maxNumOfCandidatesReturned: Integer (Optional)
     *     mode: String(matchPerson/matchFace) (Optional)
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * [
     *      (Required){
     *         confidence: double (Required)
     *         faceId: String (Optional)
     *         persistedFaceId: String (Optional)
     *     }
     * ]
     * }</pre>
     * 
     * @param findSimilarRequest The findSimilarRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> findSimilarWithResponse(BinaryData findSimilarRequest, RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return service.findSimilarSync(this.getEndpoint(), this.getServiceVersion().getVersion(), contentType, accept,
            findSimilarRequest, requestOptions, Context.NONE);
    }

    /**
     * Verify whether two faces belong to a same person.
     * 
     * &gt; [!NOTE]
     * &gt;
     * &gt; *
     * &gt; * Higher face image quality means better identification precision. Please consider high-quality faces:
     * frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.
     * &gt; * For the scenarios that are sensitive to accuracy please make your own judgment.
     * &gt; * The 'recognitionModel' associated with the both faces should be the same.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId1: String (Required)
     *     faceId2: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     isIdentical: boolean (Required)
     *     confidence: double (Required)
     * }
     * }</pre>
     * 
     * @param verifyFaceToFaceRequest The verifyFaceToFaceRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return verify result along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> verifyFaceToFaceWithResponseAsync(BinaryData verifyFaceToFaceRequest,
        RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.verifyFaceToFace(this.getEndpoint(), this.getServiceVersion().getVersion(),
                contentType, accept, verifyFaceToFaceRequest, requestOptions, context));
    }

    /**
     * Verify whether two faces belong to a same person.
     * 
     * &gt; [!NOTE]
     * &gt;
     * &gt; *
     * &gt; * Higher face image quality means better identification precision. Please consider high-quality faces:
     * frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.
     * &gt; * For the scenarios that are sensitive to accuracy please make your own judgment.
     * &gt; * The 'recognitionModel' associated with the both faces should be the same.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceId1: String (Required)
     *     faceId2: String (Required)
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     isIdentical: boolean (Required)
     *     confidence: double (Required)
     * }
     * }</pre>
     * 
     * @param verifyFaceToFaceRequest The verifyFaceToFaceRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return verify result along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> verifyFaceToFaceWithResponse(BinaryData verifyFaceToFaceRequest,
        RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return service.verifyFaceToFaceSync(this.getEndpoint(), this.getServiceVersion().getVersion(), contentType,
            accept, verifyFaceToFaceRequest, requestOptions, Context.NONE);
    }

    /**
     * Divide candidate faces into groups based on face similarity.
     * 
     * &gt;
     * *
     * * The output is one or more disjointed face groups and a messyGroup. A face group contains faces that have
     * similar looking, often of the same person. Face groups are ranked by group size, i.e. number of faces. Notice
     * that faces belonging to a same person might be split into several groups in the result.
     * * MessyGroup is a special face group containing faces that cannot find any similar counterpart face from original
     * faces. The messyGroup will not appear in the result if all faces found their counterparts.
     * * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face To Face" when you
     * only have 2 candidate faces.
     * * The 'recognitionModel' associated with the query faces' faceIds should be the same.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     groups (Required): [
     *          (Required)[
     *             String (Required)
     *         ]
     *     ]
     *     messyGroup (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * @param groupRequest The groupRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return response body for group face operation along with {@link Response} on successful completion of
     * {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Mono<Response<BinaryData>> groupWithResponseAsync(BinaryData groupRequest, RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return FluxUtil.withContext(context -> service.group(this.getEndpoint(), this.getServiceVersion().getVersion(),
            contentType, accept, groupRequest, requestOptions, context));
    }

    /**
     * Divide candidate faces into groups based on face similarity.
     * 
     * &gt;
     * *
     * * The output is one or more disjointed face groups and a messyGroup. A face group contains faces that have
     * similar looking, often of the same person. Face groups are ranked by group size, i.e. number of faces. Notice
     * that faces belonging to a same person might be split into several groups in the result.
     * * MessyGroup is a special face group containing faces that cannot find any similar counterpart face from original
     * faces. The messyGroup will not appear in the result if all faces found their counterparts.
     * * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face To Face" when you
     * only have 2 candidate faces.
     * * The 'recognitionModel' associated with the query faces' faceIds should be the same.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     faceIds (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>{@code
     * {
     *     groups (Required): [
     *          (Required)[
     *             String (Required)
     *         ]
     *     ]
     *     messyGroup (Required): [
     *         String (Required)
     *     ]
     * }
     * }</pre>
     * 
     * @param groupRequest The groupRequest parameter.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return response body for group face operation along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> groupWithResponse(BinaryData groupRequest, RequestOptions requestOptions) {
        final String contentType = "application/json";
        final String accept = "application/json";
        return service.groupSync(this.getEndpoint(), this.getServiceVersion().getVersion(), contentType, accept,
            groupRequest, requestOptions, Context.NONE);
    }
}
