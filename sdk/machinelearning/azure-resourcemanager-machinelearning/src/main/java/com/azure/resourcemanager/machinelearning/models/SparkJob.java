// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.machinelearning.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.util.logging.ClientLogger;
import com.azure.json.JsonReader;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.List;
import java.util.Map;

/**
 * Spark job definition.
 */
@Fluent
public final class SparkJob extends JobBaseProperties {
    /*
     * [Required] Specifies the type of job.
     */
    private JobType jobType = JobType.SPARK;

    /*
     * Compute Resource configuration for the job.
     */
    private SparkResourceConfiguration resources;

    /*
     * Arguments for the job.
     */
    private String args;

    /*
     * [Required] arm-id of the code asset.
     */
    private String codeId;

    /*
     * [Required] The entry to execute on startup of the job.
     */
    private SparkJobEntry entry;

    /*
     * The ARM resource ID of the Environment specification for the job.
     */
    private String environmentId;

    /*
     * Mapping of input data bindings used in the job.
     */
    private Map<String, JobInput> inputs;

    /*
     * Mapping of output data bindings used in the job.
     */
    private Map<String, JobOutput> outputs;

    /*
     * Python files used in the job.
     */
    private List<String> pyFiles;

    /*
     * Jar files used in the job.
     */
    private List<String> jars;

    /*
     * Files used in the job.
     */
    private List<String> files;

    /*
     * Archive files used in the job.
     */
    private List<String> archives;

    /*
     * Spark configured properties.
     */
    private Map<String, String> conf;

    /*
     * Queue settings for the job
     */
    private QueueSettings queueSettings;

    /*
     * Environment variables included in the job.
     */
    private Map<String, String> environmentVariables;

    /*
     * Status of the job.
     */
    private JobStatus status;

    /**
     * Creates an instance of SparkJob class.
     */
    public SparkJob() {
    }

    /**
     * Get the jobType property: [Required] Specifies the type of job.
     * 
     * @return the jobType value.
     */
    @Override
    public JobType jobType() {
        return this.jobType;
    }

    /**
     * Get the resources property: Compute Resource configuration for the job.
     * 
     * @return the resources value.
     */
    public SparkResourceConfiguration resources() {
        return this.resources;
    }

    /**
     * Set the resources property: Compute Resource configuration for the job.
     * 
     * @param resources the resources value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withResources(SparkResourceConfiguration resources) {
        this.resources = resources;
        return this;
    }

    /**
     * Get the args property: Arguments for the job.
     * 
     * @return the args value.
     */
    public String args() {
        return this.args;
    }

    /**
     * Set the args property: Arguments for the job.
     * 
     * @param args the args value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withArgs(String args) {
        this.args = args;
        return this;
    }

    /**
     * Get the codeId property: [Required] arm-id of the code asset.
     * 
     * @return the codeId value.
     */
    public String codeId() {
        return this.codeId;
    }

    /**
     * Set the codeId property: [Required] arm-id of the code asset.
     * 
     * @param codeId the codeId value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withCodeId(String codeId) {
        this.codeId = codeId;
        return this;
    }

    /**
     * Get the entry property: [Required] The entry to execute on startup of the job.
     * 
     * @return the entry value.
     */
    public SparkJobEntry entry() {
        return this.entry;
    }

    /**
     * Set the entry property: [Required] The entry to execute on startup of the job.
     * 
     * @param entry the entry value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEntry(SparkJobEntry entry) {
        this.entry = entry;
        return this;
    }

    /**
     * Get the environmentId property: The ARM resource ID of the Environment specification for the job.
     * 
     * @return the environmentId value.
     */
    public String environmentId() {
        return this.environmentId;
    }

    /**
     * Set the environmentId property: The ARM resource ID of the Environment specification for the job.
     * 
     * @param environmentId the environmentId value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEnvironmentId(String environmentId) {
        this.environmentId = environmentId;
        return this;
    }

    /**
     * Get the inputs property: Mapping of input data bindings used in the job.
     * 
     * @return the inputs value.
     */
    public Map<String, JobInput> inputs() {
        return this.inputs;
    }

    /**
     * Set the inputs property: Mapping of input data bindings used in the job.
     * 
     * @param inputs the inputs value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withInputs(Map<String, JobInput> inputs) {
        this.inputs = inputs;
        return this;
    }

    /**
     * Get the outputs property: Mapping of output data bindings used in the job.
     * 
     * @return the outputs value.
     */
    public Map<String, JobOutput> outputs() {
        return this.outputs;
    }

    /**
     * Set the outputs property: Mapping of output data bindings used in the job.
     * 
     * @param outputs the outputs value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withOutputs(Map<String, JobOutput> outputs) {
        this.outputs = outputs;
        return this;
    }

    /**
     * Get the pyFiles property: Python files used in the job.
     * 
     * @return the pyFiles value.
     */
    public List<String> pyFiles() {
        return this.pyFiles;
    }

    /**
     * Set the pyFiles property: Python files used in the job.
     * 
     * @param pyFiles the pyFiles value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withPyFiles(List<String> pyFiles) {
        this.pyFiles = pyFiles;
        return this;
    }

    /**
     * Get the jars property: Jar files used in the job.
     * 
     * @return the jars value.
     */
    public List<String> jars() {
        return this.jars;
    }

    /**
     * Set the jars property: Jar files used in the job.
     * 
     * @param jars the jars value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withJars(List<String> jars) {
        this.jars = jars;
        return this;
    }

    /**
     * Get the files property: Files used in the job.
     * 
     * @return the files value.
     */
    public List<String> files() {
        return this.files;
    }

    /**
     * Set the files property: Files used in the job.
     * 
     * @param files the files value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withFiles(List<String> files) {
        this.files = files;
        return this;
    }

    /**
     * Get the archives property: Archive files used in the job.
     * 
     * @return the archives value.
     */
    public List<String> archives() {
        return this.archives;
    }

    /**
     * Set the archives property: Archive files used in the job.
     * 
     * @param archives the archives value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withArchives(List<String> archives) {
        this.archives = archives;
        return this;
    }

    /**
     * Get the conf property: Spark configured properties.
     * 
     * @return the conf value.
     */
    public Map<String, String> conf() {
        return this.conf;
    }

    /**
     * Set the conf property: Spark configured properties.
     * 
     * @param conf the conf value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withConf(Map<String, String> conf) {
        this.conf = conf;
        return this;
    }

    /**
     * Get the queueSettings property: Queue settings for the job.
     * 
     * @return the queueSettings value.
     */
    public QueueSettings queueSettings() {
        return this.queueSettings;
    }

    /**
     * Set the queueSettings property: Queue settings for the job.
     * 
     * @param queueSettings the queueSettings value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withQueueSettings(QueueSettings queueSettings) {
        this.queueSettings = queueSettings;
        return this;
    }

    /**
     * Get the environmentVariables property: Environment variables included in the job.
     * 
     * @return the environmentVariables value.
     */
    public Map<String, String> environmentVariables() {
        return this.environmentVariables;
    }

    /**
     * Set the environmentVariables property: Environment variables included in the job.
     * 
     * @param environmentVariables the environmentVariables value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEnvironmentVariables(Map<String, String> environmentVariables) {
        this.environmentVariables = environmentVariables;
        return this;
    }

    /**
     * Get the status property: Status of the job.
     * 
     * @return the status value.
     */
    @Override
    public JobStatus status() {
        return this.status;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withDisplayName(String displayName) {
        super.withDisplayName(displayName);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withExperimentName(String experimentName) {
        super.withExperimentName(experimentName);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withServices(Map<String, JobService> services) {
        super.withServices(services);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withComputeId(String computeId) {
        super.withComputeId(computeId);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withIsArchived(Boolean isArchived) {
        super.withIsArchived(isArchived);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withIdentity(IdentityConfiguration identity) {
        super.withIdentity(identity);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withComponentId(String componentId) {
        super.withComponentId(componentId);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withNotificationSetting(NotificationSetting notificationSetting) {
        super.withNotificationSetting(notificationSetting);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withDescription(String description) {
        super.withDescription(description);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withTags(Map<String, String> tags) {
        super.withTags(tags);
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public SparkJob withProperties(Map<String, String> properties) {
        super.withProperties(properties);
        return this;
    }

    /**
     * Validates the instance.
     * 
     * @throws IllegalArgumentException thrown if the instance is not valid.
     */
    @Override
    public void validate() {
        super.validate();
        if (resources() != null) {
            resources().validate();
        }
        if (codeId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Missing required property codeId in model SparkJob"));
        }
        if (entry() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Missing required property entry in model SparkJob"));
        } else {
            entry().validate();
        }
        if (inputs() != null) {
            inputs().values().forEach(e -> {
                if (e != null) {
                    e.validate();
                }
            });
        }
        if (outputs() != null) {
            outputs().values().forEach(e -> {
                if (e != null) {
                    e.validate();
                }
            });
        }
        if (queueSettings() != null) {
            queueSettings().validate();
        }
    }

    private static final ClientLogger LOGGER = new ClientLogger(SparkJob.class);

    /**
     * {@inheritDoc}
     */
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("description", description());
        jsonWriter.writeMapField("tags", tags(), (writer, element) -> writer.writeString(element));
        jsonWriter.writeMapField("properties", properties(), (writer, element) -> writer.writeString(element));
        jsonWriter.writeStringField("displayName", displayName());
        jsonWriter.writeStringField("experimentName", experimentName());
        jsonWriter.writeMapField("services", services(), (writer, element) -> writer.writeJson(element));
        jsonWriter.writeStringField("computeId", computeId());
        jsonWriter.writeBooleanField("isArchived", isArchived());
        jsonWriter.writeJsonField("identity", identity());
        jsonWriter.writeStringField("componentId", componentId());
        jsonWriter.writeJsonField("notificationSetting", notificationSetting());
        jsonWriter.writeStringField("codeId", this.codeId);
        jsonWriter.writeJsonField("entry", this.entry);
        jsonWriter.writeStringField("jobType", this.jobType == null ? null : this.jobType.toString());
        jsonWriter.writeJsonField("resources", this.resources);
        jsonWriter.writeStringField("args", this.args);
        jsonWriter.writeStringField("environmentId", this.environmentId);
        jsonWriter.writeMapField("inputs", this.inputs, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeMapField("outputs", this.outputs, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeArrayField("pyFiles", this.pyFiles, (writer, element) -> writer.writeString(element));
        jsonWriter.writeArrayField("jars", this.jars, (writer, element) -> writer.writeString(element));
        jsonWriter.writeArrayField("files", this.files, (writer, element) -> writer.writeString(element));
        jsonWriter.writeArrayField("archives", this.archives, (writer, element) -> writer.writeString(element));
        jsonWriter.writeMapField("conf", this.conf, (writer, element) -> writer.writeString(element));
        jsonWriter.writeJsonField("queueSettings", this.queueSettings);
        jsonWriter.writeMapField("environmentVariables", this.environmentVariables,
            (writer, element) -> writer.writeString(element));
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of SparkJob from the JsonReader.
     * 
     * @param jsonReader The JsonReader being read.
     * @return An instance of SparkJob if the JsonReader was pointing to an instance of it, or null if it was pointing
     * to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the SparkJob.
     */
    public static SparkJob fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            SparkJob deserializedSparkJob = new SparkJob();
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();

                if ("description".equals(fieldName)) {
                    deserializedSparkJob.withDescription(reader.getString());
                } else if ("tags".equals(fieldName)) {
                    Map<String, String> tags = reader.readMap(reader1 -> reader1.getString());
                    deserializedSparkJob.withTags(tags);
                } else if ("properties".equals(fieldName)) {
                    Map<String, String> properties = reader.readMap(reader1 -> reader1.getString());
                    deserializedSparkJob.withProperties(properties);
                } else if ("displayName".equals(fieldName)) {
                    deserializedSparkJob.withDisplayName(reader.getString());
                } else if ("status".equals(fieldName)) {
                    deserializedSparkJob.status = JobStatus.fromString(reader.getString());
                } else if ("experimentName".equals(fieldName)) {
                    deserializedSparkJob.withExperimentName(reader.getString());
                } else if ("services".equals(fieldName)) {
                    Map<String, JobService> services = reader.readMap(reader1 -> JobService.fromJson(reader1));
                    deserializedSparkJob.withServices(services);
                } else if ("computeId".equals(fieldName)) {
                    deserializedSparkJob.withComputeId(reader.getString());
                } else if ("isArchived".equals(fieldName)) {
                    deserializedSparkJob.withIsArchived(reader.getNullable(JsonReader::getBoolean));
                } else if ("identity".equals(fieldName)) {
                    deserializedSparkJob.withIdentity(IdentityConfiguration.fromJson(reader));
                } else if ("componentId".equals(fieldName)) {
                    deserializedSparkJob.withComponentId(reader.getString());
                } else if ("notificationSetting".equals(fieldName)) {
                    deserializedSparkJob.withNotificationSetting(NotificationSetting.fromJson(reader));
                } else if ("codeId".equals(fieldName)) {
                    deserializedSparkJob.codeId = reader.getString();
                } else if ("entry".equals(fieldName)) {
                    deserializedSparkJob.entry = SparkJobEntry.fromJson(reader);
                } else if ("jobType".equals(fieldName)) {
                    deserializedSparkJob.jobType = JobType.fromString(reader.getString());
                } else if ("resources".equals(fieldName)) {
                    deserializedSparkJob.resources = SparkResourceConfiguration.fromJson(reader);
                } else if ("args".equals(fieldName)) {
                    deserializedSparkJob.args = reader.getString();
                } else if ("environmentId".equals(fieldName)) {
                    deserializedSparkJob.environmentId = reader.getString();
                } else if ("inputs".equals(fieldName)) {
                    Map<String, JobInput> inputs = reader.readMap(reader1 -> JobInput.fromJson(reader1));
                    deserializedSparkJob.inputs = inputs;
                } else if ("outputs".equals(fieldName)) {
                    Map<String, JobOutput> outputs = reader.readMap(reader1 -> JobOutput.fromJson(reader1));
                    deserializedSparkJob.outputs = outputs;
                } else if ("pyFiles".equals(fieldName)) {
                    List<String> pyFiles = reader.readArray(reader1 -> reader1.getString());
                    deserializedSparkJob.pyFiles = pyFiles;
                } else if ("jars".equals(fieldName)) {
                    List<String> jars = reader.readArray(reader1 -> reader1.getString());
                    deserializedSparkJob.jars = jars;
                } else if ("files".equals(fieldName)) {
                    List<String> files = reader.readArray(reader1 -> reader1.getString());
                    deserializedSparkJob.files = files;
                } else if ("archives".equals(fieldName)) {
                    List<String> archives = reader.readArray(reader1 -> reader1.getString());
                    deserializedSparkJob.archives = archives;
                } else if ("conf".equals(fieldName)) {
                    Map<String, String> conf = reader.readMap(reader1 -> reader1.getString());
                    deserializedSparkJob.conf = conf;
                } else if ("queueSettings".equals(fieldName)) {
                    deserializedSparkJob.queueSettings = QueueSettings.fromJson(reader);
                } else if ("environmentVariables".equals(fieldName)) {
                    Map<String, String> environmentVariables = reader.readMap(reader1 -> reader1.getString());
                    deserializedSparkJob.environmentVariables = environmentVariables;
                } else {
                    reader.skipChildren();
                }
            }

            return deserializedSparkJob;
        });
    }
}
