{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b08b1d81-028c-4dba-ac24-b23ceebc9e38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Secrets**\n",
    "\n",
    "The secrets below  like the Cosmos account key are retrieved from a secret scope. If you don't have defined a secret scope for a Cosmos Account you want to use when going through this sample you can find the instructions on how to create one here:\n",
    "- Here you can [Create a new secret scope](./#secrets/createScope) for the current Databricks workspace\n",
    "  - See how you can create an [Azure Key Vault backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope) \n",
    "  - See how you can create a [Databricks backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope)\n",
    "- And here you can find information on how to [add secrets to your Spark configuration](https://docs.microsoft.com/azure/databricks/security/secrets/secrets#read-a-secret)\n",
    "If you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74303a6e-555c-44e2-8255-c8aea71caf03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cosmosEndpoint = spark.conf.get(\"spark.cosmos.accountEndpoint\")\n",
    "cosmosMasterKey = spark.conf.get(\"spark.cosmos.accountKey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f719dedb-d782-4b54-9076-6d29da946950",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - loading data source \"[NYC Taxi & Limousine Commission - green taxi trip records](https://azure.microsoft.com/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/)\"**\n",
    "\n",
    "The green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. This data set has over 80 million records (>8 GB) of data and is available via a publicly accessible Azure Blob Storage Account located in the East-US Azure region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "080c50a8-2d33-4f0f-9fba-c5b4212b7630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "print(\"Starting preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"green\"\n",
    "blob_sas_token = r\"\"\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df_rawInput = spark.read.parquet(wasbs_path)\n",
    "\n",
    "# Adding an id column with unique values\n",
    "uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
    "df_input_withId = df_rawInput.withColumn(\"id\", uuidUdf()).cache()\n",
    "\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df_input_withId.createOrReplaceTempView('source')\n",
    "print(\"Finished preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ce582cf-5601-4827-8680-89bb3c0fa846",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - creating the Cosmos DB container to ingest the data into**\n",
    "\n",
    "Configure the Catalog API to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6d2168c-f06a-4445-8bd1-f7e1d0a902cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24ef8b89-25ee-4477-a4e7-208fabab15fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And execute the command to create the new container with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed. We will also create a second container that will be used to store metadata for the global throughput control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c61c309d-7a20-4488-a4e9-59bbaae19c96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\n",
    "USING cosmos.items\n",
    "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
    "\n",
    "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\n",
    "USING cosmos.items\n",
    "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2805de61-e652-4be6-a3e5-b084cd848dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Sample - ingesting the NYC Green Taxi data into Cosmos DB**\n",
    "\n",
    "By setting the target throughput threshold to 0.95 (95%) we reduce throttling but still allow the ingestion to consume most of the provisioned throughput. For scenarios where ingestion should only take a smaller subset of the available throughput this threshold can be reduced accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "56fc9432-df8b-456d-97c1-498c90938ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "writeCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n",
    "  \"spark.cosmos.write.maxConcurrency\": \"150\",\n",
    "  \"spark.cosmos.write.bulkEnabled\": \"true\",\n",
    "  \"spark.cosmos.throughputControlEnabled\": \"true\",\n",
    "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestion\",\n",
    "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.95\",\n",
    "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\n",
    "}\n",
    "\n",
    "df_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\n",
    "\n",
    "df_NYCGreenTaxi_Input \\\n",
    "  .write \\\n",
    "  .format(\"cosmos.items\") \\\n",
    "  .mode(\"Append\") \\\n",
    "  .options(**writeCfg) \\\n",
    "  .save()\n",
    "\n",
    "print(\"Finished ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f8b1a3b-44ed-4117-aae6-09c85bb86981",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - validating the record count via query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c56441d-c641-4751-99af-fcd9461da033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_source = spark.sql('SELECT * FROM source').count()\n",
    "print(\"Number of records in source: \", count_source) \n",
    "print(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "readCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchemaEnabled\" : \"false\"\n",
    "}\n",
    "\n",
    "query_df = spark.read.format(\"cosmos.items\").options(**readCfg).load()\n",
    "count_query = query_df.count()\n",
    "print(\"Number of records retrieved via query: \", count_query) \n",
    "print(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc07b9a6-5021-49cb-acb6-1c345caab955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - validating the record count via change feed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e02a1e59-1dcd-4e9d-b8b3-20028798f710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "changeFeedCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchemaEnabled\" : \"false\",\n",
    "  \"spark.cosmos.changeFeed.startFrom\" : \"Beginning\",\n",
    "  \"spark.cosmos.changeFeed.mode\" : \"Incremental\"\n",
    "}\n",
    "changeFeed_df = spark.read.format(\"cosmos.changeFeed\").options(**changeFeedCfg).load()\n",
    "count_changeFeed = changeFeed_df.count()\n",
    "print(\"Number of records retrieved via change feed: \", count_changeFeed) \n",
    "print(\"Finished validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44c33e38-aa02-45f1-8e36-5c20a4fb6b35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Cleanup - deleting the Cosmos DB container and database again (to reduce cost) - skip this step if you want to keep them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8afb7129-ae8c-4cc9-b177-58be5cc02445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS SampleDatabase.GreenTaxiRecords;\n",
    "\n",
    "DROP TABLE IF EXISTS SampleDatabase.ThroughputControl;\n",
    "\n",
    "DROP DATABASE IF EXISTS cosmosCatalog.SampleDatabase CASCADE;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "Ingestion",
   "notebookOrigID": 2700934222589942,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
