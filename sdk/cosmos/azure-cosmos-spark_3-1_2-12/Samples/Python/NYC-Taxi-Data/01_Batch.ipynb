{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b08b1d81-028c-4dba-ac24-b23ceebc9e38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Secrets**\n",
    "\n",
    "The secrets below  like the Cosmos account key are retrieved from a secret scope. If you don't have defined a secret scope for a Cosmos Account you want to use when going through this sample you can find the instructions on how to create one here:\n",
    "- Here you can [Create a new secret scope](./#secrets/createScope) for the current Databricks workspace\n",
    "  - See how you can create an [Azure Key Vault backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope) \n",
    "  - See how you can create a [Databricks backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope)\n",
    "- And here you can find information on how to [add secrets to your Spark configuration](https://docs.microsoft.com/azure/databricks/security/secrets/secrets#read-a-secret)\n",
    "If you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74303a6e-555c-44e2-8255-c8aea71caf03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cosmosEndpoint = spark.conf.get(\"spark.cosmos.accountEndpoint\")\n",
    "cosmosMasterKey = spark.conf.get(\"spark.cosmos.accountKey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ce582cf-5601-4827-8680-89bb3c0fa846",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - creating the Cosmos DB container to ingest the data into**\n",
    "\n",
    "Configure the Catalog API to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6d2168c-f06a-4445-8bd1-f7e1d0a902cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24ef8b89-25ee-4477-a4e7-208fabab15fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And execute the command to create the new container with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed. We will also create a second container that will be used to store metadata for the global throughput control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c61c309d-7a20-4488-a4e9-59bbaae19c96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSink\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
    "\n",
    "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\n",
    "USING cosmos.oltp\n",
    "OPTIONS(spark.cosmos.database = 'SampleDatabase')\n",
    "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f719dedb-d782-4b54-9076-6d29da946950",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - loading data source \"[NYC Taxi & Limousine Commission - green taxi trip records](https://azure.microsoft.com/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/)\"**\n",
    "\n",
    "The green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. This data set has over 80 million records (>8 GB) of data and is available via a publicly accessible Azure Blob Storage Account located in the East-US Azure region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "080c50a8-2d33-4f0f-9fba-c5b4212b7630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import uuid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "\n",
    "print(\"Starting preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"green\"\n",
    "blob_sas_token = r\"\"\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "# NOTE - if you want to experiment with larger dataset sizes - consider switching to Option B (commenting code \n",
    "# for Option A/uncommenting code for option B) the lines below or increase the value passed into the \n",
    "# limit function restricting the dataset size below\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# Option A - with limited dataset size\n",
    "#------------------------------------------------------------------------------------\n",
    "df_rawInputWithoutLimit = spark.read.parquet(wasbs_path)\n",
    "partitionCount = df_rawInputWithoutLimit.rdd.getNumPartitions()\n",
    "df_rawInput = df_rawInputWithoutLimit.limit(1_000_000).repartition(partitionCount)\n",
    "df_rawInput.persist()\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# Option B - entire dataset\n",
    "#------------------------------------------------------------------------------------\n",
    "#df_rawInput = spark.read.parquet(wasbs_path)\n",
    "\n",
    "# Adding an id column with unique values\n",
    "uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
    "nowUdf= udf(lambda : int(time.time() * 1000),LongType())\n",
    "df_input_withId = df_rawInput \\\n",
    "  .withColumn(\"id\", uuidUdf()) \\\n",
    "  .withColumn(\"insertedAt\", nowUdf()) \\\n",
    "\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df_input_withId.createOrReplaceTempView('source')\n",
    "print(\"Finished preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2805de61-e652-4be6-a3e5-b084cd848dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Sample - ingesting the NYC Green Taxi data into Cosmos DB**\n",
    "\n",
    "By setting the target throughput threshold to 0.95 (95%) we reduce throttling but still allow the ingestion to consume most of the provisioned throughput. For scenarios where ingestion should only take a smaller subset of the available throughput this threshold can be reduced accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "56fc9432-df8b-456d-97c1-498c90938ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "\n",
    "print(\"Starting ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "writeCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n",
    "  \"spark.cosmos.write.bulk.enabled\": \"true\",\n",
    "  \"spark.cosmos.throughputControl.enabled\": \"true\",\n",
    "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestion\",\n",
    "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.95\",\n",
    "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\n",
    "}\n",
    "\n",
    "df_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\n",
    "\n",
    "df_NYCGreenTaxi_Input \\\n",
    "  .write \\\n",
    "  .format(\"cosmos.oltp\") \\\n",
    "  .mode(\"Append\") \\\n",
    "  .options(**writeCfg) \\\n",
    "  .save()\n",
    "\n",
    "print(\"Finished ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a84d335d-b0f1-480f-8826-07b66ad1f1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Getting the reference record count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd176767-080a-4755-8cfd-4d89602b7c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_source = spark.sql('SELECT * FROM source').count()\n",
    "print(\"Number of records in source: \", count_source) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f8b1a3b-44ed-4117-aae6-09c85bb86981",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - validating the record count via query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c56441d-c641-4751-99af-fcd9461da033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "readCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
    "  \"spark.cosmos.read.customQuery\" : \"SELECT COUNT(0) AS Count FROM c\"\n",
    "}\n",
    "\n",
    "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\n",
    "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\n",
    "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\n",
    "print(\"Number of records retrieved via query: \", count_query) \n",
    "print(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "assert count_source == count_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc07b9a6-5021-49cb-acb6-1c345caab955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - validating the record count via change feed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e02a1e59-1dcd-4e9d-b8b3-20028798f710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "changeFeedCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
    "  \"spark.cosmos.changeFeed.startFrom\" : \"Beginning\",\n",
    "  \"spark.cosmos.changeFeed.mode\" : \"Incremental\"\n",
    "}\n",
    "changeFeed_df = spark.read.format(\"cosmos.oltp.changeFeed\").options(**changeFeedCfg).load()\n",
    "count_changeFeed = changeFeed_df.count()\n",
    "print(\"Number of records retrieved via change feed: \", count_changeFeed) \n",
    "print(\"Finished validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "assert count_source == count_changeFeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f43c9e10-95ec-4059-a764-2268472877fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - bulk deleting documents and validating document count afterwards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63be3f23-ae76-4813-8bdb-40733e56e087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print(\"Starting to identify to be deleted documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "readCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
    "}\n",
    "\n",
    "toBeDeleted_df = spark.read.format(\"cosmos.oltp\").options(**readCfg).load().limit(100_000)\n",
    "print(\"Number of records to be deleted: \", toBeDeleted_df.count()) \n",
    "\n",
    "print(\"Starting to bulk delete documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "deleteCfg = writeCfg.copy()\n",
    "deleteCfg[\"spark.cosmos.write.strategy\"] = \"ItemDelete\"\n",
    "toBeDeleted_df \\\n",
    "        .write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .mode(\"Append\") \\\n",
    "        .options(**deleteCfg) \\\n",
    "        .save()\n",
    "print(\"Finished deleting documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\n",
    "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\n",
    "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\n",
    "print(\"Number of records retrieved via query: \", count_query) \n",
    "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "assert max(0, count_source - 100_000) == count_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "907f3933-5008-45d1-bba6-a426ca13c823",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - showing the existing Containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "883d1de2-7fdc-45f8-a1e3-1cf53c3b9855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b027d779-b88b-4dab-bc0d-61e3fb35d2ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
    "assert df_Tables.count() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10d6d127-7db3-479b-aeef-c45d595a3885",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - querying a Cosmos Container via Spark Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b9e59a7-5318-4a6f-b9a8-60e89c6627d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "456b891c-e973-4bf9-97f0-a9151ed93196",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - querying a Cosmos Container with custom settings via Spark Catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a321f36f-c1f6-4b2b-aa79-5cf00f56bf3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating the view with custom settings (in this case adding a projection, disabling schema inference and switching to aggressive partitioning strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4830781a-2c67-4a1d-bb0d-a64655142fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsView \n",
    "  (id STRING, _ts TIMESTAMP, vendorID INT, totalAmount DOUBLE)\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(isCosmosView = 'True')\n",
    "OPTIONS (\n",
    "  spark.cosmos.database = 'SampleDatabase',\n",
    "  spark.cosmos.container = 'GreenTaxiRecords',\n",
    "  spark.cosmos.read.inferSchema.enabled = 'False',\n",
    "  spark.cosmos.read.inferSchema.includeSystemProperties = 'True',\n",
    "  spark.cosmos.read.partitioning.strategy = 'Aggressive');\n",
    "\n",
    "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsView LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "800d868e-1d32-4eee-8a1d-0c947d0af715",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating another view with custom settings (in this case enabling schema inference and switching to restrictive partitioning strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cea14de6-4be9-4cb7-b6d2-00928a75f912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView \n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(isCosmosView = 'True')\n",
    "OPTIONS (\n",
    "  spark.cosmos.database = 'SampleDatabase',\n",
    "  spark.cosmos.container = 'GreenTaxiRecords',\n",
    "  spark.cosmos.read.inferSchema.enabled = 'True',\n",
    "  spark.cosmos.read.inferSchema.includeSystemProperties = 'False',\n",
    "  spark.cosmos.read.partitioning.strategy = 'Restrictive');\n",
    "\n",
    "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23cc237d-251e-4f31-a73f-4e2786316eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Show all Tables in the Cosmos Catalog to show that both the \"real\" Containers as well as the views show-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ad2e367-b5d8-42b3-8a56-2505869eae6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb0df38d-27ef-462d-8181-8df3d676e86a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
    "assert df_Tables.count() == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "865ccc18-5342-4de2-969b-1e0670272797",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Cleanup the views again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ff1f132-8970-4a33-ac84-241df8e4f57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsView;\n",
    "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView;\n",
    "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85142b5a-c14b-4b93-bf9e-2bc7f9e2c3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
    "assert df_Tables.count() == 3"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Batch",
   "notebookOrigID": 2325053098461791,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
