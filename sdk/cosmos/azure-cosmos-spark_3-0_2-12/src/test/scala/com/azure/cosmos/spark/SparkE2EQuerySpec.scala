// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
package com.azure.cosmos.spark

import java.util.UUID

import com.azure.cosmos.implementation.{TestConfigurations, Utils}

class SparkE2EQuerySpec extends IntegrationSpec with Spark with CosmosClient with CosmosContainer {
  //scalastyle:off multiple.string.literals
  //scalastyle:off magic.number

  "spark query" can "use user provided schema" taggedAs (RequiresCosmosEndpoint) in {
    val cosmosEndpoint = TestConfigurations.HOST
    val cosmosMasterKey = TestConfigurations.MASTER_KEY

    val container = cosmosClient.getDatabase(cosmosDatabase).getContainer(cosmosContainer)
    for (state <- Array(true, false)) {
      val objectNode = Utils.getSimpleObjectMapper.createObjectNode()
      objectNode.put("name", "Shrodigner's cat")
      objectNode.put("type", "cat")
      objectNode.put("age", 20)
      objectNode.put("isAlive", state)
      objectNode.put("id", UUID.randomUUID().toString)
      container.createItem(objectNode).block()
    }
    val cfg = Map("spark.cosmos.accountEndpoint" -> cosmosEndpoint,
      "spark.cosmos.accountKey" -> cosmosMasterKey,
      "spark.cosmos.database" -> cosmosDatabase,
      "spark.cosmos.container" -> cosmosContainer
    )

    // scalastyle:off underscore.import
    // scalastyle:off import.grouping
    import org.apache.spark.sql.types._
    // scalastyle:on underscore.import
    // scalastyle:on import.grouping

    val customSchema = StructType(Array(
      StructField("id", StringType),
      StructField("name", StringType),
      StructField("type", StringType),
      StructField("age", IntegerType),
      StructField("isAlive", BooleanType)
    ))

    val df = spark.read.schema(customSchema).format("cosmos.items").options(cfg).load()
    val rowsArray = df.where("isAlive = 'true' and type = 'cat'").collect()
    rowsArray should have size 1

    val row = rowsArray(0)
    row.getAs[String]("name") shouldEqual "Shrodigner's cat"
    row.getAs[String]("type") shouldEqual "cat"
    row.getAs[Integer]("age") shouldEqual 20
    row.getAs[Boolean]("isAlive") shouldEqual true
  }

    "spark query" can "use schema inference" taggedAs (RequiresCosmosEndpoint) in {
        val cosmosEndpoint = TestConfigurations.HOST
        val cosmosMasterKey = TestConfigurations.MASTER_KEY

        val container = cosmosClient.getDatabase(cosmosDatabase).getContainer(cosmosContainer)
        for (state <- Array(true, false)) {
            val objectNode = Utils.getSimpleObjectMapper.createObjectNode()
            objectNode.put("name", "Shrodigner's dog")
            objectNode.put("type", "dog")
            objectNode.put("age", 20)
            objectNode.put("isAlive", state)
            objectNode.put("id", UUID.randomUUID().toString)
            container.createItem(objectNode).block()
        }

        val cfgWithInference = Map("spark.cosmos.accountEndpoint" -> cosmosEndpoint,
            "spark.cosmos.accountKey" -> cosmosMasterKey,
            "spark.cosmos.database" -> cosmosDatabase,
            "spark.cosmos.container" -> cosmosContainer,
            "spark.cosmos.read.inferSchemaEnabled" -> "true"
        )

        // Not passing schema, letting inference work
        val dfWithInference = spark.read.format("cosmos.items").options(cfgWithInference).load()
        val rowsArrayWithInference = dfWithInference.where("isAlive = 'true' and type = 'dog'").collect()
        rowsArrayWithInference should have size 1

        val rowWithInference = rowsArrayWithInference(0)
        rowWithInference.getAs[String]("name") shouldEqual "Shrodigner's dog"
        rowWithInference.getAs[String]("type") shouldEqual "dog"
        rowWithInference.getAs[Integer]("age") shouldEqual 20
        rowWithInference.getAs[Boolean]("isAlive") shouldEqual true
    }

    "spark query" can "use schema inference with custom query" taggedAs (RequiresCosmosEndpoint) in {
        val cosmosEndpoint = TestConfigurations.HOST
        val cosmosMasterKey = TestConfigurations.MASTER_KEY

        val container = cosmosClient.getDatabase(cosmosDatabase).getContainer(cosmosContainer)
        for (state <- Array(true, false)) {
            val objectNode = Utils.getSimpleObjectMapper.createObjectNode()
            objectNode.put("name", "Shrodigner's duck")
            objectNode.put("type", "duck")
            objectNode.put("age", 20)
            objectNode.put("isAlive", state)
            objectNode.put("id", UUID.randomUUID().toString)
            container.createItem(objectNode).block()
        }

        val cfgWithInference = Map("spark.cosmos.accountEndpoint" -> cosmosEndpoint,
            "spark.cosmos.accountKey" -> cosmosMasterKey,
            "spark.cosmos.database" -> cosmosDatabase,
            "spark.cosmos.container" -> cosmosContainer,
            "spark.cosmos.read.inferSchemaEnabled" -> "true",
            "spark.cosmos.read.inferSchemaQuery" -> "select TOP 1 c.isAlive, c.type, c.age from c"
        )

        // Not passing schema, letting inference work
        val dfWithInference = spark.read.format("cosmos.items").options(cfgWithInference).load()
        val rowsArrayWithInference = dfWithInference.where("isAlive = 'true' and type = 'duck'").collect()
        rowsArrayWithInference should have size 1

        val rowWithInference = rowsArrayWithInference(0)
        rowWithInference.schema.fields should have size 3
        rowWithInference.getAs[String]("type") shouldEqual "duck"
        rowWithInference.getAs[Integer]("age") shouldEqual 20
        rowWithInference.getAs[Boolean]("isAlive") shouldEqual true
    }

  //scalastyle:on magic.number
  //scalastyle:on multiple.string.literals
}
