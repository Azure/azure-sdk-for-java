parameters:
  - name: CosmosEndpoint
    type: string
  - name: CosmosKey
    type: string
  - name: DatabricksEndpoint
    type: string
  - name: DatabricksToken
    type: string
  - name: SparkVersion
    type: string
  - name: ClusterName
    type: string

stages:
  - stage:
    displayName: 'Spark Databricks integration ${{ parameters.SparkVersion }}'
    dependsOn: []
    jobs:
    - job:
      timeoutInMinutes: 40

      pool:
        vmImage: 'ubuntu-20.04'

      steps:
      - task: Maven@3
        displayName: Building spark package
        inputs:
          mavenPOMFile: pom.xml
          goals: 'package'
          options: '$(DefaultOptions) -T 1 -DsparkShading -Ppackage-assembly -DskipTests -Dgpg.skip -Dmaven.javadoc.skip=true -Dcodesnippet.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true -Drevapi.skip=true -pl com.azure:azure-cosmos,com.azure.cosmos.spark:${{ parameters.SparkVersion }}'
          javaHomeOption: 'JDKVersion'
          jdkVersionOption: '1.11'
          jdkArchitectureOption: 'x64'
          publishJUnitResults: false
      - task: UsePythonVersion@0
        displayName: Use Python 3.8
        inputs:
          versionSpec: 3.8
      - task: Bash@3
        displayName: Install Databricks CLI
        inputs:
          targetType: inline
          script: python -m pip install --upgrade pip setuptools wheel databricks-cli
      - task: Bash@3
        displayName: Connect to Databricks workspace
        inputs:
          targetType: inline
          script: >-
            databricks configure --token <<EOF

            $DATABRICKSENDPOINT

            $DATABRICKSTOKEN

            EOF
        env:
          DATABRICKSENDPOINT: ${{ parameters.DatabricksEndpoint }}
          DATABRICKSTOKEN: ${{ parameters.DatabricksToken }}
      - task: Bash@3
        displayName: Importing Jars
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-jar-install.sh
          arguments: '${{ parameters.ClusterName }} $(build.sourcesdirectory)/sdk/cosmos/${{ parameters.SparkVersion }}/target'
      - task: Bash@3
        displayName: Importing and executing notebooks
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-notebooks-install.sh
          arguments: ${{ parameters.ClusterName }} $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/notebooks ${{ parameters.CosmosEndpoint }} ${{ parameters.CosmosKey }}
