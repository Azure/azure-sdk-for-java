parameters:
  - name: CosmosEndpoint
    type: string
  - name: CosmosKey
    type: string
  - name: DatabricksEndpoint
    type: string
  - name: DatabricksToken
    type: string
  - name: SparkVersion
    type: string
  - name: ClusterName
    type: string
  - name: SubscriptionId
    type: string
  - name: TenantId
    type: string
  - name: ResourceGroupName
    type: string
  - name: ClientId
    type: string
  - name: ClientSecret
    type: string
  - name: CosmosContainerName
    type: string
  - name: CosmosDatabaseName
    type: string
  - name: AvoidDBFS
    type: boolean
    default: false
stages:
  - stage:
    displayName: 'Spark Databricks integration ${{ parameters.ClusterName }} - ${{ parameters.SparkVersion }}'
    dependsOn: []
    jobs:
    - job:
      timeoutInMinutes: 40

      pool:
        name: $(LINUXPOOL)
        image: $(LINUXVMIMAGE)
        os: linux

      steps:
      - task: Maven@4
        displayName: Building spark package
        inputs:
          mavenPOMFile: pom.xml
          goals: 'package'
          options: '$(DefaultOptions) -T 1 -Ppackage-assembly -DskipTests -Dgpg.skip -Dmaven.javadoc.skip=true -Dcodesnippet.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true -Drevapi.skip=true -pl com.azure:azure-cosmos,com.azure:azure-cosmos-test,com.azure.cosmos.spark:${{ parameters.SparkVersion }}'
          javaHomeOption: 'JDKVersion'
          jdkVersionOption: '1.11'
          jdkArchitectureOption: 'x64'
          publishJUnitResults: false
      - task: UsePythonVersion@0
        displayName: Use Python $(PythonVersion)
        inputs:
          versionSpec: $(PythonVersion)
      - task: Bash@3
        displayName: Install setuptools and wheel
        inputs:
          targetType: inline
          script: python -m pip install --upgrade pip setuptools wheel
      - task: Bash@3
        displayName: Install Databricks CLI
        inputs:
          targetType: inline
          script: |
            curl -fsSL curl https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
            databricks -v
      - task: Bash@3
        displayName: 'Connect to Databricks workspace $(DatabricksEndpoint)'
        inputs:
          targetType: inline
          script: >-
            databricks configure --token --profile default --host $DATABRICKSENDPOINT <<EOF
          
            $DATABRICKSTOKEN

            EOF
        env:
          DATABRICKSENDPOINT: ${{ parameters.DatabricksEndpoint }}
          DATABRICKSTOKEN: ${{ parameters.DatabricksToken }}
      - task: Bash@3
        displayName: Importing Jars
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-jar-install.sh
          arguments: '${{ parameters.ClusterName }} ${{ parameters.AvoidDBFS }} $(build.sourcesdirectory)/sdk/cosmos/${{ parameters.SparkVersion }}/target'
      - task: Bash@3
        displayName: Importing and executing notebooks
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-notebooks-install.sh
          arguments: ${{ parameters.ClusterName }} $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/notebooks ${{ parameters.CosmosEndpoint }} ${{ parameters.CosmosKey }} ${{ parameters.SubscriptionId }} ${{ parameters.TenantId }} ${{ parameters.ResourceGroupName }} ${{ parameters.ClientId }} ${{ parameters.ClientSecret }} ${{ parameters.CosmosContainerName }} ${{ parameters.CosmosDatabaseName }}
