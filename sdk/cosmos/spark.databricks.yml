parameters:
  - name: CosmosEndpointMsi
    type: string
  - name: CosmosEndpoint
    type: string
  - name: CosmosKey
    type: string
  - name: DatabricksEndpoint
    type: string
  - name: DatabricksToken
    type: string
  - name: SparkVersion
    type: string
  - name: ClusterName
    type: string
  - name: SubscriptionId
    type: string
  - name: TenantId
    type: string
  - name: ResourceGroupName
    type: string
  - name: ClientId
    type: string
  - name: ClientSecret
    type: string
  - name: CosmosContainerName
    type: string
  - name: CosmosDatabaseName
    type: string
  - name: AvoidDBFS
    type: boolean
    default: false
  - name: JarStorageAccountKey
    type: string
  - name: JarReadOnlySasUri
    type: string
stages:
  - stage:
    displayName: 'Spark Databricks integration ${{ parameters.ClusterName }} - ${{ parameters.SparkVersion }}'
    dependsOn: []
    jobs:
    - job:
      timeoutInMinutes: 40

      pool:
        name: $(LINUXPOOL)
        image: $(LINUXVMIMAGE)
        os: linux

      steps:
      - task: Maven@4
        displayName: Building spark package
        inputs:
          mavenPOMFile: pom.xml
          goals: 'package'
          options: '$(DefaultOptions) -T 1 -Ppackage-assembly -DskipTests -Dgpg.skip -Dmaven.javadoc.skip=true -Dcodesnippet.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true -Drevapi.skip=true -pl com.azure:azure-cosmos,com.azure:azure-cosmos-test,com.azure.cosmos.spark:${{ parameters.SparkVersion }}'
          javaHomeOption: 'JDKVersion'
          jdkVersionOption: '1.11'
          jdkArchitectureOption: 'x64'
          publishJUnitResults: false
      - task: UsePythonVersion@0
        displayName: Use Python $(PythonVersion)
        inputs:
          versionSpec: $(PythonVersion)
      - task: Bash@3
        displayName: Install setuptools and wheel
        inputs:
          targetType: inline
          script: python -m pip install --upgrade pip setuptools wheel
      - task: Bash@3
        displayName: Install Databricks CLI
        inputs:
          targetType: inline
          script: |
            curl -fsSL curl https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
            databricks -v
      - task: Bash@3
        displayName: 'Connect to Databricks workspace $(DatabricksEndpoint)'
        inputs:
          targetType: inline
          script: >-
            databricks configure --token --profile DEFAULT --host $DATABRICKSENDPOINT <<EOF
          
            $DATABRICKSTOKEN

            EOF
        env:
          DATABRICKSENDPOINT: ${{ parameters.DatabricksEndpoint }}
          DATABRICKSTOKEN: ${{ parameters.DatabricksToken }}
      - task: Bash@3
        displayName: Importing Jars
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-jar-install.sh
          arguments: '${{ parameters.ClusterName }} ${{ parameters.AvoidDBFS }} $(build.sourcesdirectory)/sdk/cosmos/${{ parameters.SparkVersion }}/target ${{ parameters.JarStorageAccountKey }}'
      - task: Bash@3
        displayName: Importing and executing notebooks
        inputs:
          filePath: $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/databricks-notebooks-install.sh
          arguments: ${{ parameters.ClusterName }} $(build.sourcesdirectory)/sdk/cosmos/azure-cosmos-spark_3_2-12/test-databricks/notebooks ${{ parameters.CosmosEndpointMsi }} ${{ parameters.CosmosEndpoint }} ${{ parameters.CosmosKey }} ${{ parameters.SubscriptionId }} ${{ parameters.TenantId }} ${{ parameters.ResourceGroupName }} ${{ parameters.ClientId }} ${{ parameters.ClientSecret }} ${{ parameters.CosmosContainerName }} ${{ parameters.CosmosDatabaseName }}
      - task: Bash@3
        displayName: 'Validating Checksum'
        inputs:
          targetType: inline
          script: >-
            if [[ "${AVOID_DBFS,,}" == "true" ]]; then
              $JAR_URL=${{ parameters.JarReadOnlySasUri }}
              JAR_NAME='azure-cosmos-spark_3-5_2-12-latest-ci-candidate.jar'
              TMP_FILE="/tmp/${JAR_NAME}.$$"
              echo "[init] Downloading $JAR_NAME to validate checksum..."
              # Download with retries; fail non-zero on HTTP errors
              if command -v curl >/dev/null 2>&1; then
                curl -fL --retry 8 --retry-connrefused --retry-delay 2 --max-time 600 \
                  "$JAR_URL" -o "$TMP_FILE"
              elif command -v wget >/dev/null 2>&1; then
                wget --tries=10 --timeout=60 -O "$TMP_FILE" "$JAR_URL"
              else
                echo "[init][error] Neither curl nor wget is available." >&2
                exit 1
              fi
            
              # Basic validity check (optional but helpful)
              if command -v unzip >/dev/null 2>&1; then
                if ! unzip -tq "$TMP_FILE" >/dev/null 2>&1; then
                  echo "[init][error] Downloaded file is not a valid JAR/ZIP." >&2
                  rm -f "$TMP_FILE"
                  exit 1
                fi
              fi

              CHECKSUM_AFTER_EXECUTION=sha256sum $TMP_FILE
              echo "CheckSum of JAR build on this agent: $JAR_CHECK_SUM"
              echo "CheckSum of jar after notebook execution: $CHECKSUM_AFTER_EXECUTION"
            
              if [[ "$CHECKSUM_AFTER_EXECUTION" != "$(JarCheckSum)" ]]; then
                echo "Checksum mismatch â€” failing." >&2
                exit 1
              fi
            else
              echo "Skipping checksum validation for runtimes still installing jars from DBFS"
            fi
        env:
          JAR_CHECK_SUM: $(JarCheckSum)
          AVOID_DBFS:  ${{ parameters.AvoidDBFS }}
