{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22c6138-537e-43eb-b2df-d23e39ee3687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "cosmosEndpoint = \"https://REPLACEME.documents.azure.com:443/\"\n",
    "cosmosMasterKey = \"REPLACEME\"\n",
    "cosmosDatabaseName = \"sampleDB\"\n",
    "cosmosContainerName = \"sampleContainer\"\n",
    "\n",
    "# Configure Catalog Api to be used\n",
    "spark.conf.set(\"spark.sql.catalog.cosmoscatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmoscatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmoscatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
    "\n",
    "cfg = {\n",
    "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n",
    "  \"spark.cosmos.database\" : cosmosDatabaseName,\n",
    "  \"spark.cosmos.container\" : cosmosContainerName,\n",
    "  \"spark.cosmos.read.partitioning.strategy\" : \"Restrictive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892b48e1-f74f-4acd-8cee-5b19c5fcf43e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an Azure Cosmos DB database using catalog api\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS cosmoscatalog.{};\".format(cosmosDatabaseName))\n",
    "\n",
    "# create an Azure Cosmos DB container with hierarchical partitioning using catalog api\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS cosmoscatalog.{}.{} using cosmos.oltp TBLPROPERTIES(partitionKeyPath = '/tenantId,/userId,/sessionId', manualThroughput = '1100')\".format(cosmosDatabaseName, cosmosContainerName))\n",
    "\n",
    "#ingest some data\n",
    "spark.createDataFrame(((\"id1\", \"tenant 1\", \"User 1\", \"session 1\"), (\"id2\", \"tenant 1\", \"User 1\", \"session 1\"), (\"id3\", \"tenant 2\", \"User 1\", \"session 1\"))) \\\n",
    "  .toDF(\"id\",\"tenantId\",\"userId\",\"sessionId\") \\\n",
    "   .write \\\n",
    "   .format(\"cosmos.oltp\") \\\n",
    "   .options(**cfg) \\\n",
    "   .mode(\"APPEND\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7fbb59-0e34-40bf-a11b-d8c0dd1ce8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#query by filtering the first two levels in the hierarchy without feedRangeFilter - this is less efficient as it will go through all physical partitions\n",
    "query_df = spark.read.format(\"cosmos.oltp\").options(**cfg) \\\n",
    ".option(\"spark.cosmos.read.customQuery\" , \"SELECT * from c where c.tenantId = 'tenant 1' and c.userId = 'User 1'\").load()\n",
    "query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc16142a-0b29-4c2c-8f5b-6a75b29c3b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare feed range to filter on first two levels in the hierarchy\n",
    "spark.udf.registerJavaFunction(\"GetFeedRangeForPartitionKey\", \"com.azure.cosmos.spark.udf.GetFeedRangeForHierarchicalPartitionKeyValues\", StringType())\n",
    "pkDefinition = \"{\\\"paths\\\":[\\\"/tenantId\\\",\\\"/userId\\\",\\\"/sessionId\\\"],\\\"kind\\\":\\\"MultiHash\\\"}\"\n",
    "pkValues = \"[\\\"tenant 1\\\", \\\"User 1\\\"]\"\n",
    "feedRangeDf = spark.sql(f\"SELECT GetFeedRangeForPartitionKey('{pkDefinition}', '{pkValues}')\")\n",
    "feedRange = feedRangeDf.collect()[0][0]\n",
    "\n",
    "# query by filtering the first two levels in the hierarchy using feedRangeFilter (will target the physical partition in which all sub-partitions are co-located)\n",
    "query_df = spark.read.format(\"cosmos.oltp\").options(**cfg).option(\"spark.cosmos.partitioning.feedRangeFilter\",feedRange).load()\n",
    "query_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hierarchical-partition-key-sample",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
