{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Secrets**\n\nThe secrets below  like the Cosmos account key are retrieved from a secret scope. If you don't have defined a secret scope for a Cosmos Account you want to use when going through this sample you can find the instructions on how to create one here:\n- Here you can [Create a new secret scope](./#secrets/createScope) for the current Databricks workspace\n  - See how you can create an [Azure Key Vault backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope) \n  - See how you can create a [Databricks backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope)\n- And here you can find information on how to [add secrets to your Spark configuration](https://docs.microsoft.com/azure/databricks/security/secrets/secrets#read-a-secret)\nIf you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b08b1d81-028c-4dba-ac24-b23ceebc9e38"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosmosEndpoint = spark.conf.get(\"spark.cosmos.accountEndpoint\")\ncosmosMasterKey = spark.conf.get(\"spark.cosmos.accountKey\")\n\n# It is allowed to use different account for throughput control. So please choose the accountEndpoint and accountKey accordingly.\n# throughputControlEndpoint = spark.conf.get(\"spark.cosmos.throughputControlAccountEndpoint\")\n# throughputControlMasterKey = spark.conf.get(\"spark.cosmos.throughputControlAccountKey\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "74303a6e-555c-44e2-8255-c8aea71caf03"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - creating the Cosmos DB container to ingest the data into**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "42a7c2d3-1562-46e9-a2dc-4421dbda7a99"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the Catalog API to be used for main workload"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2ce582cf-5601-4827-8680-89bb3c0fa846"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b6d2168c-f06a-4445-8bd1-f7e1d0a902cb"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the Catalog API to be used for throughput control. This will only be needed if different account is used for throughput control"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d4374e4b-07fb-401a-a229-6fd1f1e06948"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import uuid\n# spark.conf.set(\"spark.sql.catalog.throughputControlCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n# spark.conf.set(\"spark.sql.catalog.throughputControlCatalog.spark.cosmos.accountEndpoint\", throughputControlEndpoint)\n# spark.conf.set(\"spark.sql.catalog.throughputControlCatalog.spark.cosmos.accountKey\", throughputControlMasterKey)\n# spark.conf.set(\"spark.sql.catalog.throughputControlCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "08d86e35-ec72-41ab-9214-a58e5c84cd0a"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "And execute the command to create the new container with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed. We will also create a second container that will be used to store metadata for the global throughput control"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "24ef8b89-25ee-4477-a4e7-208fabab15fe"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nCREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\n\nCREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\nUSING cosmos.oltp\nTBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n\nCREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSink\nUSING cosmos.oltp\nTBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n\n/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\n/* If you are using a different account for throughput control, then please reference following commented examples */\nCREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\nUSING cosmos.oltp\nOPTIONS(spark.cosmos.database = 'SampleDatabase')\nTBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');\n\n-- /* If you are using a different account for throughput control, then please use throughput control catalog account for initializing containers */\n-- CREATE DATABASE IF NOT EXISTS throughputControlCatalog.SampleDatabase;\n\n-- CREATE TABLE IF NOT EXISTS throughputControlCatalog.SampleDatabase.ThroughputControl\n-- USING cosmos.oltp\n-- OPTIONS(spark.cosmos.database = 'SampleDatabase')\n-- TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c61c309d-7a20-4488-a4e9-59bbaae19c96"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - loading data source \"[NYC Taxi & Limousine Commission - green taxi trip records](https://azure.microsoft.com/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/)\"**\n\nThe green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. This data set has over 80 million records (>8 GB) of data and is available via a publicly accessible Azure Blob Storage Account located in the East-US Azure region."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f719dedb-d782-4b54-9076-6d29da946950"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\nimport time\nimport uuid\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, LongType\n\nprint(\"Starting preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"nyctlc\"\nblob_relative_path = \"green\"\nblob_sas_token = r\"\"\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n# SPARK read parquet, note that it won't load any data yet by now\n# NOTE - if you want to experiment with larger dataset sizes - consider switching to Option B (commenting code \n# for Option A/uncommenting code for option B) the lines below or increase the value passed into the \n# limit function restricting the dataset size below\n\n#------------------------------------------------------------------------------------\n# Option A - with limited dataset size\n#------------------------------------------------------------------------------------\ndf_rawInputWithoutLimit = spark.read.parquet(wasbs_path)\npartitionCount = df_rawInputWithoutLimit.rdd.getNumPartitions()\ndf_rawInput = df_rawInputWithoutLimit.limit(1_000_000).repartition(partitionCount)\ndf_rawInput.persist()\n\n#------------------------------------------------------------------------------------\n# Option B - entire dataset\n#------------------------------------------------------------------------------------\n#df_rawInput = spark.read.parquet(wasbs_path)\n\n# Adding an id column with unique values\nuuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\nnowUdf= udf(lambda : int(time.time() * 1000),LongType())\ndf_input_withId = df_rawInput \\\n  .withColumn(\"id\", uuidUdf()) \\\n  .withColumn(\"insertedAt\", nowUdf()) \\\n\nprint('Register the DataFrame as a SQL temporary view: source')\ndf_input_withId.createOrReplaceTempView('source')\nprint(\"Finished preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "080c50a8-2d33-4f0f-9fba-c5b4212b7630"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Sample - ingesting the NYC Green Taxi data into Cosmos DB**\n\nBy setting the target throughput threshold to 0.95 (95%) we reduce throttling but still allow the ingestion to consume most of the provisioned throughput. For scenarios where ingestion should only take a smaller subset of the available throughput this threshold can be reduced accordingly."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2805de61-e652-4be6-a3e5-b084cd848dc9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\nimport datetime\n\nprint(\"Starting ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\nwriteCfg = {\n  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n  \"spark.cosmos.accountKey\": cosmosMasterKey,\n  \"spark.cosmos.database\": \"SampleDatabase\",\n  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n  \"spark.cosmos.write.bulk.enabled\": \"true\",\n  \"spark.cosmos.throughputControl.enabled\": \"true\",\n#   \"spark.cosmos.throughputControl.accountEndpoint\": throughputControlEndpoint, # Only need if throughput control is configured with different database account\n#   \"spark.cosmos.throughputControl.accountKey\": throughputControlMasterKey, # Only need if throughput control is configured with different database account\n  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestion\",\n  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.95\",\n  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\n  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\n}\n\ndf_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\n\ndf_NYCGreenTaxi_Input \\\n  .write \\\n  .format(\"cosmos.oltp\") \\\n  .mode(\"Append\") \\\n  .options(**writeCfg) \\\n  .save()\n\nprint(\"Finished ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56fc9432-df8b-456d-97c1-498c90938ed3"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the reference record count**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a84d335d-b0f1-480f-8826-07b66ad1f1d8"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_source = spark.sql('SELECT * FROM source').count()\nprint(\"Number of records in source: \", count_source) "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dd176767-080a-4755-8cfd-4d89602b7c5c"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via query**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4f8b1a3b-44ed-4117-aae6-09c85bb86981"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\nimport pyspark.sql.functions as F\n\nprint(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\nreadCfg = {\n  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n  \"spark.cosmos.accountKey\": cosmosMasterKey,\n  \"spark.cosmos.database\": \"SampleDatabase\",\n  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n  \"spark.cosmos.read.partitioning.strategy\": \"Restrictive\",#IMPORTANT - any other partitioning strategy will result in indexing not being use to count - so latency and RU would spike up\n  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n  \"spark.cosmos.read.customQuery\" : \"SELECT COUNT(0) AS Count FROM c\"\n}\n\ncount_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\nquery_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\ncount_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\nprint(\"Number of records retrieved via query: \", count_query) \nprint(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\nassert count_source == count_query"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6c56441d-c641-4751-99af-fcd9461da033"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via change feed**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fc07b9a6-5021-49cb-acb6-1c345caab955"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\nchangeFeedCfg = {\n  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n  \"spark.cosmos.accountKey\": cosmosMasterKey,\n  \"spark.cosmos.database\": \"SampleDatabase\",\n  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n  \"spark.cosmos.changeFeed.startFrom\" : \"Beginning\",\n  \"spark.cosmos.changeFeed.mode\" : \"Incremental\"\n}\nchangeFeed_df = spark.read.format(\"cosmos.oltp.changeFeed\").options(**changeFeedCfg).load()\ncount_changeFeed = changeFeed_df.count()\nprint(\"Number of records retrieved via change feed: \", count_changeFeed) \nprint(\"Finished validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\nassert count_source == count_changeFeed"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e02a1e59-1dcd-4e9d-b8b3-20028798f710"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - bulk deleting documents and validating document count afterwards**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f43c9e10-95ec-4059-a764-2268472877fd"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n\nprint(\"Starting to identify to be deleted documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\nreadCfg = {\n  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n  \"spark.cosmos.accountKey\": cosmosMasterKey,\n  \"spark.cosmos.database\": \"SampleDatabase\",\n  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n}\n\ntoBeDeleted_df = spark.read.format(\"cosmos.oltp\").options(**readCfg).load().limit(100_000)\nprint(\"Number of records to be deleted: \", toBeDeleted_df.count()) \n\nprint(\"Starting to bulk delete documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\ndeleteCfg = writeCfg.copy()\ndeleteCfg[\"spark.cosmos.write.strategy\"] = \"ItemDelete\"\ntoBeDeleted_df \\\n        .write \\\n        .format(\"cosmos.oltp\") \\\n        .mode(\"Append\") \\\n        .options(**deleteCfg) \\\n        .save()\nprint(\"Finished deleting documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\nprint(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\ncount_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\nreadCfg[\"spark.cosmos.read.customQuery\"] = \"SELECT COUNT(0) AS Count FROM c\"\nquery_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\ncount_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\nprint(\"Number of records retrieved via query: \", count_query) \nprint(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n\nassert max(0, count_source - 100_000) == count_query"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "63be3f23-ae76-4813-8bdb-40733e56e087"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - showing the existing Containers**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "907f3933-5008-45d1-bba6-a426ca13c823"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nSHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "883d1de2-7fdc-45f8-a1e3-1cf53c3b9855"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\nassert df_Tables.count() == 3"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b027d779-b88b-4dab-bc0d-61e3fb35d2ce"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container via Spark Catalog**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "10d6d127-7db3-479b-aeef-c45d595a3885"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nSELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords LIMIT 10"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5b9e59a7-5318-4a6f-b9a8-60e89c6627d6"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container with custom settings via Spark Catalog**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "456b891c-e973-4bf9-97f0-a9151ed93196"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the view with custom settings (in this case adding a projection, disabling schema inference and switching to aggressive partitioning strategy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a321f36f-c1f6-4b2b-aa79-5cf00f56bf3e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nCREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsView \n  (id STRING, _ts TIMESTAMP, vendorID INT, totalAmount DOUBLE)\nUSING cosmos.oltp\nTBLPROPERTIES(isCosmosView = 'True')\nOPTIONS (\n  spark.cosmos.database = 'SampleDatabase',\n  spark.cosmos.container = 'GreenTaxiRecords',\n  spark.cosmos.read.inferSchema.enabled = 'False',\n  spark.cosmos.read.inferSchema.includeSystemProperties = 'True',\n  spark.cosmos.read.partitioning.strategy = 'Aggressive');\n\nSELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsView LIMIT 10"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4830781a-2c67-4a1d-bb0d-a64655142fa1"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating another view with custom settings (in this case enabling schema inference and switching to restrictive partitioning strategy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "800d868e-1d32-4eee-8a1d-0c947d0af715"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nCREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView \nUSING cosmos.oltp\nTBLPROPERTIES(isCosmosView = 'True')\nOPTIONS (\n  spark.cosmos.database = 'SampleDatabase',\n  spark.cosmos.container = 'GreenTaxiRecords',\n  spark.cosmos.read.inferSchema.enabled = 'True',\n  spark.cosmos.read.inferSchema.includeSystemProperties = 'False',\n  spark.cosmos.read.partitioning.strategy = 'Restrictive');\n\nSELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView LIMIT 10"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cea14de6-4be9-4cb7-b6d2-00928a75f912"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show all Tables in the Cosmos Catalog to show that both the \"real\" Containers as well as the views show-up"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "23cc237d-251e-4f31-a73f-4e2786316eaf"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nSHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2ad2e367-b5d8-42b3-8a56-2505869eae6b"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\nassert df_Tables.count() == 5"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb0df38d-27ef-462d-8181-8df3d676e86a"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleanup the views again**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "865ccc18-5342-4de2-969b-1e0670272797"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\nDROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsView;\nDROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView;\nSHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1ff1f132-8970-4a33-ac84-241df8e4f57f"
        }
      },
      "outputs": [],
      "execution_count": 0
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\nassert df_Tables.count() == 3"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "85142b5a-c14b-4b93-bf9e-2bc7f9e2c3f5"
        }
      },
      "outputs": [],
      "execution_count": 0
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "01_Batch",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 86486029782769
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}