{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd772ff5-deea-4ad4-8883-4e6081110c23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Secrets**\n",
    "\n",
    "The secrets below  like the Cosmos account key are retrieved from a secret scope. If you don't have defined a secret scope for a Cosmos Account you want to use when going through this sample you can find the instructions on how to create one here:\n",
    "- Here you can [Create a new secret scope](./#secrets/createScope) for the current Databricks workspace\n",
    "  - See how you can create an [Azure Key Vault backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope) \n",
    "  - See how you can create a [Databricks backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope)\n",
    "- And here you can find information on how to [add secrets to your Spark configuration](https://docs.microsoft.com/azure/databricks/security/secrets/secrets#read-a-secret)\n",
    "If you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47be48a7-407c-4f0d-822b-8add181ed872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cosmosEndpoint = spark.conf.get(\"spark.cosmos.accountEndpoint\")\n",
    "cosmosMasterKey = spark.conf.get(\"spark.cosmos.accountKey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1477a60-c5d4-4333-a89e-42dbdc06d64b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - creating the Cosmos DB container to ingest the data into**\n",
    "\n",
    "Configure the Catalog API to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "373ae245-aa8b-4c94-997f-bb6335fdad8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4d27ade-f3d1-42e2-bc4d-4f012096208c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And execute the command to create the new container with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed. We will also create a second container that will be used to store metadata for the global throughput control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58567fbc-6932-4ab8-9bc4-9c5cee81b94b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSink\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
    "\n",
    "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\n",
    "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\n",
    "USING cosmos.oltp\n",
    "OPTIONS(spark.cosmos.database = 'SampleDatabase')\n",
    "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "439a7628-2b2d-4059-ad37-7172f69cc073",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Preparation - create a runId (which identifies the logical streaming query and progress/bookmarks/offset are stored for this query) - so when running a query with the same runId it would continue on the offsets persisted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b9f37b0-f4b5-49a0-b239-621c139b16a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "runId = str(uuid.uuid4())\n",
    "print(\"Run ID: \", runId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a7ebf34-7e7b-4ca4-ab3a-861f2a1f4320",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Sample - running the streaming query**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0e3cb49-4658-4ead-bbe9-93731d4c07e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Waiting until at least one record is available in the source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99c5d323-8fb9-4422-be2a-d3589700e359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sourceRecordCount = 0\n",
    "emptyCount = 0\n",
    "while (sourceRecordCount == 0):\n",
    "  records_DF = spark.sql('SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords LIMIT 1')\n",
    "  sourceRecordCount = records_DF.count() \n",
    "  if (sourceRecordCount == 0):\n",
    "    emptyCount += 1\n",
    "    time.sleep(emptyCount * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfe944c9-300e-488d-a3fd-e0e5161b3c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Running the query for a couple of times - the way it is done (processAllAvailable) is just for debugging purposes - repeating this a couple of times is necessary because ingestion might happen while running this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc6b8289-968d-40a9-af45-6881308af26f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "changeFeedCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
    "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
    "  \"spark.cosmos.read.inferSchema.enabled\" : \"true\",\n",
    "  \"spark.cosmos.read.inferSchema.forceNullableProperties\" : \"true\",\n",
    "  \"spark.cosmos.changeFeed.startFrom\" : \"Beginning\",\n",
    "  \"spark.cosmos.changeFeed.mode\" : \"Incremental\"\n",
    "  #\"spark.cosmos.changeFeed.maxItemCountPerTriggerHint\" : \"50000\"\n",
    "}\n",
    "\n",
    "writeCfg = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
    "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
    "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\n",
    "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n",
    "  \"spark.cosmos.write.bulk.enabled\": \"true\",\n",
    "  \"checkpointLocation\": \"/tmp/\" + runId + \"/\"\n",
    "}\n",
    "\n",
    "idleCount = 0\n",
    "lastProgressJson = \"n/a\"\n",
    "while idleCount <= 5:\n",
    "  print(\"IdleCount: \", idleCount)\n",
    "\n",
    "  changeFeedDF = spark \\\n",
    "    .readStream  \\\n",
    "    .format(\"cosmos.oltp.changeFeed\") \\\n",
    "    .options(**changeFeedCfg) \\\n",
    "    .load()\n",
    " \n",
    "  nowUdf= udf(lambda : int(time.time() * 1000),LongType())\n",
    "  df_withTimestamps = changeFeedDF \\\n",
    "    .withColumnRenamed(\"_ts\",\"original_ts\") \\\n",
    "    .withColumnRenamed(\"insertedAt\",\"original_insertedAt\") \\\n",
    "    .withColumn(\"copiedAt\", nowUdf())\n",
    "\n",
    "  print(\"Starting to copy records: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "  microBatchQuery = df_withTimestamps \\\n",
    "    .writeStream \\\n",
    "    .format(\"cosmos.oltp\") \\\n",
    "    .queryName(runId) \\\n",
    "    .options(**writeCfg) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "  microBatchQuery.processAllAvailable()\n",
    "\n",
    "  if (microBatchQuery.lastProgress[\"numInputRows\"] == 0):\n",
    "    print(\"No progress - sleeping for \", str(5 * idleCount), \" seconds\")\n",
    "    idleCount += 1\n",
    "    if (idleCount < 5):\n",
    "      time.sleep(5 * idleCount)\n",
    "\n",
    "  microBatchQuery.stop()  \n",
    "\n",
    "  print(\"Finished copying records: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "  \n",
    "print(\"No more activity expected - terminating loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6597670-e30a-4339-8af7-d03fa6464a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Validation - ensuring that the record count in the target container is identical with the one in the source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1be8a94a-d554-4659-94e7-ebf673402c4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSinkView \n",
    "  (id STRING)\n",
    "USING cosmos.oltp\n",
    "TBLPROPERTIES(isCosmosView = 'True')\n",
    "OPTIONS (\n",
    "  spark.cosmos.database = 'SampleDatabase',\n",
    "  spark.cosmos.container = 'GreenTaxiRecordsCFSink',\n",
    "  spark.cosmos.read.inferSchema.enabled = 'False',\n",
    "  spark.cosmos.read.partitioning.strategy = 'Default');\n",
    "\n",
    "SELECT COUNT(*) FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSinkView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ebda77f0-3936-4bbf-987a-c432873e7978",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "***NOTE*** the query below is pretty inefficient because Spark 3.1 doesn't allow DataSources to push-down aggregates yet (so count is happening in Spark after retrieving all records). Aggregate push-down is planned for Spark 3.2 - we will also consider allowing to execute custom Cosmos queries which would provide a workaround in these cases for Spark 3.1 soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88bc2cea-5247-45fa-91e1-0d3931e4194b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_source = spark.sql('SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords')\n",
    "df_target = spark.sql('SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSinkView')\n",
    "assert df_source.count() == df_target.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcb993de-0c8a-4f06-ac8c-562e7c41eaec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Cleaning up the view **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "026422bf-2daa-4573-9159-58497ca2f94a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSinkView;\n",
    "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_StructuredStreaming",
   "notebookOrigID": 4465502912943280,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
