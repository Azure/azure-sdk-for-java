{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Secrets\r\n",
        "\r\n",
        "## In Databricks\r\n",
        "The secrets below like the Cosmos account key are retrieved from a secret scope. If you don't have defined a secret scope for a Cosmos Account you want to use when going through this sample you can find the instructions on how to create one here:\r\n",
        "\r\n",
        "- Here you can Create a new secret scope for the current Databricks workspace\r\n",
        "  - See how you can create an [Azure Key Vault backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)\r\n",
        "  - See how you can create a [Databricks backed secret scope](https://docs.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope)\r\n",
        "- And here you can find information on how to [add secrets to your Spark configuration](https://docs.microsoft.com/azure/databricks/security/secrets/secrets#read-a-secret).\r\n",
        "\r\n",
        "## In Synapse\r\n",
        "- You can find instructions on how to define a Cosmos DB account as linked service (with safe handling of secrets) [here](https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-connect-synapse-link-cosmos-db#connect-an-azure-cosmos-db-database-to-an-azure-synapse-workspace)\r\n",
        "\r\n",
        "If you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string cosmosEndpoint = spark.Conf().Get(\"spark.cosmos.accountEndpoint\");\r\n",
        "string cosmosMasterKey = spark.Conf().Get(\"spark.cosmos.accountKey\");\r\n",
        "\r\n",
        "Console.WriteLine($\"Cosmos Account endpoint: {cosmosEndpoint}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - creating the Cosmos DB container to ingest the data into**\r\n",
        "\r\n",
        "Configure the Catalog API to be used"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\");\r\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint);\r\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey);\r\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", $\"/viewDefinitions/{Guid.NewGuid().ToString()}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And execute the command to create the new container with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed. We will also create a second container that will be used to store metadata for the global throughput control"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\r\n",
        "USING cosmos.oltp\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\r\n",
        "\r\n",
        "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\r\n",
        "USING cosmos.oltp\r\n",
        "OPTIONS(spark.cosmos.database = 'SampleDatabase')\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - loading data source \\\"[NYC Taxi & Limousine Commission - green taxi trip records](https://azure.microsoft.com/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/)\\\"**\r\n",
        "\r\n",
        "The green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. This data set has over 80 million records (>8 GB) of data and is available via a publicly accessible Azure Blob Storage Account located in the East-US Azure region."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Sql;\r\n",
        "using Microsoft.Spark.Sql.Types;\r\n",
        "using static Microsoft.Spark.Sql.Functions;\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting preparation: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "// Azure storage access info\r\n",
        "string blob_account_name = \"azureopendatastorage\";\r\n",
        "string blob_container_name = \"nyctlc\";\r\n",
        "string blob_relative_path = \"green\";\r\n",
        "string blob_sas_token = String.Empty;\r\n",
        "\r\n",
        "// Allow SPARK to read from Blob remotely\r\n",
        "string wasbs_path = $\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}\";\r\n",
        "spark.Conf().Set(\r\n",
        "  $\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\r\n",
        "  blob_sas_token);\r\n",
        "Console.WriteLine($\"Remote blob path: {wasbs_path}\");\r\n",
        "\r\n",
        "// SPARK read parquet, note that it won't load any data yet by now\r\n",
        "// NOTE - if you want to experiment with larger dataset sizes - consider switching to Option B (commenting code \r\n",
        "// for Option A/uncommenting code for option B) the lines below or increase the value passed into the \r\n",
        "// limit function restricting the dataset size below\r\n",
        "\r\n",
        "// ------------------------------------------------------------------------------------\r\n",
        "//  Option A - with limited dataset size\r\n",
        "// ------------------------------------------------------------------------------------\r\n",
        "DataFrame df_rawInputWithoutLimit = spark.Read().Parquet(wasbs_path);\r\n",
        "DataFrame df_rawInput = df_rawInputWithoutLimit.Limit(1_000_000);\r\n",
        "\r\n",
        "// ------------------------------------------------------------------------------------\r\n",
        "//  Option B - entire dataset\r\n",
        "// ------------------------------------------------------------------------------------\r\n",
        "// DataFrame df_rawInput = spark.Read().Parquet(wasbs_path)\r\n",
        "\r\n",
        "// Adding an id column with unique values\r\n",
        "Func<Column> uuidUdf= Udf<string>(() => Guid.NewGuid().ToString());\r\n",
        "DataFrame df_input_withId = df_rawInput.WithColumn(\"id\", uuidUdf()).Persist(); \r\n",
        "\r\n",
        "Console.WriteLine(\"Register the DataFrame as a SQL temporary view: source\");\r\n",
        "df_input_withId.CreateOrReplaceTempView(\"source\");\r\n",
        "\r\n",
        "Console.WriteLine($\"Finished preparation: {DateTimeOffset.UtcNow.ToString(\"o\")}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - ingesting the NYC Green Taxi data into Cosmos DB**\r\n",
        "\r\n",
        "By setting the target throughput threshold to 0.95 (95%) we reduce throttling but still allow the ingestion to consume most of the provisioned throughput. For scenarios where ingestion should only take a smaller subset of the available throughput this threshold can be reduced accordingly."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Console.WriteLine($\"Starting ingestion: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "var writeCfg = new Dictionary<string, string>() {\r\n",
        "  { \"spark.cosmos.accountEndpoint\",  cosmosEndpoint },\r\n",
        "  { \"spark.cosmos.accountKey\", cosmosMasterKey },\r\n",
        "  { \"spark.cosmos.database\", \"SampleDatabase\" },\r\n",
        "  { \"spark.cosmos.container\", \"GreenTaxiRecords\" },\r\n",
        "  { \"spark.cosmos.write.strategy\", \"ItemOverwrite\" },\r\n",
        "  { \"spark.cosmos.write.bulk.enabled\", \"true\" },\r\n",
        "  { \"spark.cosmos.throughputControl.enabled\", \"true\" },\r\n",
        "  { \"spark.cosmos.throughputControl.name\", \"NYCGreenTaxiDataIngestion\" },\r\n",
        "  { \"spark.cosmos.throughputControl.targetThroughputThreshold\", \"0.95\" },\r\n",
        "  { \"spark.cosmos.throughputControl.globalControl.database\", \"SampleDatabase\" },\r\n",
        "  { \"spark.cosmos.throughputControl.globalControl.container\", \"ThroughputControl\" },\r\n",
        "};\r\n",
        "\r\n",
        "DataFrame df_NYCGreenTaxi_Input = spark.Sql(\"SELECT * FROM source\");\r\n",
        "\r\n",
        "df_NYCGreenTaxi_Input\r\n",
        "  .Write()\r\n",
        "  .Format(\"cosmos.oltp\")\r\n",
        "  .Mode(\"Append\")\r\n",
        "  .Options(writeCfg)\r\n",
        "  .Save();\r\n",
        "\r\n",
        "Console.WriteLine($\"Finished ingestion: {DateTimeOffset.UtcNow.ToString(\"o\")}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the reference record count**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long count_source = spark.Sql(\"SELECT * FROM source\").Count();\r\n",
        "\r\n",
        "Console.WriteLine($\"Number of records in source:  {count_source}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via query**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Sql;\r\n",
        "using Microsoft.Spark.Sql.Types;\r\n",
        "using static Microsoft.Spark.Sql.Functions;\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting validation via query: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "var readCfg = new Dictionary<string, string>() {\r\n",
        "  { \"spark.cosmos.accountEndpoint\", cosmosEndpoint },\r\n",
        "  { \"spark.cosmos.accountKey\", cosmosMasterKey },\r\n",
        "  { \"spark.cosmos.database\", \"SampleDatabase\" },\r\n",
        "  { \"spark.cosmos.container\", \"GreenTaxiRecords\" },\r\n",
        "\r\n",
        "  //IMPORTANT - any other partitioning strategy will result in indexing not being \r\n",
        "  // used to count - so latency and RU would spike up\r\n",
        "  { \"spark.cosmos.read.partitioning.strategy\", \"Restrictive\" }, \r\n",
        "  \r\n",
        "  { \"spark.cosmos.read.inferSchema.enabled\", \"false\" },\r\n",
        "  { \"spark.cosmos.read.customQuery\", \"SELECT COUNT(0) AS Count FROM c\" }\r\n",
        "};\r\n",
        "\r\n",
        "var count_query_schema=new StructType(new [] {new StructField(\"Count\", new LongType(), true)});\r\n",
        "DataFrame query_df = spark\r\n",
        "    .Read()\r\n",
        "    .Format(\"cosmos.oltp\")\r\n",
        "    .Schema(count_query_schema)\r\n",
        "    .Options(readCfg)\r\n",
        "    .Load();\r\n",
        ";\r\n",
        "int count_query = query_df.Select(Functions.Sum(\"Count\").Alias(\"TotalCount\")).First().GetAs<int>(\"TotalCount\");\r\n",
        "Console.WriteLine($\"Number of records retrieved via query: {count_query}\");\r\n",
        "Console.WriteLine($\"Finished validation via query: {DateTimeOffset.UtcNow.ToString(\"o\")}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via change feed**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Sql;\r\n",
        "using Microsoft.Spark.Sql.Types;\r\n",
        "using static Microsoft.Spark.Sql.Functions;\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting validation via change feed: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "var changeFeedCfg  = new Dictionary<string, string>() {\r\n",
        "  { \"spark.cosmos.accountEndpoint\", cosmosEndpoint },\r\n",
        "  { \"spark.cosmos.accountKey\", cosmosMasterKey },\r\n",
        "  { \"spark.cosmos.database\", \"SampleDatabase\" },\r\n",
        "  { \"spark.cosmos.container\", \"GreenTaxiRecords\" },\r\n",
        "  { \"spark.cosmos.read.partitioning.strategy\", \"Default\" }, \r\n",
        "  { \"spark.cosmos.read.inferSchema.enabled\", \"false\" },\r\n",
        "  { \"spark.cosmos.changeFeed.startFrom\", \"Beginning\" },\r\n",
        "  { \"spark.cosmos.changeFeed.mode\", \"Incremental\" }\r\n",
        "};\r\n",
        "\r\n",
        "DataFrame changeFeed_df = spark\r\n",
        "    .Read()\r\n",
        "    .Format(\"cosmos.oltp.changeFeed\")\r\n",
        "    .Options(changeFeedCfg)\r\n",
        "    .Load();\r\n",
        ";\r\n",
        "long count_changeFeed  = changeFeed_df.Count();\r\n",
        "Console.WriteLine($\"Number of records retrieved via change feed: {count_changeFeed}\");\r\n",
        "Console.WriteLine($\"Finished validation via change feed: {DateTimeOffset.UtcNow.ToString(\"o\")}\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - bulk deleting documents and validating document count afterwards**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Sql;\r\n",
        "using Microsoft.Spark.Sql.Types;\r\n",
        "using static Microsoft.Spark.Sql.Functions;\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting to identify to be deleted documents: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "var readCfg = new Dictionary<string, string>() {\r\n",
        "  { \"spark.cosmos.accountEndpoint\", cosmosEndpoint },\r\n",
        "  { \"spark.cosmos.accountKey\", cosmosMasterKey },\r\n",
        "  { \"spark.cosmos.database\", \"SampleDatabase\" },\r\n",
        "  { \"spark.cosmos.container\", \"GreenTaxiRecords\" },\r\n",
        "  { \"spark.cosmos.read.partitioning.strategy\", \"Default\" }, \r\n",
        "  { \"spark.cosmos.read.inferSchema.enabled\", \"false\" }\r\n",
        "};\r\n",
        "\r\n",
        "DataFrame toBeDeleted_df = spark\r\n",
        "    .Read()\r\n",
        "    .Format(\"cosmos.oltp\")\r\n",
        "    .Options(readCfg)\r\n",
        "    .Load()\r\n",
        "    .Limit(100_000);\r\n",
        "Console.WriteLine($\"Number of records to be deleted: {toBeDeleted_df.Count()}\");\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting to bulk delete documents: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "var deleteCfg = new Dictionary<string, string>(writeCfg);\r\n",
        "deleteCfg[\"spark.cosmos.write.strategy\"] = \"ItemDelete\";\r\n",
        "\r\n",
        "toBeDeleted_df\r\n",
        "  .Write()\r\n",
        "  .Format(\"cosmos.oltp\")\r\n",
        "  .Mode(\"Append\")\r\n",
        "  .Options(deleteCfg)\r\n",
        "  .Save();\r\n",
        "\r\n",
        "Console.WriteLine($\"Finished deleting documents: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "Console.WriteLine($\"Starting count validation via query: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n",
        "\r\n",
        "var count_query_schema=new StructType(new [] {new StructField(\"Count\", new LongType(), true)});\r\n",
        "readCfg[\"spark.cosmos.read.customQuery\"] = \"SELECT COUNT(0) AS Count FROM c\";\r\n",
        "DataFrame query_df = spark\r\n",
        "    .Read()\r\n",
        "    .Format(\"cosmos.oltp\")\r\n",
        "    .Schema(count_query_schema)\r\n",
        "    .Options(readCfg)\r\n",
        "    .Load();\r\n",
        "\r\n",
        "int count_query = query_df.Select(Functions.Sum(\"Count\").Alias(\"TotalCount\")).First().GetAs<int>(\"TotalCount\");\r\n",
        "Console.WriteLine($\"Number of records retrieved via query: {count_query}\");\r\n",
        "Console.WriteLine($\"Finished validation via query: {DateTimeOffset.UtcNow.ToString(\"o\")}\");\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - showing the existing Containers**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System.Diagnostics;\r\n",
        "\r\n",
        "var df_Tables = spark.Sql(\"SHOW TABLES FROM cosmosCatalog.SampleDatabase\");\r\n",
        "Trace.Assert(df_Tables.Count() == 3);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container via Spark Catalog**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container with custom settings via Spark Catalog**\r\n",
        "\r\n",
        "Creating the view with custom settings (in this case adding a projection, disabling schema inference and switching to aggressive partitioning strategy)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsView \r\n",
        "  (id STRING, _ts TIMESTAMP, vendorID INT, totalAmount DOUBLE)\r\n",
        "USING cosmos.oltp\r\n",
        "TBLPROPERTIES(isCosmosView = 'True')\r\n",
        "OPTIONS (\r\n",
        "  spark.cosmos.database = 'SampleDatabase',\r\n",
        "  spark.cosmos.container = 'GreenTaxiRecords',\r\n",
        "  spark.cosmos.read.inferSchema.enabled = 'False',\r\n",
        "  spark.cosmos.read.inferSchema.includeSystemProperties = 'True',\r\n",
        "  spark.cosmos.read.partitioning.strategy = 'Aggressive');\r\n",
        "\r\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsView LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating another view with custom settings (in this case enabling schema inference and switching to restrictive partitioning strategy)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView \r\n",
        "USING cosmos.oltp\r\n",
        "TBLPROPERTIES(isCosmosView = 'True')\r\n",
        "OPTIONS (\r\n",
        "  spark.cosmos.database = 'SampleDatabase',\r\n",
        "  spark.cosmos.container = 'GreenTaxiRecords',\r\n",
        "  spark.cosmos.read.inferSchema.enabled = 'True',\r\n",
        "  spark.cosmos.read.inferSchema.includeSystemProperties = 'False',\r\n",
        "  spark.cosmos.read.partitioning.strategy = 'Restrictive');\r\n",
        "\r\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show all Tables in the Cosmos Catalog to show that both the \"real\" Containers as well as the views show-up"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System.Diagnostics;\r\n",
        "\r\n",
        "var df_Tables = spark.Sql(\"SHOW TABLES FROM cosmosCatalog.SampleDatabase\");\r\n",
        "Trace.Assert(df_Tables.Count() == 5);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleanup the views again**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsView;\r\n",
        "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView;\r\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System.Diagnostics;\r\n",
        "\r\n",
        "var df_Tables = spark.Sql(\"SHOW TABLES FROM cosmosCatalog.SampleDatabase\");\r\n",
        "Trace.Assert(df_Tables.Count() == 3);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "csharp"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}