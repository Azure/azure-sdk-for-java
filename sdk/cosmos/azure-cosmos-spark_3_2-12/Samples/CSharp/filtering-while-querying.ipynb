{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "string cosmosEndpoint = \"<cosmos-endpoint>\";\n",
        "string cosmosMasterKey = \"<cosmos-master-key>\";\n",
        "string cosmosDatabaseName = \"ContosoHospital\";\n",
        "string cosmosContainerName =  \"Patient\";\n",
        "string checkpointLocation = \"/tmp/streaming\";\n",
        "string targetContainerName = \"CopyContainer\";\n",
        "\n",
        "// Patient Document -- partition key: patientId\n",
        "// {\n",
        "//   \"id\": \"9c9a1156-e936-40f3-a442-e9528b55a2fb\",\n",
        "//   \"patientId\": \"423ab2cf-dd1c-4404-8524-86cee045c179\",\n",
        "//   \"patientName\": \"John Doe\",\n",
        "//   \"doctorId\" : \"629f49da-9cfc-45a4-8e1c-d4f8b7ab1f4e\",\n",
        "//   \"doctorName\": \"Sung Ondricka\"\n",
        "// }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "//-----filtering examples-with-schema-inference-disabled---------------------\n",
        "\n",
        "var changeFeedCfg = new Dictionary<string, string>(){\n",
        "  {\"spark.synapse.linkedService\", \"CosmosDBLSContosoHospital\"},\n",
        "  {\"spark.cosmos.container\" , cosmosContainerName},\n",
        "  {\"spark.cosmos.read.inferSchema.enabled\" , \"false\"},   \n",
        "  {\"spark.cosmos.changeFeed.startFrom\" , \"Beginning\"},\n",
        "  {\"spark.cosmos.changeFeed.mode\" , \"Incremental\"},\n",
        "  {\"spark.cosmos.changeFeed.itemCountPerTriggerHint\" , \"100000\"}\n",
        "  //optional configuration for throughput control\n",
        "  // {\"spark.cosmos.throughputControl.enabled\", \"true\"},\n",
        "  // {\"spark.cosmos.throughputControl.name\", \"SourceContainerThroughputControl\"},\n",
        "  // {\"spark.cosmos.throughputControl.targetThroughputThreshold\", \"0.30\"}, \n",
        "  // {\"spark.cosmos.throughputControl.globalControl.database\", \"database-v4\"}, \n",
        "  // {\"spark.cosmos.throughputControl.globalControl.container\", \"ThroughputControl}\"\n",
        "};\n",
        "\n",
        "var writeCfg = new Dictionary<string, string>(){\n",
        "  {\"spark.synapse.linkedService\", \"CosmosDBLSContosoHospital\"},\n",
        "  {\"spark.cosmos.container\" , targetContainerName},\n",
        "  {\"checkpointLocation\" , checkpointLocation}\n",
        "};"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "//optional configuration for creating throughput control metadata container\n",
        "\n",
        "// spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\");\n",
        "// spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint);\n",
        "// spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- %%sql\n",
        "\n",
        "-- CREATE DATABASE IF NOT EXISTS cosmosCatalog.database-v4;\n",
        "\n",
        "-- CREATE TABLE IF NOT EXISTS cosmosCatalog.database-v4.ThroughputControl\n",
        "-- USING cosmos.oltp\n",
        "-- TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// ----- EXAMPLE 1 -----\n",
        "\n",
        "var changeFeedDF = spark\n",
        "    .ReadStream()\n",
        "    .Format(\"cosmos.oltp.changefeed\")\n",
        "    .Options(changeFeedCfg)\n",
        "    .Load();\n",
        "\n",
        "// Here is an example of splitting the id column by \"-\" and creating a new column with only the second item\n",
        "// e.g. Id = XXXXX-SubIdWeAreInterestedIn-XXXXX\n",
        "DataFrame splitByDash = changeFeedDF.WithColumn(\"idSplit\",Functions.Split(Column(\"id\"),\"-\"));\n",
        "DataFrame withSplitId = splitByDash.WithColumn(\"newId\", Column(\"idSplit\").GetItem(1));\n",
        "\n",
        "// Filter by newId to get only the columns we want to migrate, then drop helper columns\n",
        "DataFrame filteredByNewId = withSplitId.Filter(Column(\"newId\") == 7926).Drop(\"idSplit\").Drop(\"newId\");\n",
        "\n",
        "// preserve system properties like _ts, _etag by renaming the original column\n",
        "DataFrame df_withAuditFields = filteredByNewId.WithColumnRenamed(\"_rawbody\", \"_origin_rawBody\");\n",
        "\n",
        "// Write a streaming Spark DataFrame to a Cosmos DB container\n",
        "df_withAuditFields\n",
        "    .WriteStream()\n",
        "    .Format(\"cosmos.oltp\")\n",
        "    .Options(writeCfg)\n",
        "    .OutputMode(\"append\")\n",
        "    .Start()\n",
        "    .AwaitTermination();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// ----- EXAMPLE 2 -----\n",
        "// The following example uses filter and string matching to parse out rows where doctorId values match a given value, without the need for the spark.read.json feature or any joins\n",
        "\n",
        "var changeFeedDF = spark\n",
        "    .ReadStream()\n",
        "    .Format(\"cosmos.oltp.changefeed\")\n",
        "    .Options(changeFeedCfg)\n",
        "    .Load();\n",
        "\n",
        "// Filter by an example doctorId we are interested in from raw document into a new df\n",
        "DataFrame filteredDf = changeFeedDF.Filter(Column(\"_rawBody\").Contains(\"\\\"doctorId\\\":\\\"5b15f027-74d1-4ab8-9ad3-cca848837f66\\\"\"));\n",
        "\n",
        "// preserve system properties like _ts, _etag by renaming the original column\n",
        "DataFrame df_withAuditFields = filteredDf.WithColumnRenamed(\"_rawbody\", \"_origin_rawBody\");\n",
        "\n",
        "// write streaming dataframe to the target container\n",
        "df_withAuditFields\n",
        "    .WriteStream()\n",
        "    .Format(\"cosmos.oltp\")\n",
        "    .Options(writeCfg)\n",
        "    .OutputMode(\"append\")\n",
        "    .Start()\n",
        "    .AwaitTermination();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// ----- EXAMPLE 3 ----- \n",
        "//In this example we will write data into a container with different partition key from source container\n",
        "\n",
        "string targetContainerName = \"CopyWithDoctorId\";\n",
        "string checkpointLocation = \"/tmp/pk_doctorId_checkpoint\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Configure Catalog Api to be used\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\");\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint);\n",
        "spark.Conf().Set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "// create an Azure Cosmos DB container using catalog api\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.ContosoHospital.CopyWithDoctorId\n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(partitionKeyPath = '/doctorId', autoScaleMaxThroughput = '1000');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "var writeCfgWithNewPK = new Dictionary<string, string>(){\n",
        " {\"spark.synapse.linkedService\", \"CosmosDBLSContosoHospital\"},\n",
        "  {\"spark.cosmos.container\" , targetContainerName},\n",
        "  {\"checkpointLocation\" , checkpointLocation}\n",
        "};\n",
        "\n",
        "// read streaming data from changeFeed\n",
        "DataFrame changeFeedDF = spark.\n",
        "        ReadStream().\n",
        "        Format(\"cosmos.oltp.changeFeed\")\n",
        "        .Options(changeFeedCfg)\n",
        "        .Load();\n",
        "\n",
        "// write streaming data into new container with different PK\n",
        "df_withAuditFields\n",
        "    .WriteStream()\n",
        "    .Format(\"cosmos.oltp\")\n",
        "    .Options(writeCfgWithNewPK)\n",
        "    .OutputMode(\"append\")\n",
        "    .Start()\n",
        "    .AwaitTermination();"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "csharp"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
