// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.openai.responses.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.annotation.Generated;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.List;
import java.util.Map;

/**
 * The CreateResponsesRequest model.
 */
@Fluent
public final class CreateResponsesRequest implements JsonSerializable<CreateResponsesRequest> {

    /*
     * Model ID used to generate the response, like `gpt-4o` or `o1`.
     * Refer to the [model guide](/docs/models) for more information and supported features for each model.
     */
    @Generated
    private final CreateResponsesRequestModel model;

    /*
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
     * lower values like 0.2 will make it more focused and deterministic.
     * 
     * We generally recommend altering this or `top_p` but not both.
     */
    @Generated
    private Double temperature;

    /*
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of
     * the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are
     * considered.
     * 
     * We generally recommend altering this or `temperature` but not both.
     */
    @Generated
    private Double topP;

    /*
     * **o1 models only**
     * 
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     */
    @Generated
    private CreateResponsesRequestReasoningEffort reasoningEffort;

    /*
     * If set to true, the model response data will be streamed to the client as it is generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#
     * Event_stream_format).
     * 
     * See the "Streaming" section below for more information.
     */
    @Generated
    private Boolean stream;

    /*
     * Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     */
    @Generated
    private Map<String, String> metadata;

    /*
     * Text, image, or audio inputs to the model, used to generate a response.
     * Can also contain previous assistant responses and tool call outputs.
     * 
     * Learn more about prompting a model with the [Responses API](/docs/guides/responses).
     */
    @Generated
    private final List<ResponsesItem> input;

    /*
     * The unique ID of the previous response to the model. Use this to create multi-turn conversations.
     */
    @Generated
    private String previousResponseId;

    /*
     * The tools to use to generate a response.
     */
    @Generated
    private List<ResponsesTool> tools;

    /*
     * Specify additional output data to include in the model response.
     * Currently supported values are:
     * - `output[*].file_search_call.search_results`: Include the search results of the file search tool call.
     * - `output[*].web_search_call.search_results`: Include the search results of the web search tool call.
     */
    @Generated
    private List<CreateResponsesRequestIncludable> include;

    /*
     * The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     */
    @Generated
    private CreateResponsesRequestTruncation truncation;

    /*
     * Whether to allow the model to run tool calls in parallel.
     */
    @Generated
    private Boolean parallelToolCalls;

    /**
     * Creates an instance of CreateResponsesRequest class.
     *
     * @param model the model value to set.
     * @param input the input value to set.
     */
    @Generated
    public CreateResponsesRequest(CreateResponsesRequestModel model, List<ResponsesItem> input) {
        this.model = model;
        this.input = input;
    }

    public CreateResponsesRequest(String model, List<ResponsesItem> input) {
        this.model = CreateResponsesRequestModel.fromString(model);
        this.input = input;
    }

    /**
     * Get the model property: Model ID used to generate the response, like `gpt-4o` or `o1`.
     * Refer to the [model guide](/docs/models) for more information and supported features for each model.
     *
     * @return the model value.
     */
    @Generated
    public CreateResponsesRequestModel getModel() {
        return this.model;
    }

    /**
     * Get the temperature property: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
     * the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @return the temperature value.
     */
    @Generated
    public Double getTemperature() {
        return this.temperature;
    }

    /**
     * Set the temperature property: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
     * the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @param temperature the temperature value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTemperature(Double temperature) {
        this.temperature = temperature;
        return this;
    }

    /**
     * Get the topP property: An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top
     * 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @return the topP value.
     */
    @Generated
    public Double getTopP() {
        return this.topP;
    }

    /**
     * Set the topP property: An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top
     * 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @param topP the topP value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTopP(Double topP) {
        this.topP = topP;
        return this;
    }

    /**
     * Get the reasoningEffort property: **o1 models only**
     *
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     *
     * @return the reasoningEffort value.
     */
    @Generated
    public CreateResponsesRequestReasoningEffort getReasoningEffort() {
        return this.reasoningEffort;
    }

    /**
     * Set the reasoningEffort property: **o1 models only**
     *
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     *
     * @param reasoningEffort the reasoningEffort value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setReasoningEffort(CreateResponsesRequestReasoningEffort reasoningEffort) {
        this.reasoningEffort = reasoningEffort;
        return this;
    }

    /**
     * Get the stream property: If set to true, the model response data will be streamed to the client as it is
     * generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
     *
     * See the "Streaming" section below for more information.
     *
     * @return the stream value.
     */
    @Generated
    public Boolean isStream() {
        return this.stream;
    }

    /**
     * Set the stream property: If set to true, the model response data will be streamed to the client as it is
     * generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
     *
     * See the "Streaming" section below for more information.
     *
     * @param stream the stream value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setStream(Boolean stream) {
        this.stream = stream;
        return this;
    }

    /**
     * Get the metadata property: Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     *
     * @return the metadata value.
     */
    @Generated
    public Map<String, String> getMetadata() {
        return this.metadata;
    }

    /**
     * Set the metadata property: Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     *
     * @param metadata the metadata value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setMetadata(Map<String, String> metadata) {
        this.metadata = metadata;
        return this;
    }

    /**
     * Get the input property: Text, image, or audio inputs to the model, used to generate a response.
     * Can also contain previous assistant responses and tool call outputs.
     *
     * Learn more about prompting a model with the [Responses API](/docs/guides/responses).
     *
     * @return the input value.
     */
    @Generated
    public List<ResponsesItem> getInput() {
        return this.input;
    }

    /**
     * Get the previousResponseId property: The unique ID of the previous response to the model. Use this to create
     * multi-turn conversations.
     *
     * @return the previousResponseId value.
     */
    @Generated
    public String getPreviousResponseId() {
        return this.previousResponseId;
    }

    /**
     * Set the previousResponseId property: The unique ID of the previous response to the model. Use this to create
     * multi-turn conversations.
     *
     * @param previousResponseId the previousResponseId value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setPreviousResponseId(String previousResponseId) {
        this.previousResponseId = previousResponseId;
        return this;
    }

    /**
     * Get the tools property: The tools to use to generate a response.
     *
     * @return the tools value.
     */
    @Generated
    public List<ResponsesTool> getTools() {
        return this.tools;
    }

    /**
     * Set the tools property: The tools to use to generate a response.
     *
     * @param tools the tools value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTools(List<ResponsesTool> tools) {
        this.tools = tools;
        return this;
    }

    /**
     * Get the include property: Specify additional output data to include in the model response.
     * Currently supported values are:
     * - `output[*].file_search_call.search_results`: Include the search results of the file search tool call.
     * - `output[*].web_search_call.search_results`: Include the search results of the web search tool call.
     *
     * @return the include value.
     */
    @Generated
    public List<CreateResponsesRequestIncludable> getInclude() {
        return this.include;
    }

    /**
     * Set the include property: Specify additional output data to include in the model response.
     * Currently supported values are:
     * - `output[*].file_search_call.search_results`: Include the search results of the file search tool call.
     * - `output[*].web_search_call.search_results`: Include the search results of the web search tool call.
     *
     * @param include the include value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setInclude(List<CreateResponsesRequestIncludable> include) {
        this.include = include;
        return this;
    }

    /**
     * Get the truncation property: The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     *
     * @return the truncation value.
     */
    @Generated
    public CreateResponsesRequestTruncation getTruncation() {
        return this.truncation;
    }

    /**
     * Set the truncation property: The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     *
     * @param truncation the truncation value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTruncation(CreateResponsesRequestTruncation truncation) {
        this.truncation = truncation;
        return this;
    }

    /**
     * Get the parallelToolCalls property: Whether to allow the model to run tool calls in parallel.
     *
     * @return the parallelToolCalls value.
     */
    @Generated
    public Boolean isParallelToolCalls() {
        return this.parallelToolCalls;
    }

    /**
     * Set the parallelToolCalls property: Whether to allow the model to run tool calls in parallel.
     *
     * @param parallelToolCalls the parallelToolCalls value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setParallelToolCalls(Boolean parallelToolCalls) {
        this.parallelToolCalls = parallelToolCalls;
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("model", this.model == null ? null : this.model.toString());
        jsonWriter.writeArrayField("input", this.input, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeNumberField("temperature", this.temperature);
        jsonWriter.writeNumberField("top_p", this.topP);
        jsonWriter.writeStringField("reasoning_effort",
            this.reasoningEffort == null ? null : this.reasoningEffort.toString());
        jsonWriter.writeBooleanField("stream", this.stream);
        jsonWriter.writeMapField("metadata", this.metadata, (writer, element) -> writer.writeString(element));
        jsonWriter.writeStringField("previous_response_id", this.previousResponseId);
        jsonWriter.writeArrayField("tools", this.tools, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeArrayField("include", this.include,
            (writer, element) -> writer.writeString(element == null ? null : element.toString()));
        jsonWriter.writeStringField("truncation", this.truncation == null ? null : this.truncation.toString());
        jsonWriter.writeBooleanField("parallel_tool_calls", this.parallelToolCalls);
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of CreateResponsesRequest from the JsonReader.
     *
     * @param jsonReader The JsonReader being read.
     * @return An instance of CreateResponsesRequest if the JsonReader was pointing to an instance of it, or null if it
     * was pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the CreateResponsesRequest.
     */
    @Generated
    public static CreateResponsesRequest fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            CreateResponsesRequestModel model = null;
            List<ResponsesItem> input = null;
            Double temperature = null;
            Double topP = null;
            CreateResponsesRequestReasoningEffort reasoningEffort = null;
            Boolean stream = null;
            Map<String, String> metadata = null;
            String previousResponseId = null;
            List<ResponsesTool> tools = null;
            List<CreateResponsesRequestIncludable> include = null;
            CreateResponsesRequestTruncation truncation = null;
            Boolean parallelToolCalls = null;
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();
                if ("model".equals(fieldName)) {
                    model = CreateResponsesRequestModel.fromString(reader.getString());
                } else if ("input".equals(fieldName)) {
                    input = reader.readArray(reader1 -> ResponsesItem.fromJson(reader1));
                } else if ("temperature".equals(fieldName)) {
                    temperature = reader.getNullable(JsonReader::getDouble);
                } else if ("top_p".equals(fieldName)) {
                    topP = reader.getNullable(JsonReader::getDouble);
                } else if ("reasoning_effort".equals(fieldName)) {
                    reasoningEffort = CreateResponsesRequestReasoningEffort.fromString(reader.getString());
                } else if ("stream".equals(fieldName)) {
                    stream = reader.getNullable(JsonReader::getBoolean);
                } else if ("metadata".equals(fieldName)) {
                    metadata = reader.readMap(reader1 -> reader1.getString());
                } else if ("previous_response_id".equals(fieldName)) {
                    previousResponseId = reader.getString();
                } else if ("tools".equals(fieldName)) {
                    tools = reader.readArray(reader1 -> ResponsesTool.fromJson(reader1));
                } else if ("include".equals(fieldName)) {
                    include
                        = reader.readArray(reader1 -> CreateResponsesRequestIncludable.fromString(reader1.getString()));
                } else if ("truncation".equals(fieldName)) {
                    truncation = CreateResponsesRequestTruncation.fromString(reader.getString());
                } else if ("parallel_tool_calls".equals(fieldName)) {
                    parallelToolCalls = reader.getNullable(JsonReader::getBoolean);
                } else {
                    reader.skipChildren();
                }
            }
            CreateResponsesRequest deserializedCreateResponsesRequest = new CreateResponsesRequest(model, input);
            deserializedCreateResponsesRequest.temperature = temperature;
            deserializedCreateResponsesRequest.topP = topP;
            deserializedCreateResponsesRequest.reasoningEffort = reasoningEffort;
            deserializedCreateResponsesRequest.stream = stream;
            deserializedCreateResponsesRequest.metadata = metadata;
            deserializedCreateResponsesRequest.previousResponseId = previousResponseId;
            deserializedCreateResponsesRequest.tools = tools;
            deserializedCreateResponsesRequest.include = include;
            deserializedCreateResponsesRequest.truncation = truncation;
            deserializedCreateResponsesRequest.parallelToolCalls = parallelToolCalls;
            return deserializedCreateResponsesRequest;
        });
    }
}
