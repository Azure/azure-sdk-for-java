// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.openai.responses.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.annotation.Generated;
import com.azure.core.util.BinaryData;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.util.List;
import java.util.Map;

/**
 * The CreateResponsesRequest model.
 */
@Fluent
public final class CreateResponsesRequest implements JsonSerializable<CreateResponsesRequest> {

    /*
     * Model ID used to generate the response, like `gpt-4o` or `o1`.
     * Refer to the [model guide](/docs/models) for more information and supported features for each model.
     */
    @Generated
    private final CreateResponsesRequestModel model;

    /*
     * Text, image, or audio inputs to the model, used to generate a response.
     * Can also contain previous assistant responses and tool call outputs.
     * 
     * Learn more about prompting a model with the [Responses API](/docs/guides/responses).
     */
    @Generated
    private final List<ResponsesItem> input;

    /*
     * The unique ID of the previous response to the model. Use this to create multi-turn conversations.
     */
    @Generated
    private String previousResponseId;

    /*
     * Specifies additional output data to include in the model response.
     */
    @Generated
    private List<CreateResponsesRequestIncludable> include;

    /*
     * The tools to use to generate a response.
     */
    @Generated
    private List<ResponsesTool> tools;

    /*
     * The instructions property.
     */
    @Generated
    private String instructions;

    /*
     * **o1 models only**
     * 
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     */
    @Generated
    private ReasoningEffort reasoningEffort;

    /*
     * Output types that you would like the model to generate.
     * Most models are capable of generating text, which is the default:
     * `["text"]`
     * 
     * The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio).
     * To request that this model generate both text and audio responses, you can use:
     * `["text", "audio"]`
     */
    @Generated
    private List<CreateResponsesRequestModality> modalities;

    /*
     * The text property.
     */
    @Generated
    private CreateResponsesRequestText text;

    /*
     * Parameters for audio output. Required when audio output is requested with
     * `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
     */
    @Generated
    private CreateResponsesRequestAudio1 audio;

    /*
     * How the model should select which tool (or tools) to use when generating a response.
     */
    @Generated
    private BinaryData toolChoice;

    /*
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
     * lower values like 0.2 will make it more focused and deterministic.
     * 
     * We generally recommend altering this or `top_p` but not both.
     */
    @Generated
    private Double temperature;

    /*
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of
     * the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are
     * considered.
     * 
     * We generally recommend altering this or `temperature` but not both.
     */
    @Generated
    private Double topP;

    /*
     * An integer between 0 and 20 specifying the number of most likely tokens to
     * return at each token position, each with an associated log probability.
     */
    @Generated
    private Integer topLogprobs;

    /*
     * A floating point number between -2.0 and 2.0, inclusive, default 0. Higher values penalize new tokens based on
     * whether they appear in the generated text so far, decreasing the model's likelihood to repeat the same text
     * verbatim.
     */
    @Generated
    private Double presencePenalty;

    /*
     * A floating point number between -2.0 and 2.0, inclusive, default 0. Higher values penalize new tokens based on
     * their existing frequency in the generated text so far, decreasing the model's likelihood to repeat the same text
     * verbatim.
     */
    @Generated
    private Double frequencyPenalty;

    /*
     * An upper bound for the number of tokens that can be generated for a response, including visible output tokens and
     * [reasoning tokens](/docs/guides/reasoning).
     */
    @Generated
    private Integer maxCompletionTokens;

    /*
     * The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     */
    @Generated
    private CreateResponsesRequestTruncation truncation;

    /*
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn
     * more](/docs/guides/safety-best-practices#end-user-ids).
     */
    @Generated
    private String user;

    /*
     * Specifies the latency tier to use for processing the request.
     * This parameter is relevant for customers subscribed to the scale tier service:
     * - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they
     * are exhausted.
     * - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default
     * service tier with a lower uptime SLA and no latency guarentee.
     * - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and
     * no latency guarentee.
     * - When not set, the default behavior is 'auto'.
     * 
     * When this parameter is set, the response body will include the `service_tier` utilized.
     */
    @Generated
    private CreateResponsesRequestServiceTier serviceTier;

    /*
     * Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     */
    @Generated
    private Map<String, String> metadata;

    /*
     * Specifies whether parallel tool calling should be enabled for this response.
     */
    @Generated
    private Boolean parallelToolCalls;

    /*
     * If set to true, the model response data will be streamed to the client as it is generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#
     * Event_stream_format).
     * 
     * See the "Streaming" section below for more information.
     */
    @Generated
    private Boolean stream;

    /*
     * Whether or not to store the output of this response for
     * use in our [model distillation](/docs/guides/distillation) or
     * [evals](/docs/guides/evals) products.
     */
    @Generated
    private Boolean store;

    /**
     * Creates an instance of CreateResponsesRequest class.
     *
     * @param model the model value to set.
     * @param input the input value to set.
     */
    @Generated
    public CreateResponsesRequest(CreateResponsesRequestModel model, List<ResponsesItem> input) {
        this.model = model;
        this.input = input;
    }

    /**
     * Get the model property: Model ID used to generate the response, like `gpt-4o` or `o1`.
     * Refer to the [model guide](/docs/models) for more information and supported features for each model.
     *
     * @return the model value.
     */
    @Generated
    public CreateResponsesRequestModel getModel() {
        return this.model;
    }

    /**
     * Get the input property: Text, image, or audio inputs to the model, used to generate a response.
     * Can also contain previous assistant responses and tool call outputs.
     *
     * Learn more about prompting a model with the [Responses API](/docs/guides/responses).
     *
     * @return the input value.
     */
    @Generated
    public List<ResponsesItem> getInput() {
        return this.input;
    }

    /**
     * Get the previousResponseId property: The unique ID of the previous response to the model. Use this to create
     * multi-turn conversations.
     *
     * @return the previousResponseId value.
     */
    @Generated
    public String getPreviousResponseId() {
        return this.previousResponseId;
    }

    /**
     * Set the previousResponseId property: The unique ID of the previous response to the model. Use this to create
     * multi-turn conversations.
     *
     * @param previousResponseId the previousResponseId value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setPreviousResponseId(String previousResponseId) {
        this.previousResponseId = previousResponseId;
        return this;
    }

    /**
     * Get the include property: Specifies additional output data to include in the model response.
     *
     * @return the include value.
     */
    @Generated
    public List<CreateResponsesRequestIncludable> getInclude() {
        return this.include;
    }

    /**
     * Set the include property: Specifies additional output data to include in the model response.
     *
     * @param include the include value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setInclude(List<CreateResponsesRequestIncludable> include) {
        this.include = include;
        return this;
    }

    /**
     * Get the tools property: The tools to use to generate a response.
     *
     * @return the tools value.
     */
    @Generated
    public List<ResponsesTool> getTools() {
        return this.tools;
    }

    /**
     * Set the tools property: The tools to use to generate a response.
     *
     * @param tools the tools value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTools(List<ResponsesTool> tools) {
        this.tools = tools;
        return this;
    }

    /**
     * Get the instructions property: The instructions property.
     *
     * @return the instructions value.
     */
    @Generated
    public String getInstructions() {
        return this.instructions;
    }

    /**
     * Set the instructions property: The instructions property.
     *
     * @param instructions the instructions value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setInstructions(String instructions) {
        this.instructions = instructions;
        return this;
    }

    /**
     * Get the reasoningEffort property: **o1 models only**
     *
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     *
     * @return the reasoningEffort value.
     */
    @Generated
    public ReasoningEffort getReasoningEffort() {
        return this.reasoningEffort;
    }

    /**
     * Set the reasoningEffort property: **o1 models only**
     *
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     *
     * @param reasoningEffort the reasoningEffort value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setReasoningEffort(ReasoningEffort reasoningEffort) {
        this.reasoningEffort = reasoningEffort;
        return this;
    }

    /**
     * Get the modalities property: Output types that you would like the model to generate.
     * Most models are capable of generating text, which is the default:
     * `["text"]`
     *
     * The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio).
     * To request that this model generate both text and audio responses, you can use:
     * `["text", "audio"]`.
     *
     * @return the modalities value.
     */
    @Generated
    public List<CreateResponsesRequestModality> getModalities() {
        return this.modalities;
    }

    /**
     * Set the modalities property: Output types that you would like the model to generate.
     * Most models are capable of generating text, which is the default:
     * `["text"]`
     *
     * The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio).
     * To request that this model generate both text and audio responses, you can use:
     * `["text", "audio"]`.
     *
     * @param modalities the modalities value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setModalities(List<CreateResponsesRequestModality> modalities) {
        this.modalities = modalities;
        return this;
    }

    /**
     * Get the text property: The text property.
     *
     * @return the text value.
     */
    @Generated
    public CreateResponsesRequestText getText() {
        return this.text;
    }

    /**
     * Set the text property: The text property.
     *
     * @param text the text value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setText(CreateResponsesRequestText text) {
        this.text = text;
        return this;
    }

    /**
     * Get the audio property: Parameters for audio output. Required when audio output is requested with
     * `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
     *
     * @return the audio value.
     */
    @Generated
    public CreateResponsesRequestAudio1 getAudio() {
        return this.audio;
    }

    /**
     * Set the audio property: Parameters for audio output. Required when audio output is requested with
     * `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
     *
     * @param audio the audio value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setAudio(CreateResponsesRequestAudio1 audio) {
        this.audio = audio;
        return this;
    }

    /**
     * Get the toolChoice property: How the model should select which tool (or tools) to use when generating a response.
     *
     * @return the toolChoice value.
     */
    @Generated
    public BinaryData getToolChoice() {
        return this.toolChoice;
    }

    /**
     * Set the toolChoice property: How the model should select which tool (or tools) to use when generating a response.
     *
     * @param toolChoice the toolChoice value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setToolChoice(BinaryData toolChoice) {
        this.toolChoice = toolChoice;
        return this;
    }

    /**
     * Get the temperature property: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
     * the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @return the temperature value.
     */
    @Generated
    public Double getTemperature() {
        return this.temperature;
    }

    /**
     * Set the temperature property: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
     * the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @param temperature the temperature value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTemperature(Double temperature) {
        this.temperature = temperature;
        return this;
    }

    /**
     * Get the topP property: An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top
     * 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @return the topP value.
     */
    @Generated
    public Double getTopP() {
        return this.topP;
    }

    /**
     * Set the topP property: An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top
     * 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @param topP the topP value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTopP(Double topP) {
        this.topP = topP;
        return this;
    }

    /**
     * Get the topLogprobs property: An integer between 0 and 20 specifying the number of most likely tokens to
     * return at each token position, each with an associated log probability.
     *
     * @return the topLogprobs value.
     */
    @Generated
    public Integer getTopLogprobs() {
        return this.topLogprobs;
    }

    /**
     * Set the topLogprobs property: An integer between 0 and 20 specifying the number of most likely tokens to
     * return at each token position, each with an associated log probability.
     *
     * @param topLogprobs the topLogprobs value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTopLogprobs(Integer topLogprobs) {
        this.topLogprobs = topLogprobs;
        return this;
    }

    /**
     * Get the presencePenalty property: A floating point number between -2.0 and 2.0, inclusive, default 0. Higher
     * values penalize new tokens based on whether they appear in the generated text so far, decreasing the model's
     * likelihood to repeat the same text verbatim.
     *
     * @return the presencePenalty value.
     */
    @Generated
    public Double getPresencePenalty() {
        return this.presencePenalty;
    }

    /**
     * Set the presencePenalty property: A floating point number between -2.0 and 2.0, inclusive, default 0. Higher
     * values penalize new tokens based on whether they appear in the generated text so far, decreasing the model's
     * likelihood to repeat the same text verbatim.
     *
     * @param presencePenalty the presencePenalty value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setPresencePenalty(Double presencePenalty) {
        this.presencePenalty = presencePenalty;
        return this;
    }

    /**
     * Get the frequencyPenalty property: A floating point number between -2.0 and 2.0, inclusive, default 0. Higher
     * values penalize new tokens based on their existing frequency in the generated text so far, decreasing the model's
     * likelihood to repeat the same text verbatim.
     *
     * @return the frequencyPenalty value.
     */
    @Generated
    public Double getFrequencyPenalty() {
        return this.frequencyPenalty;
    }

    /**
     * Set the frequencyPenalty property: A floating point number between -2.0 and 2.0, inclusive, default 0. Higher
     * values penalize new tokens based on their existing frequency in the generated text so far, decreasing the model's
     * likelihood to repeat the same text verbatim.
     *
     * @param frequencyPenalty the frequencyPenalty value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setFrequencyPenalty(Double frequencyPenalty) {
        this.frequencyPenalty = frequencyPenalty;
        return this;
    }

    /**
     * Get the maxCompletionTokens property: An upper bound for the number of tokens that can be generated for a
     * response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
     *
     * @return the maxCompletionTokens value.
     */
    @Generated
    public Integer getMaxCompletionTokens() {
        return this.maxCompletionTokens;
    }

    /**
     * Set the maxCompletionTokens property: An upper bound for the number of tokens that can be generated for a
     * response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
     *
     * @param maxCompletionTokens the maxCompletionTokens value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setMaxCompletionTokens(Integer maxCompletionTokens) {
        this.maxCompletionTokens = maxCompletionTokens;
        return this;
    }

    /**
     * Get the truncation property: The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     *
     * @return the truncation value.
     */
    @Generated
    public CreateResponsesRequestTruncation getTruncation() {
        return this.truncation;
    }

    /**
     * Set the truncation property: The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     *
     * @param truncation the truncation value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setTruncation(CreateResponsesRequestTruncation truncation) {
        this.truncation = truncation;
        return this;
    }

    /**
     * Get the user property: A unique identifier representing your end-user, which can help OpenAI to monitor and
     * detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
     *
     * @return the user value.
     */
    @Generated
    public String getUser() {
        return this.user;
    }

    /**
     * Set the user property: A unique identifier representing your end-user, which can help OpenAI to monitor and
     * detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
     *
     * @param user the user value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setUser(String user) {
        this.user = user;
        return this;
    }

    /**
     * Get the serviceTier property: Specifies the latency tier to use for processing the request.
     * This parameter is relevant for customers subscribed to the scale tier service:
     * - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they
     * are exhausted.
     * - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default
     * service tier with a lower uptime SLA and no latency guarentee.
     * - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and
     * no latency guarentee.
     * - When not set, the default behavior is 'auto'.
     *
     * When this parameter is set, the response body will include the `service_tier` utilized.
     *
     * @return the serviceTier value.
     */
    @Generated
    public CreateResponsesRequestServiceTier getServiceTier() {
        return this.serviceTier;
    }

    /**
     * Set the serviceTier property: Specifies the latency tier to use for processing the request.
     * This parameter is relevant for customers subscribed to the scale tier service:
     * - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they
     * are exhausted.
     * - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default
     * service tier with a lower uptime SLA and no latency guarentee.
     * - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and
     * no latency guarentee.
     * - When not set, the default behavior is 'auto'.
     *
     * When this parameter is set, the response body will include the `service_tier` utilized.
     *
     * @param serviceTier the serviceTier value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setServiceTier(CreateResponsesRequestServiceTier serviceTier) {
        this.serviceTier = serviceTier;
        return this;
    }

    /**
     * Get the metadata property: Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     *
     * @return the metadata value.
     */
    @Generated
    public Map<String, String> getMetadata() {
        return this.metadata;
    }

    /**
     * Set the metadata property: Developer-defined tags and values used for filtering model responses in the
     * [dashboard](https://platform.openai.com/).
     *
     * @param metadata the metadata value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setMetadata(Map<String, String> metadata) {
        this.metadata = metadata;
        return this;
    }

    /**
     * Get the parallelToolCalls property: Specifies whether parallel tool calling should be enabled for this response.
     *
     * @return the parallelToolCalls value.
     */
    @Generated
    public Boolean isParallelToolCalls() {
        return this.parallelToolCalls;
    }

    /**
     * Set the parallelToolCalls property: Specifies whether parallel tool calling should be enabled for this response.
     *
     * @param parallelToolCalls the parallelToolCalls value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setParallelToolCalls(Boolean parallelToolCalls) {
        this.parallelToolCalls = parallelToolCalls;
        return this;
    }

    /**
     * Get the stream property: If set to true, the model response data will be streamed to the client as it is
     * generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
     *
     * See the "Streaming" section below for more information.
     *
     * @return the stream value.
     */
    @Generated
    public Boolean isStream() {
        return this.stream;
    }

    /**
     * Set the stream property: If set to true, the model response data will be streamed to the client as it is
     * generated using [server-sent
     * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
     *
     * See the "Streaming" section below for more information.
     *
     * @param stream the stream value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setStream(Boolean stream) {
        this.stream = stream;
        return this;
    }

    /**
     * Get the store property: Whether or not to store the output of this response for
     * use in our [model distillation](/docs/guides/distillation) or
     * [evals](/docs/guides/evals) products.
     *
     * @return the store value.
     */
    @Generated
    public Boolean isStore() {
        return this.store;
    }

    /**
     * Set the store property: Whether or not to store the output of this response for
     * use in our [model distillation](/docs/guides/distillation) or
     * [evals](/docs/guides/evals) products.
     *
     * @param store the store value to set.
     * @return the CreateResponsesRequest object itself.
     */
    @Generated
    public CreateResponsesRequest setStore(Boolean store) {
        this.store = store;
        return this;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("model", this.model == null ? null : this.model.toString());
        jsonWriter.writeArrayField("input", this.input, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeStringField("previous_response_id", this.previousResponseId);
        jsonWriter.writeArrayField("include", this.include,
            (writer, element) -> writer.writeString(element == null ? null : element.toString()));
        jsonWriter.writeArrayField("tools", this.tools, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeStringField("instructions", this.instructions);
        jsonWriter.writeStringField("reasoning_effort",
            this.reasoningEffort == null ? null : this.reasoningEffort.toString());
        jsonWriter.writeArrayField("modalities", this.modalities,
            (writer, element) -> writer.writeString(element == null ? null : element.toString()));
        jsonWriter.writeJsonField("text", this.text);
        jsonWriter.writeJsonField("audio", this.audio);
        if (this.toolChoice != null) {
            jsonWriter.writeFieldName("tool_choice");
            this.toolChoice.writeTo(jsonWriter);
        }
        jsonWriter.writeNumberField("temperature", this.temperature);
        jsonWriter.writeNumberField("top_p", this.topP);
        jsonWriter.writeNumberField("top_logprobs", this.topLogprobs);
        jsonWriter.writeNumberField("presence_penalty", this.presencePenalty);
        jsonWriter.writeNumberField("frequency_penalty", this.frequencyPenalty);
        jsonWriter.writeNumberField("max_completion_tokens", this.maxCompletionTokens);
        jsonWriter.writeStringField("truncation", this.truncation == null ? null : this.truncation.toString());
        jsonWriter.writeStringField("user", this.user);
        jsonWriter.writeStringField("service_tier", this.serviceTier == null ? null : this.serviceTier.toString());
        jsonWriter.writeMapField("metadata", this.metadata, (writer, element) -> writer.writeString(element));
        jsonWriter.writeBooleanField("parallel_tool_calls", this.parallelToolCalls);
        jsonWriter.writeBooleanField("stream", this.stream);
        jsonWriter.writeBooleanField("store", this.store);
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of CreateResponsesRequest from the JsonReader.
     *
     * @param jsonReader The JsonReader being read.
     * @return An instance of CreateResponsesRequest if the JsonReader was pointing to an instance of it, or null if it
     * was pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the CreateResponsesRequest.
     */
    @Generated
    public static CreateResponsesRequest fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            CreateResponsesRequestModel model = null;
            List<ResponsesItem> input = null;
            String previousResponseId = null;
            List<CreateResponsesRequestIncludable> include = null;
            List<ResponsesTool> tools = null;
            String instructions = null;
            ReasoningEffort reasoningEffort = null;
            List<CreateResponsesRequestModality> modalities = null;
            CreateResponsesRequestText text = null;
            CreateResponsesRequestAudio1 audio = null;
            BinaryData toolChoice = null;
            Double temperature = null;
            Double topP = null;
            Integer topLogprobs = null;
            Double presencePenalty = null;
            Double frequencyPenalty = null;
            Integer maxCompletionTokens = null;
            CreateResponsesRequestTruncation truncation = null;
            String user = null;
            CreateResponsesRequestServiceTier serviceTier = null;
            Map<String, String> metadata = null;
            Boolean parallelToolCalls = null;
            Boolean stream = null;
            Boolean store = null;
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();
                if ("model".equals(fieldName)) {
                    model = CreateResponsesRequestModel.fromString(reader.getString());
                } else if ("input".equals(fieldName)) {
                    input = reader.readArray(reader1 -> ResponsesItem.fromJson(reader1));
                } else if ("previous_response_id".equals(fieldName)) {
                    previousResponseId = reader.getString();
                } else if ("include".equals(fieldName)) {
                    include
                        = reader.readArray(reader1 -> CreateResponsesRequestIncludable.fromString(reader1.getString()));
                } else if ("tools".equals(fieldName)) {
                    tools = reader.readArray(reader1 -> ResponsesTool.fromJson(reader1));
                } else if ("instructions".equals(fieldName)) {
                    instructions = reader.getString();
                } else if ("reasoning_effort".equals(fieldName)) {
                    reasoningEffort = ReasoningEffort.fromString(reader.getString());
                } else if ("modalities".equals(fieldName)) {
                    modalities
                        = reader.readArray(reader1 -> CreateResponsesRequestModality.fromString(reader1.getString()));
                } else if ("text".equals(fieldName)) {
                    text = CreateResponsesRequestText.fromJson(reader);
                } else if ("audio".equals(fieldName)) {
                    audio = CreateResponsesRequestAudio1.fromJson(reader);
                } else if ("tool_choice".equals(fieldName)) {
                    toolChoice
                        = reader.getNullable(nonNullReader -> BinaryData.fromObject(nonNullReader.readUntyped()));
                } else if ("temperature".equals(fieldName)) {
                    temperature = reader.getNullable(JsonReader::getDouble);
                } else if ("top_p".equals(fieldName)) {
                    topP = reader.getNullable(JsonReader::getDouble);
                } else if ("top_logprobs".equals(fieldName)) {
                    topLogprobs = reader.getNullable(JsonReader::getInt);
                } else if ("presence_penalty".equals(fieldName)) {
                    presencePenalty = reader.getNullable(JsonReader::getDouble);
                } else if ("frequency_penalty".equals(fieldName)) {
                    frequencyPenalty = reader.getNullable(JsonReader::getDouble);
                } else if ("max_completion_tokens".equals(fieldName)) {
                    maxCompletionTokens = reader.getNullable(JsonReader::getInt);
                } else if ("truncation".equals(fieldName)) {
                    truncation = CreateResponsesRequestTruncation.fromString(reader.getString());
                } else if ("user".equals(fieldName)) {
                    user = reader.getString();
                } else if ("service_tier".equals(fieldName)) {
                    serviceTier = CreateResponsesRequestServiceTier.fromString(reader.getString());
                } else if ("metadata".equals(fieldName)) {
                    metadata = reader.readMap(reader1 -> reader1.getString());
                } else if ("parallel_tool_calls".equals(fieldName)) {
                    parallelToolCalls = reader.getNullable(JsonReader::getBoolean);
                } else if ("stream".equals(fieldName)) {
                    stream = reader.getNullable(JsonReader::getBoolean);
                } else if ("store".equals(fieldName)) {
                    store = reader.getNullable(JsonReader::getBoolean);
                } else {
                    reader.skipChildren();
                }
            }
            CreateResponsesRequest deserializedCreateResponsesRequest = new CreateResponsesRequest(model, input);
            deserializedCreateResponsesRequest.previousResponseId = previousResponseId;
            deserializedCreateResponsesRequest.include = include;
            deserializedCreateResponsesRequest.tools = tools;
            deserializedCreateResponsesRequest.instructions = instructions;
            deserializedCreateResponsesRequest.reasoningEffort = reasoningEffort;
            deserializedCreateResponsesRequest.modalities = modalities;
            deserializedCreateResponsesRequest.text = text;
            deserializedCreateResponsesRequest.audio = audio;
            deserializedCreateResponsesRequest.toolChoice = toolChoice;
            deserializedCreateResponsesRequest.temperature = temperature;
            deserializedCreateResponsesRequest.topP = topP;
            deserializedCreateResponsesRequest.topLogprobs = topLogprobs;
            deserializedCreateResponsesRequest.presencePenalty = presencePenalty;
            deserializedCreateResponsesRequest.frequencyPenalty = frequencyPenalty;
            deserializedCreateResponsesRequest.maxCompletionTokens = maxCompletionTokens;
            deserializedCreateResponsesRequest.truncation = truncation;
            deserializedCreateResponsesRequest.user = user;
            deserializedCreateResponsesRequest.serviceTier = serviceTier;
            deserializedCreateResponsesRequest.metadata = metadata;
            deserializedCreateResponsesRequest.parallelToolCalls = parallelToolCalls;
            deserializedCreateResponsesRequest.stream = stream;
            deserializedCreateResponsesRequest.store = store;
            return deserializedCreateResponsesRequest;
        });
    }
}
