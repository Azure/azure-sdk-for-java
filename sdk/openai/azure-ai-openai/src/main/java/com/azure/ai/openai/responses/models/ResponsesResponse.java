// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.openai.responses.models;

import com.azure.core.annotation.Generated;
import com.azure.core.annotation.Immutable;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;
import java.time.Instant;
import java.time.OffsetDateTime;
import java.time.ZoneOffset;
import java.util.List;
import java.util.Map;

/**
 * The ResponsesResponse model.
 */
@Immutable
public final class ResponsesResponse implements JsonSerializable<ResponsesResponse> {

    /*
     * A unique identifier for this Response.
     */
    @Generated
    private final String id;

    /*
     * The object type of this resource - always set to `response`.
     */
    @Generated
    private final String object = "response";

    /*
     * Unix timestamp (in seconds) of when this Response was created.
     */
    @Generated
    private final long createdAt;

    /*
     * The status property.
     */
    @Generated
    private final ResponsesResponseStatus status;

    /*
     * The model used to generate this Response.
     */
    @Generated
    private final String model;

    /*
     * The unique ID of the previous response to the model. Use this to create multi-turn conversations.
     */
    @Generated
    private final String previousResponseId;

    /*
     * An array of content items generated by the model.
     */
    @Generated
    private final List<ResponsesItem> output;

    /*
     * An error object returned when the model fails to generate a Response.
     */
    @Generated
    private final ResponsesError error;

    /*
     * The tools available to the model when generating a Response.
     */
    @Generated
    private final List<ResponsesTool> tools;

    /*
     * The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     */
    @Generated
    private final ResponsesResponseTruncation truncation;

    /*
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
     * lower values like 0.2 will make it more focused and deterministic.
     * 
     * We generally recommend altering this or `top_p` but not both.
     */
    @Generated
    private final double temperature;

    /*
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of
     * the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are
     * considered.
     */
    @Generated
    private final double topP;

    /*
     * **o1 models only**
     * 
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     */
    @Generated
    private final ResponsesResponseReasoningEffort reasoningEffort;

    /*
     * The usage property.
     */
    @Generated
    private final ResponsesResponseUsage usage;

    /*
     * The metadata property.
     */
    @Generated
    private final Map<String, String> metadata;

    /**
     * Creates an instance of ResponsesResponse class.
     *
     * @param id the id value to set.
     * @param createdAt the createdAt value to set.
     * @param status the status value to set.
     * @param model the model value to set.
     * @param previousResponseId the previousResponseId value to set.
     * @param output the output value to set.
     * @param error the error value to set.
     * @param tools the tools value to set.
     * @param truncation the truncation value to set.
     * @param temperature the temperature value to set.
     * @param topP the topP value to set.
     * @param reasoningEffort the reasoningEffort value to set.
     * @param usage the usage value to set.
     * @param metadata the metadata value to set.
     */
    @Generated
    private ResponsesResponse(String id, OffsetDateTime createdAt, ResponsesResponseStatus status, String model,
        String previousResponseId, List<ResponsesItem> output, ResponsesError error, List<ResponsesTool> tools,
        ResponsesResponseTruncation truncation, double temperature, double topP,
        ResponsesResponseReasoningEffort reasoningEffort, ResponsesResponseUsage usage, Map<String, String> metadata) {
        this.id = id;
        if (createdAt == null) {
            this.createdAt = 0L;
        } else {
            this.createdAt = createdAt.toEpochSecond();
        }
        this.status = status;
        this.model = model;
        this.previousResponseId = previousResponseId;
        this.output = output;
        this.error = error;
        this.tools = tools;
        this.truncation = truncation;
        this.temperature = temperature;
        this.topP = topP;
        this.reasoningEffort = reasoningEffort;
        this.usage = usage;
        this.metadata = metadata;
    }

    /**
     * Get the id property: A unique identifier for this Response.
     *
     * @return the id value.
     */
    @Generated
    public String getId() {
        return this.id;
    }

    /**
     * Get the object property: The object type of this resource - always set to `response`.
     *
     * @return the object value.
     */
    @Generated
    public String getObject() {
        return this.object;
    }

    /**
     * Get the createdAt property: Unix timestamp (in seconds) of when this Response was created.
     *
     * @return the createdAt value.
     */
    @Generated
    public OffsetDateTime getCreatedAt() {
        return OffsetDateTime.ofInstant(Instant.ofEpochSecond(this.createdAt), ZoneOffset.UTC);
    }

    /**
     * Get the status property: The status property.
     *
     * @return the status value.
     */
    @Generated
    public ResponsesResponseStatus getStatus() {
        return this.status;
    }

    /**
     * Get the model property: The model used to generate this Response.
     *
     * @return the model value.
     */
    @Generated
    public String getModel() {
        return this.model;
    }

    /**
     * Get the previousResponseId property: The unique ID of the previous response to the model. Use this to create
     * multi-turn conversations.
     *
     * @return the previousResponseId value.
     */
    @Generated
    public String getPreviousResponseId() {
        return this.previousResponseId;
    }

    /**
     * Get the output property: An array of content items generated by the model.
     *
     * @return the output value.
     */
    @Generated
    public List<ResponsesItem> getOutput() {
        return this.output;
    }

    /**
     * Get the error property: An error object returned when the model fails to generate a Response.
     *
     * @return the error value.
     */
    @Generated
    public ResponsesError getError() {
        return this.error;
    }

    /**
     * Get the tools property: The tools available to the model when generating a Response.
     *
     * @return the tools value.
     */
    @Generated
    public List<ResponsesTool> getTools() {
        return this.tools;
    }

    /**
     * Get the truncation property: The truncation strategy to use for the model response.
     * - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model
     * will truncate the response to fit the context window by dropping input items in the middle of the conversation.
     * - `disabled`: If a model response will exceed the context window size for a model, the request will fail with a
     * 400 error.
     *
     * @return the truncation value.
     */
    @Generated
    public ResponsesResponseTruncation getTruncation() {
        return this.truncation;
    }

    /**
     * Get the temperature property: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
     * the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @return the temperature value.
     */
    @Generated
    public double getTemperature() {
        return this.temperature;
    }

    /**
     * Get the topP property: An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top
     * 10% probability mass are considered.
     *
     * @return the topP value.
     */
    @Generated
    public double getTopP() {
        return this.topP;
    }

    /**
     * Get the reasoningEffort property: **o1 models only**
     *
     * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
     * Currently supported values are `low`, `medium`, and `high`.
     * Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
     *
     * @return the reasoningEffort value.
     */
    @Generated
    public ResponsesResponseReasoningEffort getReasoningEffort() {
        return this.reasoningEffort;
    }

    /**
     * Get the usage property: The usage property.
     *
     * @return the usage value.
     */
    @Generated
    public ResponsesResponseUsage getUsage() {
        return this.usage;
    }

    /**
     * Get the metadata property: The metadata property.
     *
     * @return the metadata value.
     */
    @Generated
    public Map<String, String> getMetadata() {
        return this.metadata;
    }

    /**
     * {@inheritDoc}
     */
    @Generated
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("id", this.id);
        jsonWriter.writeStringField("object", this.object);
        jsonWriter.writeLongField("created_at", this.createdAt);
        jsonWriter.writeStringField("status", this.status == null ? null : this.status.toString());
        jsonWriter.writeStringField("model", this.model);
        jsonWriter.writeStringField("previous_response_id", this.previousResponseId);
        jsonWriter.writeArrayField("output", this.output, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeJsonField("error", this.error);
        jsonWriter.writeArrayField("tools", this.tools, (writer, element) -> writer.writeJson(element));
        jsonWriter.writeStringField("truncation", this.truncation == null ? null : this.truncation.toString());
        jsonWriter.writeDoubleField("temperature", this.temperature);
        jsonWriter.writeDoubleField("top_p", this.topP);
        jsonWriter.writeStringField("reasoning_effort",
            this.reasoningEffort == null ? null : this.reasoningEffort.toString());
        jsonWriter.writeJsonField("usage", this.usage);
        jsonWriter.writeMapField("metadata", this.metadata, (writer, element) -> writer.writeString(element));
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of ResponsesResponse from the JsonReader.
     *
     * @param jsonReader The JsonReader being read.
     * @return An instance of ResponsesResponse if the JsonReader was pointing to an instance of it, or null if it was
     * pointing to JSON null.
     * @throws IllegalStateException If the deserialized JSON object was missing any required properties.
     * @throws IOException If an error occurs while reading the ResponsesResponse.
     */
    @Generated
    public static ResponsesResponse fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            String id = null;
            OffsetDateTime createdAt = null;
            ResponsesResponseStatus status = null;
            String model = null;
            String previousResponseId = null;
            List<ResponsesItem> output = null;
            ResponsesError error = null;
            List<ResponsesTool> tools = null;
            ResponsesResponseTruncation truncation = null;
            double temperature = 0.0;
            double topP = 0.0;
            ResponsesResponseReasoningEffort reasoningEffort = null;
            ResponsesResponseUsage usage = null;
            Map<String, String> metadata = null;
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();
                if ("id".equals(fieldName)) {
                    id = reader.getString();
                } else if ("created_at".equals(fieldName)) {
                    createdAt = OffsetDateTime.ofInstant(Instant.ofEpochSecond(reader.getLong()), ZoneOffset.UTC);
                } else if ("status".equals(fieldName)) {
                    status = ResponsesResponseStatus.fromString(reader.getString());
                } else if ("model".equals(fieldName)) {
                    model = reader.getString();
                } else if ("previous_response_id".equals(fieldName)) {
                    previousResponseId = reader.getString();
                } else if ("output".equals(fieldName)) {
                    output = reader.readArray(reader1 -> ResponsesItem.fromJson(reader1));
                } else if ("error".equals(fieldName)) {
                    error = ResponsesError.fromJson(reader);
                } else if ("tools".equals(fieldName)) {
                    tools = reader.readArray(reader1 -> ResponsesTool.fromJson(reader1));
                } else if ("truncation".equals(fieldName)) {
                    truncation = ResponsesResponseTruncation.fromString(reader.getString());
                } else if ("temperature".equals(fieldName)) {
                    temperature = reader.getDouble();
                } else if ("top_p".equals(fieldName)) {
                    topP = reader.getDouble();
                } else if ("reasoning_effort".equals(fieldName)) {
                    reasoningEffort = ResponsesResponseReasoningEffort.fromString(reader.getString());
                } else if ("usage".equals(fieldName)) {
                    usage = ResponsesResponseUsage.fromJson(reader);
                } else if ("metadata".equals(fieldName)) {
                    metadata = reader.readMap(reader1 -> reader1.getString());
                } else {
                    reader.skipChildren();
                }
            }
            return new ResponsesResponse(id, createdAt, status, model, previousResponseId, output, error, tools,
                truncation, temperature, topP, reasoningEffort, usage, metadata);
        });
    }
}
