// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.cognitiveservices.face;

import com.azure.core.annotation.Generated;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceClient;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.exception.ClientAuthenticationException;
import com.azure.core.exception.HttpResponseException;
import com.azure.core.exception.ResourceModifiedException;
import com.azure.core.exception.ResourceNotFoundException;
import com.azure.core.http.rest.RequestOptions;
import com.azure.core.http.rest.Response;
import com.azure.core.util.BinaryData;

/** Initializes a new instance of the synchronous FaceClient type. */
@ServiceClient(builder = FaceClientBuilder.class)
public final class FaceClient {
    @Generated private final FaceAsyncClient asyncClient;

    /**
     * Initializes an instance of FaceClient class.
     *
     * @param asyncClient the async client.
     */
    @Generated
    FaceClient(FaceAsyncClient asyncClient) {
        this.asyncClient = asyncClient;
    }

    /**
     * Given query face's faceId, to search the similar-looking faces from a faceId array, a face list or a large face
     * list. faceId array contains the faces created by [Face - Detect With
     * Url](https://docs.microsoft.com/rest/api/faceapi/face/detectwithurl) or [Face - Detect With
     * Stream](https://docs.microsoft.com/rest/api/faceapi/face/detectwithstream), which will expire at the time
     * specified by faceIdTimeToLive after creation. A "faceListId" is created by [FaceList -
     * Create](https://docs.microsoft.com/rest/api/faceapi/facelist/create) containing persistedFaceIds that will not
     * expire. And a "largeFaceListId" is created by [LargeFaceList -
     * Create](https://docs.microsoft.com/rest/api/faceapi/largefacelist/create) containing persistedFaceIds that will
     * also not expire. Depending on the input the returned similar faces list contains faceIds or persistedFaceIds
     * ranked by similarity. &lt;br/&gt;Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson"
     * is the default mode that it tries to find faces of the same person as possible by using internal same-person
     * thresholds. It is useful to find a known person's other photos. Note that an empty list will be returned if no
     * faces pass the internal thresholds. "matchFace" mode ignores same-person thresholds and returns ranked similar
     * faces anyway, even the similarity is low. It can be used in the cases like searching celebrity-looking faces.
     * &lt;br/&gt;The 'recognitionModel' associated with the query face's faceId should be the same as the
     * 'recognitionModel' used by the target faceId array, face list or large face list.
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     faceId: String
     *     faceListId: String
     *     largeFaceListId: String
     *     faceIds: [
     *         String
     *     ]
     *     maxNumOfCandidatesReturned: Integer
     *     mode: String(matchPerson/matchFace)
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * [
     *     {
     *         faceId: String
     *         persistedFaceId: String
     *         confidence: float
     *     }
     * ]
     * }</pre>
     *
     * @param body Request body for Find Similar.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return array of SimilarFace along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> findSimilarWithResponse(BinaryData body, RequestOptions requestOptions) {
        return this.asyncClient.findSimilarWithResponse(body, requestOptions).block();
    }

    /**
     * Divide candidate faces into groups based on face similarity.&lt;br /&gt; * The output is one or more disjointed
     * face groups and a messyGroup. A face group contains faces that have similar looking, often of the same person.
     * Face groups are ranked by group size, i.e. number of faces. Notice that faces belonging to a same person might be
     * split into several groups in the result. * MessyGroup is a special face group containing faces that cannot find
     * any similar counterpart face from original faces. The messyGroup will not appear in the result if all faces found
     * their counterparts. * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try [Face -
     * Verify](https://docs.microsoft.com/rest/api/faceapi/face/verifyfacetoface) when you only have 2 candidate faces.
     * * The 'recognitionModel' associated with the query faces' faceIds should be the same.
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     faceIds: [
     *         String
     *     ]
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     groups: [
     *         [
     *             String
     *         ]
     *     ]
     *     messyGroup: [
     *         String
     *     ]
     * }
     * }</pre>
     *
     * @param body Request body for grouping.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return an array of face groups based on face similarity along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> groupWithResponse(BinaryData body, RequestOptions requestOptions) {
        return this.asyncClient.groupWithResponse(body, requestOptions).block();
    }

    /**
     * 1-to-many identification to find the closest matches of the specific query person face from a person group, large
     * person group, person directory dynamic person group or person directory personIds array. &lt;br/&gt; For each
     * face in the faceIds array, Face Identify will compute similarities between the query face and all the faces in
     * the person group (given by personGroupId) or large person group (given by largePersonGroupId), and return
     * candidate person(s) for that face ranked by similarity confidence. The person group/large person group should be
     * trained to make it ready for identification. See more in [PersonGroup -
     * Train](https://docs.microsoft.com/rest/api/faceapi/persongroup/train) and [LargePersonGroup -
     * Train](https://docs.microsoft.com/rest/api/faceapi/largepersongroup/train). &lt;br/&gt;
     *
     * <p>Remarks:&lt;br /&gt; * The algorithm allows more than one face to be identified independently at the same
     * request, but no more than 10 faces. * Each person in the person group/large person group could have more than one
     * face, but no more than 248 faces. * Higher face image quality means better identification precision. Please
     * consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.
     * * Number of candidates returned is restricted by maxNumOfCandidatesReturned and confidenceThreshold. If no person
     * is identified, the returned candidates will be an empty array. * Try [Face - Find
     * Similar](https://docs.microsoft.com/rest/api/faceapi/face/findsimilar) when you need to find similar faces from a
     * face list/large face list instead of a person group/large person group. * The 'recognitionModel' associated with
     * the query faces' faceIds should be the same as the 'recognitionModel' used by the target person group or large
     * person group.
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     faceIds: [
     *         String
     *     ]
     *     personGroupId: String
     *     largePersonGroupId: String
     *     dynamicPersonGroupId: String
     *     personIds: [
     *         String
     *     ]
     *     maxNumOfCandidatesReturned: Integer
     *     confidenceThreshold: Float
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * [
     *     {
     *         faceId: String
     *         candidates: [
     *             {
     *                 personId: String
     *                 confidence: float
     *             }
     *         ]
     *     }
     * ]
     * }</pre>
     *
     * @param body Request body for identify operation.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return array of IdentifyResult along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> identifyWithResponse(BinaryData body, RequestOptions requestOptions) {
        return this.asyncClient.identifyWithResponse(body, requestOptions).block();
    }

    /**
     * Verify whether two faces belong to a same person or whether one face belongs to a person. &lt;br/&gt;
     * Remarks:&lt;br /&gt; * Higher face image quality means better identification precision. Please consider
     * high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger. * For
     * the scenarios that are sensitive to accuracy please make your own judgment. * The 'recognitionModel' associated
     * with the query faces' faceIds should be the same as the 'recognitionModel' used by the target face, person group
     * or large person group.
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     faceId1: String
     *     faceId2: String
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     isIdentical: boolean
     *     confidence: float
     * }
     * }</pre>
     *
     * @param body Request body for face to face verification.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return result of the verify operation along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> verifyFaceToFaceWithResponse(BinaryData body, RequestOptions requestOptions) {
        return this.asyncClient.verifyFaceToFaceWithResponse(body, requestOptions).block();
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and
     * attributes.&lt;br /&gt; * No image will be stored. Only the extracted face feature will be stored on server. The
     * faceId is an identifier of the face feature and will be used in [Face -
     * Identify](https://docs.microsoft.com/rest/api/faceapi/face/identify), [Face -
     * Verify](https://docs.microsoft.com/rest/api/faceapi/face/verifyfacetoface), and [Face - Find
     * Similar](https://docs.microsoft.com/rest/api/faceapi/face/findsimilar). The stored face feature(s) will expire
     * and be deleted at the time specified by faceIdTimeToLive after the original detection call. * Optional parameters
     * include faceId, landmarks, and attributes. Attributes include age, gender, headPose, smile, facialHair, glasses,
     * emotion, hair, makeup, occlusion, accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of
     * the results returned for specific attributes may not be highly accurate. * JPEG, PNG, GIF (the first frame), and
     * BMP format are supported. The allowed image file size is from 1KB to 6MB. * Up to 100 faces can be returned for
     * an image. Faces are ranked by face rectangle size from large to small. * For optimal results when querying [Face
     * - Identify](https://docs.microsoft.com/rest/api/faceapi/face/identify), [Face -
     * Verify](https://docs.microsoft.com/rest/api/faceapi/face/verifyfacetoface), and [Face - Find
     * Similar](https://docs.microsoft.com/rest/api/faceapi/face/findsimilar) ('returnFaceId' is true), please use faces
     * that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes). * The minimum
     * detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher
     * than 1920x1080 pixels will need a proportionally larger minimum face size. * Different 'detectionModel' values
     * can be provided. To use and compare different detection models, please refer to [How to specify a detection
     * model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model).
     *
     * <p>* Different 'recognitionModel' values are provided. If follow-up operations like Verify, Identify, Find
     * Similar are needed, please specify the recognition model with 'recognitionModel' parameter. The default value for
     * 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need in
     * this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to [Specify a recognition
     * model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-recognition-model).
     *
     * <p><strong>Query Parameters</strong>
     *
     * <table border="1">
     *     <caption>Query Parameters</caption>
     *     <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     *     <tr><td>returnFaceId</td><td>String</td><td>No</td><td>A value indicating whether the operation should return faceIds of detected faces.</td></tr>
     *     <tr><td>returnFaceLandmarks</td><td>String</td><td>No</td><td>A value indicating whether the operation should return landmarks of the detected faces.</td></tr>
     *     <tr><td>returnFaceAttributes</td><td>String</td><td>No</td><td>Analyze and return the one or more specified face attributes in the comma-separated string like "returnFaceAttributes=age,gender". The available attributes depends on the 'detectionModel' specified. 'detection_01' supports age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure, noise, and qualityForRecognition. While 'detection_02' does not support any attributes and 'detection_03' only supports mask and qualityForRecognition. Additionally, qualityForRecognition is only supported when the 'recognitionModel' is specified as 'recognition_03' or 'recognition_04'. Note that each face attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     *     <tr><td>recognitionModel</td><td>String</td><td>No</td><td>Name of recognition model. Recognition model is used when the face features are extracted and associated with detected faceIds, (Large)FaceList or (Large)PersonGroup. A recognition model name can be provided when performing Face - Detect or (Large)FaceList - Create or (Large)PersonGroup - Create. The default value is 'recognition_01', if latest model needed, please explicitly specify the model you need.</td></tr>
     *     <tr><td>returnRecognitionModel</td><td>String</td><td>No</td><td>A value indicating whether the operation should return 'recognitionModel' in response.</td></tr>
     *     <tr><td>detectionModel</td><td>String</td><td>No</td><td>Name of detection model. Detection model is used to detect faces in the submitted image. A detection model name can be provided when performing Face - Detect or (Large)FaceList - Add Face or (Large)PersonGroup - Add Face. The default value is 'detection_01', if another model is needed, please explicitly specify it.</td></tr>
     *     <tr><td>faceIdTimeToLive</td><td>String</td><td>No</td><td>The number of seconds for the faceId being cached. Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     url: String
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * [
     *     {
     *         faceId: String
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04)
     *         faceRectangle: {
     *             width: int
     *             height: int
     *             left: int
     *             top: int
     *         }
     *         faceLandmarks: {
     *             pupilLeft: {
     *                 x: float
     *                 y: float
     *             }
     *             pupilRight: (recursive schema, see pupilRight above)
     *             noseTip: (recursive schema, see noseTip above)
     *             mouthLeft: (recursive schema, see mouthLeft above)
     *             mouthRight: (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter: (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner: (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter: (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop: (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom: (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner: (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner: (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter: (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner: (recursive schema, see eyeRightInner above)
     *             eyeRightTop: (recursive schema, see eyeRightTop above)
     *             eyeRightBottom: (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter: (recursive schema, see eyeRightOuter above)
     *             noseRootLeft: (recursive schema, see noseRootLeft above)
     *             noseRootRight: (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop: (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop: (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip: (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip: (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop: (recursive schema, see upperLipTop above)
     *             upperLipBottom: (recursive schema, see upperLipBottom above)
     *             underLipTop: (recursive schema, see underLipTop above)
     *             underLipBottom: (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes: {
     *             age: Float
     *             gender: String(male/female)
     *             smile: Float
     *             facialHair: {
     *                 moustache: Float
     *                 beard: Float
     *                 sideburns: Float
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles)
     *             headPose: {
     *                 roll: Float
     *                 yaw: Float
     *                 pitch: Float
     *             }
     *             emotion: {
     *                 anger: Float
     *                 contempt: Float
     *                 disgust: Float
     *                 fear: Float
     *                 happiness: Float
     *                 neutral: Float
     *                 sadness: Float
     *                 surprise: Float
     *             }
     *             hair: {
     *                 bald: Float
     *                 invisible: Boolean
     *                 hairColor: [
     *                     {
     *                         color: String(unknown/white/gray/blond/brown/red/black/other)
     *                         confidence: Float
     *                     }
     *                 ]
     *             }
     *             makeup: {
     *                 eyeMakeup: Boolean
     *                 lipMakeup: Boolean
     *             }
     *             occlusion: {
     *                 foreheadOccluded: Boolean
     *                 eyeOccluded: Boolean
     *                 mouthOccluded: Boolean
     *             }
     *             accessories: [
     *                 {
     *                     type: String(headWear/glasses/mask)
     *                     confidence: Float
     *                 }
     *             ]
     *             blur: {
     *                 blurLevel: String(Low/Medium/High)
     *                 value: Float
     *             }
     *             exposure: {
     *                 exposureLevel: String(UnderExposure/GoodExposure/OverExposure)
     *                 value: Float
     *             }
     *             noise: {
     *                 noiseLevel: String(Low/Medium/High)
     *                 value: Float
     *             }
     *             mask: {
     *                 type: String(noMask/faceMask/otherMaskOrOcclusion/uncertain)
     *                 noseAndMouthCovered: Boolean
     *             }
     *             qualityForRecognition: String(Low/Medium/High)
     *         }
     *     }
     * ]
     * }</pre>
     *
     * @param imageUrl A JSON document with a URL pointing to the image that is to be analyzed.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return array of DetectedFace along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> detectWithUrlWithResponse(BinaryData imageUrl, RequestOptions requestOptions) {
        return this.asyncClient.detectWithUrlWithResponse(imageUrl, requestOptions).block();
    }

    /**
     * Verify whether two faces belong to a same person. Compares a face Id with a Person Id.
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     faceId: String
     *     personGroupId: String
     *     largePersonGroupId: String
     *     personId: String
     * }
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * {
     *     isIdentical: boolean
     *     confidence: float
     * }
     * }</pre>
     *
     * @param body Request body for face to person verification.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return result of the verify operation along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> verifyFaceToPersonWithResponse(BinaryData body, RequestOptions requestOptions) {
        return this.asyncClient.verifyFaceToPersonWithResponse(body, requestOptions).block();
    }

    /**
     * Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and
     * attributes.&lt;br /&gt; * No image will be stored. Only the extracted face feature will be stored on server. The
     * faceId is an identifier of the face feature and will be used in [Face -
     * Identify](https://docs.microsoft.com/rest/api/faceapi/face/identify), [Face -
     * Verify](https://docs.microsoft.com/rest/api/faceapi/face/verifyfacetoface), and [Face - Find
     * Similar](https://docs.microsoft.com/rest/api/faceapi/face/findsimilar). The stored face feature(s) will expire
     * and be deleted at the time specified by faceIdTimeToLive after the original detection call. * Optional parameters
     * include faceId, landmarks, and attributes. Attributes include age, gender, headPose, smile, facialHair, glasses,
     * emotion, hair, makeup, occlusion, accessories, blur, exposure, noise, mask, and qualityForRecognition. Some of
     * the results returned for specific attributes may not be highly accurate. * JPEG, PNG, GIF (the first frame), and
     * BMP format are supported. The allowed image file size is from 1KB to 6MB. * Up to 100 faces can be returned for
     * an image. Faces are ranked by face rectangle size from large to small. * For optimal results when querying [Face
     * - Identify](https://docs.microsoft.com/rest/api/faceapi/face/identify), [Face -
     * Verify](https://docs.microsoft.com/rest/api/faceapi/face/verifyfacetoface), and [Face - Find
     * Similar](https://docs.microsoft.com/rest/api/faceapi/face/findsimilar) ('returnFaceId' is true), please use faces
     * that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes). * The minimum
     * detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher
     * than 1920x1080 pixels will need a proportionally larger minimum face size. * Different 'detectionModel' values
     * can be provided. To use and compare different detection models, please refer to [How to specify a detection
     * model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model) *
     * Different 'recognitionModel' values are provided. If follow-up operations like Verify, Identify, Find Similar are
     * needed, please specify the recognition model with 'recognitionModel' parameter. The default value for
     * 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need in
     * this parameter. Once specified, the detected faceIds will be associated with the specified recognition model.
     * More details, please refer to [Specify a recognition
     * model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-recognition-model).
     *
     * <p><strong>Query Parameters</strong>
     *
     * <table border="1">
     *     <caption>Query Parameters</caption>
     *     <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     *     <tr><td>returnFaceId</td><td>String</td><td>No</td><td>A value indicating whether the operation should return faceIds of detected faces.</td></tr>
     *     <tr><td>returnFaceLandmarks</td><td>String</td><td>No</td><td>A value indicating whether the operation should return landmarks of the detected faces.</td></tr>
     *     <tr><td>returnFaceAttributes</td><td>String</td><td>No</td><td>Analyze and return the one or more specified face attributes in the comma-separated string like "returnFaceAttributes=age,gender". The available attributes depends on the 'detectionModel' specified. 'detection_01' supports age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure, noise, and qualityForRecognition. While 'detection_02' does not support any attributes and 'detection_03' only supports mask and qualityForRecognition. Additionally, qualityForRecognition is only supported when the 'recognitionModel' is specified as 'recognition_03' or 'recognition_04'. Note that each face attribute analysis has additional computational and time cost. In the form of "," separated string.</td></tr>
     *     <tr><td>recognitionModel</td><td>String</td><td>No</td><td>Name of recognition model. Recognition model is used when the face features are extracted and associated with detected faceIds, (Large)FaceList or (Large)PersonGroup. A recognition model name can be provided when performing Face - Detect or (Large)FaceList - Create or (Large)PersonGroup - Create. The default value is 'recognition_01', if latest model needed, please explicitly specify the model you need.</td></tr>
     *     <tr><td>returnRecognitionModel</td><td>String</td><td>No</td><td>A value indicating whether the operation should return 'recognitionModel' in response.</td></tr>
     *     <tr><td>detectionModel</td><td>String</td><td>No</td><td>Name of detection model. Detection model is used to detect faces in the submitted image. A detection model name can be provided when performing Face - Detect or (Large)FaceList - Add Face or (Large)PersonGroup - Add Face. The default value is 'detection_01', if another model is needed, please explicitly specify it.</td></tr>
     *     <tr><td>faceIdTimeToLive</td><td>String</td><td>No</td><td>The number of seconds for the faceId being cached. Supported range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).</td></tr>
     * </table>
     *
     * <p><strong>Header Parameters</strong>
     *
     * <table border="1">
     *     <caption>Header Parameters</caption>
     *     <tr><th>Name</th><th>Type</th><th>Required</th><th>Description</th></tr>
     *     <tr><td>Content-Length</td><td>long</td><td>Yes</td><td>The contentLength parameter</td></tr>
     * </table>
     *
     * <p><strong>Request Body Schema</strong>
     *
     * <pre>{@code
     * Flux<ByteBuffer>
     * }</pre>
     *
     * <p><strong>Response Body Schema</strong>
     *
     * <pre>{@code
     * [
     *     {
     *         faceId: String
     *         recognitionModel: String(recognition_01/recognition_02/recognition_03/recognition_04)
     *         faceRectangle: {
     *             width: int
     *             height: int
     *             left: int
     *             top: int
     *         }
     *         faceLandmarks: {
     *             pupilLeft: {
     *                 x: float
     *                 y: float
     *             }
     *             pupilRight: (recursive schema, see pupilRight above)
     *             noseTip: (recursive schema, see noseTip above)
     *             mouthLeft: (recursive schema, see mouthLeft above)
     *             mouthRight: (recursive schema, see mouthRight above)
     *             eyebrowLeftOuter: (recursive schema, see eyebrowLeftOuter above)
     *             eyebrowLeftInner: (recursive schema, see eyebrowLeftInner above)
     *             eyeLeftOuter: (recursive schema, see eyeLeftOuter above)
     *             eyeLeftTop: (recursive schema, see eyeLeftTop above)
     *             eyeLeftBottom: (recursive schema, see eyeLeftBottom above)
     *             eyeLeftInner: (recursive schema, see eyeLeftInner above)
     *             eyebrowRightInner: (recursive schema, see eyebrowRightInner above)
     *             eyebrowRightOuter: (recursive schema, see eyebrowRightOuter above)
     *             eyeRightInner: (recursive schema, see eyeRightInner above)
     *             eyeRightTop: (recursive schema, see eyeRightTop above)
     *             eyeRightBottom: (recursive schema, see eyeRightBottom above)
     *             eyeRightOuter: (recursive schema, see eyeRightOuter above)
     *             noseRootLeft: (recursive schema, see noseRootLeft above)
     *             noseRootRight: (recursive schema, see noseRootRight above)
     *             noseLeftAlarTop: (recursive schema, see noseLeftAlarTop above)
     *             noseRightAlarTop: (recursive schema, see noseRightAlarTop above)
     *             noseLeftAlarOutTip: (recursive schema, see noseLeftAlarOutTip above)
     *             noseRightAlarOutTip: (recursive schema, see noseRightAlarOutTip above)
     *             upperLipTop: (recursive schema, see upperLipTop above)
     *             upperLipBottom: (recursive schema, see upperLipBottom above)
     *             underLipTop: (recursive schema, see underLipTop above)
     *             underLipBottom: (recursive schema, see underLipBottom above)
     *         }
     *         faceAttributes: {
     *             age: Float
     *             gender: String(male/female)
     *             smile: Float
     *             facialHair: {
     *                 moustache: Float
     *                 beard: Float
     *                 sideburns: Float
     *             }
     *             glasses: String(noGlasses/readingGlasses/sunglasses/swimmingGoggles)
     *             headPose: {
     *                 roll: Float
     *                 yaw: Float
     *                 pitch: Float
     *             }
     *             emotion: {
     *                 anger: Float
     *                 contempt: Float
     *                 disgust: Float
     *                 fear: Float
     *                 happiness: Float
     *                 neutral: Float
     *                 sadness: Float
     *                 surprise: Float
     *             }
     *             hair: {
     *                 bald: Float
     *                 invisible: Boolean
     *                 hairColor: [
     *                     {
     *                         color: String(unknown/white/gray/blond/brown/red/black/other)
     *                         confidence: Float
     *                     }
     *                 ]
     *             }
     *             makeup: {
     *                 eyeMakeup: Boolean
     *                 lipMakeup: Boolean
     *             }
     *             occlusion: {
     *                 foreheadOccluded: Boolean
     *                 eyeOccluded: Boolean
     *                 mouthOccluded: Boolean
     *             }
     *             accessories: [
     *                 {
     *                     type: String(headWear/glasses/mask)
     *                     confidence: Float
     *                 }
     *             ]
     *             blur: {
     *                 blurLevel: String(Low/Medium/High)
     *                 value: Float
     *             }
     *             exposure: {
     *                 exposureLevel: String(UnderExposure/GoodExposure/OverExposure)
     *                 value: Float
     *             }
     *             noise: {
     *                 noiseLevel: String(Low/Medium/High)
     *                 value: Float
     *             }
     *             mask: {
     *                 type: String(noMask/faceMask/otherMaskOrOcclusion/uncertain)
     *                 noseAndMouthCovered: Boolean
     *             }
     *             qualityForRecognition: String(Low/Medium/High)
     *         }
     *     }
     * ]
     * }</pre>
     *
     * @param image An image stream.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return array of DetectedFace along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> detectWithStreamWithResponse(BinaryData image, RequestOptions requestOptions) {
        return this.asyncClient.detectWithStreamWithResponse(image, requestOptions).block();
    }
}
