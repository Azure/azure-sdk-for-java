// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.ai.speech.transcription;

import com.azure.ai.speech.transcription.models.AudioFileDetails;
import com.azure.ai.speech.transcription.models.EnhancedModeOptions;
import com.azure.ai.speech.transcription.models.ProfanityFilterMode;
import com.azure.ai.speech.transcription.models.TranscriptionDiarizationOptions;
import com.azure.ai.speech.transcription.models.TranscriptionContent;
import com.azure.ai.speech.transcription.models.TranscriptionOptions;
import com.azure.ai.speech.transcription.models.TranscriptionResult;
import com.azure.core.credential.KeyCredential;
import com.azure.core.http.policy.ExponentialBackoffOptions;
import com.azure.core.http.policy.HttpLogDetailLevel;
import com.azure.core.http.policy.HttpLogOptions;
import com.azure.core.http.policy.RetryOptions;
import com.azure.core.util.BinaryData;

import java.nio.file.Files;
import java.nio.file.Paths;

public final class ReadmeSamples {
    /**
     * Sample for basic audio transcription.
     */
    public void readmeSamples() {
        // BEGIN: com.azure.ai.speech.transcription.readme
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        try {
            // Read audio file
            byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

            // Create audio file details
            AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
                .setFilename("audio.wav");

            // Create transcription options
            TranscriptionOptions options = new TranscriptionOptions(audioFileDetails);

            // Create transcribe request content
            TranscriptionContent requestContent = new TranscriptionContent()
                .setOptions(options);

            // Transcribe audio
            TranscriptionResult result = client.transcribe(requestContent);

            // Process results
            System.out.println("Duration: " + result.getDuration() + "ms");
            result.getCombinedPhrases().forEach(phrase -> {
                System.out.println("Channel " + phrase.getChannel() + ": " + phrase.getText());
            });
        } catch (Exception e) {
            System.err.println("Error during transcription: " + e.getMessage());
        }
        // END: com.azure.ai.speech.transcription.readme
    }

    /**
     * Sample for creating a synchronous TranscriptionClient.
     */
    public void createSyncClient() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionclient.instantiation
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();
        // END: com.azure.ai.speech.transcription.transcriptionclient.instantiation
    }

    /**
     * Sample for creating an asynchronous TranscriptionAsyncClient.
     */
    public void createAsyncClient() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionasyncclient.instantiation
        TranscriptionAsyncClient asyncClient = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildAsyncClient();
        // END: com.azure.ai.speech.transcription.transcriptionasyncclient.instantiation
    }

    /**
     * Sample for transcribing audio with the synchronous client.
     */
    public void transcribeAudioSync() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionclient.transcribe
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails);

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);

        System.out.println("Duration: " + result.getDuration() + "ms");
        result.getCombinedPhrases().forEach(phrase -> {
            System.out.println("Transcription: " + phrase.getText());
        });
        // END: com.azure.ai.speech.transcription.transcriptionclient.transcribe
    }

    /**
     * Sample for transcribing audio with the asynchronous client.
     */
    public void transcribeAudioAsync() throws Exception {
        TranscriptionAsyncClient asyncClient = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildAsyncClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionasyncclient.transcribe
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails);

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        asyncClient.transcribe(requestContent)
            .subscribe(result -> {
                System.out.println("Duration: " + result.getDuration() + "ms");
                result.getCombinedPhrases().forEach(phrase -> {
                    System.out.println("Transcription: " + phrase.getText());
                });
            });
        // END: com.azure.ai.speech.transcription.transcriptionasyncclient.transcribe
    }

    /**
     * Sample for configuring advanced transcription options.
     */
    public void advancedTranscriptionOptions() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionoptions.advanced
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        // Configure advanced options
        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails)
            .setLocales(java.util.Arrays.asList("en-US", "es-ES"))  // Specify candidate locales
            .setProfanityFilterMode(ProfanityFilterMode.MASKED)     // Mask profanity
            .setDiarizationOptions(new TranscriptionDiarizationOptions()   // Enable speaker diarization
                .setMaxSpeakers(5));

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);

        // Access detailed results
        result.getPhrases().forEach(phrase -> {
            System.out.println("Speaker " + phrase.getSpeaker() + ": " + phrase.getText());
            System.out.println("Confidence: " + phrase.getConfidence());
            System.out.println("Offset: " + phrase.getOffset() + "ms");
        });
        // END: com.azure.ai.speech.transcription.transcriptionoptions.advanced
    }

    /**
     * Sample for building client with custom configuration.
     */
    public void clientWithCustomConfiguration() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionclientbuilder.configuration
        HttpLogOptions logOptions = new HttpLogOptions()
            .setLogLevel(HttpLogDetailLevel.BODY_AND_HEADERS);

        RetryOptions retryOptions = new RetryOptions(new ExponentialBackoffOptions()
            .setMaxRetries(5)
            .setBaseDelay(java.time.Duration.ofSeconds(1))
            .setMaxDelay(java.time.Duration.ofSeconds(60)));

        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .httpLogOptions(logOptions)
            .retryOptions(retryOptions)
            .buildClient();
        // END: com.azure.ai.speech.transcription.transcriptionclientbuilder.configuration
    }

    /**
     * Sample for processing detailed transcription results.
     */
    public void processDetailedResults() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));
        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");
        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(new TranscriptionOptions(audioFileDetails));

        // BEGIN: com.azure.ai.speech.transcription.transcriptionresult.detailed
        TranscriptionResult result = client.transcribe(requestContent);

        // Get overall duration
        System.out.println("Total duration: " + result.getDuration() + "ms");

        // Process each phrase with detailed information
        result.getPhrases().forEach(phrase -> {
            System.out.println("\nPhrase: " + phrase.getText());
            System.out.println("  Channel: " + phrase.getChannel());
            System.out.println("  Speaker: " + phrase.getSpeaker());
            System.out.println("  Locale: " + phrase.getLocale());
            System.out.println("  Confidence: " + phrase.getConfidence());
            System.out.println("  Timing: " + phrase.getOffset() + "ms - "
                + (phrase.getOffset() + phrase.getDuration().toMillis()) + "ms");

            // Process individual words with timestamps
            if (phrase.getWords() != null) {
                phrase.getWords().forEach(word -> {
                    System.out.println("    Word: " + word.getText() + " @ "
                        + word.getOffset() + "ms");
                });
            }
        });
        // END: com.azure.ai.speech.transcription.transcriptionresult.detailed
    }

    /**
     * Sample for using enhanced mode to improve transcription quality.
     */
    public void enhancedModeBasic() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: readme-sample-enhancedModeBasic
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        // Enable enhanced mode for improved transcription quality
        EnhancedModeOptions enhancedMode = new EnhancedModeOptions();

        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails)
            .setLocales(java.util.Arrays.asList("en-US"))
            .setEnhancedModeOptions(enhancedMode);

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);
        // END: readme-sample-enhancedModeBasic
    }

    /**
     * Sample for using enhanced mode with custom prompts.
     */
    public void enhancedModeWithPrompts() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: readme-sample-enhancedModeWithPrompts
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        // Use prompts to guide transcription with domain-specific terminology
        EnhancedModeOptions enhancedMode = new EnhancedModeOptions()
            .setPrompts(java.util.Arrays.asList(
                "Medical consultation discussing hypertension and diabetes",
                "Common medications: metformin, lisinopril, atorvastatin",
                "Patient symptoms and treatment plan"
            ));

        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails)
            .setLocales(java.util.Arrays.asList("en-US"))
            .setEnhancedModeOptions(enhancedMode);

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);
        // END: readme-sample-enhancedModeWithPrompts
    }

    /**
     * Sample for using enhanced mode with translation.
     */
    public void enhancedModeWithTranslation() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: readme-sample-enhancedModeWithTranslation
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        // Configure enhanced mode to transcribe Spanish audio and translate to English
        EnhancedModeOptions enhancedMode = new EnhancedModeOptions()
            .setTargetLanguage("en-US"); // Translate to English

        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails)
            .setLocales(java.util.Arrays.asList("es-ES")) // Source language: Spanish
            .setEnhancedModeOptions(enhancedMode);

        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);
        // END: readme-sample-enhancedModeWithTranslation
    }

    /**
     * Sample for transcribing audio using audio URL constructor.
     */
    public void transcribeWithAudioUrl() {
        // BEGIN: readme-sample-transcribeWithAudioUrl
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // Create transcription options with audio URL
        TranscriptionOptions options = new TranscriptionOptions("https://example.com/audio.wav");

        // Create transcribe request content
        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        // Transcribe audio
        TranscriptionResult result = client.transcribe(requestContent);

        // Process results
        result.getCombinedPhrases().forEach(phrase -> {
            System.out.println(phrase.getText());
        });
        // END: readme-sample-transcribeWithAudioUrl
    }

    /**
     * Sample for transcribing audio using AudioFileDetails constructor.
     */
    public void transcribeWithAudioFileDetails() throws Exception {
        // BEGIN: readme-sample-transcribeWithAudioFileDetails
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // Read audio file
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        // Create audio file details
        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav")
            .setContentType("audio/wav");

        // Create transcription options with AudioFileDetails
        TranscriptionOptions options = new TranscriptionOptions(audioFileDetails);

        // Create transcribe request content
        TranscriptionContent requestContent = new TranscriptionContent()
            .setOptions(options);

        // Transcribe audio
        TranscriptionResult result = client.transcribe(requestContent);

        // Process results
        result.getCombinedPhrases().forEach(phrase -> {
            System.out.println(phrase.getText());
        });
        // END: readme-sample-transcribeWithAudioFileDetails
    }
}

