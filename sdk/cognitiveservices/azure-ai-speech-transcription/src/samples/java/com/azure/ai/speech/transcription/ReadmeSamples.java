// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.ai.speech.transcription;

import com.azure.ai.speech.transcription.models.AudioFileDetails;
import com.azure.ai.speech.transcription.models.ProfanityFilterMode;
import com.azure.ai.speech.transcription.models.TranscriptionDiarizationOptions;
import com.azure.ai.speech.transcription.models.TranscribeRequestContent;
import com.azure.ai.speech.transcription.models.TranscriptionOptions;
import com.azure.ai.speech.transcription.models.TranscriptionResult;
import com.azure.core.credential.KeyCredential;
import com.azure.core.http.policy.ExponentialBackoffOptions;
import com.azure.core.http.policy.HttpLogDetailLevel;
import com.azure.core.http.policy.HttpLogOptions;
import com.azure.core.http.policy.RetryOptions;
import com.azure.core.util.BinaryData;

import java.nio.file.Files;
import java.nio.file.Paths;

public final class ReadmeSamples {
    /**
     * Sample for basic audio transcription.
     */
    public void readmeSamples() {
        // BEGIN: com.azure.ai.speech.transcription.readme
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        try {
            // Read audio file
            byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

            // Create audio file details
            AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
                .setFilename("audio.wav");

            // Create transcription options
            TranscriptionOptions options = new TranscriptionOptions();

            // Create transcribe request content
            TranscribeRequestContent requestContent = new TranscribeRequestContent()
                .setAudio(audioFileDetails)
                .setOptions(options);

            // Transcribe audio
            TranscriptionResult result = client.transcribe(requestContent);

            // Process results
            System.out.println("Duration: " + result.getDurationMilliseconds() + "ms");
            result.getCombinedPhrases().forEach(phrase -> {
                System.out.println("Channel " + phrase.getChannel() + ": " + phrase.getText());
            });
        } catch (Exception e) {
            System.err.println("Error during transcription: " + e.getMessage());
        }
        // END: com.azure.ai.speech.transcription.readme
    }

    /**
     * Sample for creating a synchronous TranscriptionClient.
     */
    public void createSyncClient() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionclient.instantiation
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();
        // END: com.azure.ai.speech.transcription.transcriptionclient.instantiation
    }

    /**
     * Sample for creating an asynchronous TranscriptionAsyncClient.
     */
    public void createAsyncClient() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionasyncclient.instantiation
        TranscriptionAsyncClient asyncClient = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildAsyncClient();
        // END: com.azure.ai.speech.transcription.transcriptionasyncclient.instantiation
    }

    /**
     * Sample for transcribing audio with the synchronous client.
     */
    public void transcribeAudioSync() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionclient.transcribe
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        TranscriptionOptions options = new TranscriptionOptions();

        TranscribeRequestContent requestContent = new TranscribeRequestContent()
            .setAudio(audioFileDetails)
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);

        System.out.println("Duration: " + result.getDurationMilliseconds() + "ms");
        result.getCombinedPhrases().forEach(phrase -> {
            System.out.println("Transcription: " + phrase.getText());
        });
        // END: com.azure.ai.speech.transcription.transcriptionclient.transcribe
    }

    /**
     * Sample for transcribing audio with the asynchronous client.
     */
    public void transcribeAudioAsync() throws Exception {
        TranscriptionAsyncClient asyncClient = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildAsyncClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionasyncclient.transcribe
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        TranscriptionOptions options = new TranscriptionOptions();

        TranscribeRequestContent requestContent = new TranscribeRequestContent()
            .setAudio(audioFileDetails)
            .setOptions(options);

        asyncClient.transcribe(requestContent)
            .subscribe(result -> {
                System.out.println("Duration: " + result.getDurationMilliseconds() + "ms");
                result.getCombinedPhrases().forEach(phrase -> {
                    System.out.println("Transcription: " + phrase.getText());
                });
            });
        // END: com.azure.ai.speech.transcription.transcriptionasyncclient.transcribe
    }

    /**
     * Sample for configuring advanced transcription options.
     */
    public void advancedTranscriptionOptions() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        // BEGIN: com.azure.ai.speech.transcription.transcriptionoptions.advanced
        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));

        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");

        // Configure advanced options
        TranscriptionOptions options = new TranscriptionOptions()
            .setLocales(java.util.Arrays.asList("en-US", "es-ES"))  // Specify candidate locales
            .setProfanityFilterMode(ProfanityFilterMode.MASKED)     // Mask profanity
            .setDiarizationOptions(new TranscriptionDiarizationOptions()   // Enable speaker diarization
                .setEnabled(true)
                .setMaxSpeakers(5));

        TranscribeRequestContent requestContent = new TranscribeRequestContent()
            .setAudio(audioFileDetails)
            .setOptions(options);

        TranscriptionResult result = client.transcribe(requestContent);

        // Access detailed results
        result.getPhrases().forEach(phrase -> {
            System.out.println("Speaker " + phrase.getSpeaker() + ": " + phrase.getText());
            System.out.println("Confidence: " + phrase.getConfidence());
            System.out.println("Offset: " + phrase.getOffsetMilliseconds() + "ms");
        });
        // END: com.azure.ai.speech.transcription.transcriptionoptions.advanced
    }

    /**
     * Sample for building client with custom configuration.
     */
    public void clientWithCustomConfiguration() {
        // BEGIN: com.azure.ai.speech.transcription.transcriptionclientbuilder.configuration
        HttpLogOptions logOptions = new HttpLogOptions()
            .setLogLevel(HttpLogDetailLevel.BODY_AND_HEADERS);

        RetryOptions retryOptions = new RetryOptions(new ExponentialBackoffOptions()
            .setMaxRetries(5)
            .setBaseDelay(java.time.Duration.ofSeconds(1))
            .setMaxDelay(java.time.Duration.ofSeconds(60)));

        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .httpLogOptions(logOptions)
            .retryOptions(retryOptions)
            .buildClient();
        // END: com.azure.ai.speech.transcription.transcriptionclientbuilder.configuration
    }

    /**
     * Sample for processing detailed transcription results.
     */
    public void processDetailedResults() throws Exception {
        TranscriptionClient client = new TranscriptionClientBuilder()
            .endpoint("https://<your-resource-name>.cognitiveservices.azure.com/")
            .credential(new KeyCredential("<your-api-key>"))
            .buildClient();

        byte[] audioData = Files.readAllBytes(Paths.get("path/to/audio.wav"));
        AudioFileDetails audioFileDetails = new AudioFileDetails(BinaryData.fromBytes(audioData))
            .setFilename("audio.wav");
        TranscribeRequestContent requestContent = new TranscribeRequestContent()
            .setAudio(audioFileDetails)
            .setOptions(new TranscriptionOptions());

        // BEGIN: com.azure.ai.speech.transcription.transcriptionresult.detailed
        TranscriptionResult result = client.transcribe(requestContent);

        // Get overall duration
        System.out.println("Total duration: " + result.getDurationMilliseconds() + "ms");

        // Process each phrase with detailed information
        result.getPhrases().forEach(phrase -> {
            System.out.println("\nPhrase: " + phrase.getText());
            System.out.println("  Channel: " + phrase.getChannel());
            System.out.println("  Speaker: " + phrase.getSpeaker());
            System.out.println("  Locale: " + phrase.getLocale());
            System.out.println("  Confidence: " + phrase.getConfidence());
            System.out.println("  Timing: " + phrase.getOffsetMilliseconds() + "ms - "
                + (phrase.getOffsetMilliseconds() + phrase.getDurationMilliseconds()) + "ms");

            // Process individual words with timestamps
            if (phrase.getWords() != null) {
                phrase.getWords().forEach(word -> {
                    System.out.println("    Word: " + word.getText() + " @ "
                        + word.getOffsetMilliseconds() + "ms");
                });
            }
        });
        // END: com.azure.ai.speech.transcription.transcriptionresult.detailed
    }
}

